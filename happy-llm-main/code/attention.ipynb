{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ModelArgs configuration class\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    \"\"\"æ¨¡å‹é…ç½®å‚æ•°ç±»\"\"\"\n",
    "    dim: int = 512                    # æ¨¡å‹ç»´åº¦\n",
    "    n_heads: int = 8                  # æ³¨æ„åŠ›å¤´æ•°\n",
    "    n_layers: int = 6                 # å±‚æ•°\n",
    "    vocab_size: int = 50000           # è¯æ±‡è¡¨å¤§å°\n",
    "    max_seq_len: int = 2048           # æœ€å¤§åºåˆ—é•¿åº¦\n",
    "    dropout: float = 0.1              # Dropout æ¯”ç‡\n",
    "    bias: bool = False                # æ˜¯å¦ä½¿ç”¨åç½®\n",
    "\n",
    "print(\"ModelArgs class defined successfully!\")\n",
    "print(\"Available parameters:\")\n",
    "for field_name in ModelArgs.__dataclass_fields__:\n",
    "    field = ModelArgs.__dataclass_fields__[field_name]\n",
    "    print(f\"  {field_name}: {field.type.__name__} = {field.default}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''æ³¨æ„åŠ›è®¡ç®—å‡½æ•°'''\n",
    "def attention(query, key, value, dropout=None):\n",
    "    '''\n",
    "    è®¡ç®—\"ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›\"ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "    query: æŸ¥è¯¢å¼ é‡ï¼Œå½¢çŠ¶ä¸º (..., seq_len_q, d_k)\n",
    "    key: é”®å¼ é‡ï¼Œå½¢çŠ¶ä¸º (..., seq_len_k, d_k)\n",
    "    value: å€¼å¼ é‡ï¼Œå½¢çŠ¶ä¸º (..., seq_len_v, d_v)ï¼Œé€šå¸¸ seq_len_k == seq_len_v\n",
    "    dropout: Dropout å±‚ï¼Œå¯é€‰\n",
    "\n",
    "    è¿”å›:\n",
    "    åŠ æƒåçš„å€¼å¼ é‡ï¼Œå½¢çŠ¶ä¸º (..., seq_len_q, d_v)\n",
    "    æ³¨æ„åŠ›æƒé‡å¼ é‡ï¼Œå½¢çŠ¶ä¸º (..., seq_len_q, seq_len_k)\n",
    "    '''\n",
    "    # è·å–é”®å‘é‡çš„ç»´åº¦ d_k\n",
    "    d_k = query.size(-1)\n",
    "    print(f\"d_k: {d_k}\")\n",
    "    \n",
    "    # è®¡ç®— Q ä¸ K çš„è½¬ç½®çš„ç‚¹ç§¯ï¼Œç„¶åç¼©æ”¾\n",
    "    # æ³¨æ„åŠ›å¾—åˆ†: scores = Q Ã— K^T / âˆšd_k\n",
    "    # scores çš„å½¢çŠ¶: (..., seq_len_q, seq_len_k)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # å¯¹ scores çš„æœ€åä¸€ä¸ªç»´åº¦åº”ç”¨ softmaxï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡\n",
    "    # p_attn çš„å½¢çŠ¶: (..., seq_len_q, seq_len_k)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    \n",
    "    # (å¯é€‰) åº”ç”¨ dropout\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "        \n",
    "    # å°†æ³¨æ„åŠ›æƒé‡ p_attn ä¸ V ç›¸ä¹˜ï¼Œå¾—åˆ°åŠ æƒçš„è¾“å‡º\n",
    "    # output çš„å½¢çŠ¶: (..., seq_len_q, d_v)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to execute\n",
    "def attention_test():\n",
    "    # è®¾å®šè¶…å‚æ•°\n",
    "    batch_size = 2      # æ‰¹å¤„ç†å¤§å°\n",
    "    seq_len = 5         # åºåˆ—é•¿åº¦\n",
    "    d_k = 8             # Query å’Œ Key çš„ç»´åº¦\n",
    "    d_v = 10            # Value çš„ç»´åº¦ (å¯ä»¥å’Œ d_k ä¸åŒ)\n",
    "    dropout_rate = 0.1  # Dropout æ¯”ç‡\n",
    "\n",
    "    # 1. å®ä¾‹åŒ– Dropout å±‚\n",
    "    dropout_layer = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    # 2. åˆ›å»ºæ¨¡æ‹Ÿçš„ Q, K, V å¼ é‡\n",
    "    # å½¢çŠ¶: (batch_size, seq_len, dimension)\n",
    "    query = torch.randn(batch_size, seq_len, d_k)\n",
    "    key = torch.randn(batch_size, seq_len, d_k)\n",
    "    value = torch.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "    print(\"--- è¾“å…¥å¼ é‡å½¢çŠ¶ ---\")\n",
    "    print(f\"Query (Q) shape: {query.shape}\")\n",
    "    print(f\"Key (K) shape:   {key.shape}\")\n",
    "    print(f\"Value (V) shape: {value.shape}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # 3. è°ƒç”¨ attention å‡½æ•°\n",
    "    # å°† dropout_layer ä½œä¸ºå‚æ•°ä¼ å…¥\n",
    "    output, attention_weights = attention(query, key, value, dropout=dropout_layer)\n",
    "\n",
    "    # 4. æ‰“å°è¾“å‡ºç»“æœçš„å½¢çŠ¶\n",
    "    print(\"\\n--- è¾“å‡ºç»“æœå½¢çŠ¶ ---\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention Weights shape: {attention_weights.shape}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # 5. æ£€æŸ¥è¾“å‡ºå†…å®¹çš„å«ä¹‰\n",
    "    print(\"\\n--- è¾“å‡ºå†…å®¹æ£€æŸ¥ ---\")\n",
    "    print(\"Output[0, 0, :]:\\n\", output[0, 0, :])\n",
    "    print(\"\\nç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æƒé‡ (éƒ¨åˆ†):\\n\", attention_weights[0].round(decimals=4))\n",
    "    # æ£€æŸ¥æ³¨æ„åŠ›æƒé‡çš„å’Œæ˜¯å¦ä¸º1 (åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Š)\n",
    "    sum_of_weights = attention_weights[0, 0, :].sum()\n",
    "    print(f\"\\nç¬¬ä¸€è¡Œæ³¨æ„åŠ›æƒé‡çš„å’Œ: {sum_of_weights:.4f} (åº”çº¦ç­‰äº 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- è¾“å…¥å¼ é‡å½¢çŠ¶ ---\n",
      "Query (Q) shape: torch.Size([2, 5, 8])\n",
      "Key (K) shape:   torch.Size([2, 5, 8])\n",
      "Value (V) shape: torch.Size([2, 5, 10])\n",
      "--------------------\n",
      "d_k: 8\n",
      "\n",
      "--- è¾“å‡ºç»“æœå½¢çŠ¶ ---\n",
      "Output shape: torch.Size([2, 5, 10])\n",
      "Attention Weights shape: torch.Size([2, 5, 5])\n",
      "--------------------\n",
      "\n",
      "--- è¾“å‡ºå†…å®¹æ£€æŸ¥ ---\n",
      "Output[0, 0, :]:\n",
      " tensor([ 0.9657,  0.2202, -0.5204, -0.5764, -1.5707, -0.3773, -0.9463,  1.3893,\n",
      "         1.2438,  0.4634])\n",
      "\n",
      "ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æƒé‡ (éƒ¨åˆ†):\n",
      " tensor([[0.0000, 0.1387, 0.0465, 0.0915, 0.7841],\n",
      "        [0.3416, 0.0135, 0.0407, 0.6249, 0.0904],\n",
      "        [0.1342, 0.0799, 0.0927, 0.2342, 0.5700],\n",
      "        [0.2875, 0.0857, 0.0203, 0.3677, 0.3498],\n",
      "        [0.2472, 0.2715, 0.1490, 0.3754, 0.0680]])\n",
      "\n",
      "ç¬¬ä¸€è¡Œæ³¨æ„åŠ›æƒé‡çš„å’Œ: 1.0608 (åº”çº¦ç­‰äº 1.0)\n"
     ]
    }
   ],
   "source": [
    "attention_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention_test():\n",
    "    # self attention æ˜¯å¯ä»¥å°†ç›¸åŒçš„ Q, K, V ä¼ å…¥attentionæœºåˆ¶é‡Œ\n",
    "    batch_size = 2      # æ‰¹å¤„ç†å¤§å°\n",
    "    seq_len = 5         # åºåˆ—é•¿åº¦\n",
    "    d_k = 8             # Query å’Œ Key çš„ç»´åº¦ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯è‡ªæ³¨æ„åŠ›ï¼Œå› æ­¤è¿™ä¹Ÿæ˜¯Valueçš„ç»´åº¦\n",
    "    dropout_rate = 0.1  # Dropout æ¯”ç‡\n",
    "\n",
    "    # 1. å®ä¾‹åŒ– Dropout å±‚\n",
    "    dropout_layer = nn.Dropout(p=dropout_rate)\n",
    "    \n",
    "    x = torch.randn(batch_size, seq_len, d_k)\n",
    "    print(f\"x.shape: {x.shape}\")\n",
    "    \n",
    "    # 2. è®¡ç®—è‡ªæ³¨æ„åŠ›\n",
    "    output, p_attn = attention(x, x, x, dropout_layer)\n",
    "    \n",
    "    # 3. æ‰“å°è¾“å‡ºå½¢çŠ¶\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {p_attn.shape}\")\n",
    "    \n",
    "    return output, p_attn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([2, 5, 8])\n",
      "d_k: 8\n",
      "Output shape: torch.Size([2, 5, 8])\n",
      "Attention weights shape: torch.Size([2, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.6683,  0.8487,  0.3628, -1.3097, -0.8950,  0.6976, -1.5617,\n",
       "           -0.6750],\n",
       "          [ 2.4377,  0.0092, -0.2172,  1.1283,  0.5985, -2.2264,  1.5915,\n",
       "           -0.6868],\n",
       "          [-0.8538,  0.0929,  0.2378, -0.3035,  1.2549,  0.0255,  0.4164,\n",
       "            0.3867],\n",
       "          [ 1.5358,  0.2174,  0.0970, -0.2330,  1.0490, -0.1617,  1.1028,\n",
       "            0.4111],\n",
       "          [ 1.0493,  0.3808, -0.1211, -0.8172,  0.7783,  0.2532,  0.8740,\n",
       "            0.6752]],\n",
       " \n",
       "         [[-0.4442, -0.1237, -0.1255, -0.0936,  0.2974,  0.1481, -0.6693,\n",
       "            0.5486],\n",
       "          [-0.4099,  0.6186,  0.3617, -0.8760, -0.7344, -0.4572,  0.2512,\n",
       "           -0.3626],\n",
       "          [-1.2514, -0.2184, -1.0676,  0.4270,  0.5719, -0.3268, -2.2355,\n",
       "            1.5484],\n",
       "          [-0.4894,  0.0703,  0.0202, -0.4927, -0.7036, -0.4252,  0.3350,\n",
       "           -0.3648],\n",
       "          [-0.2227, -0.7650,  0.9707, -1.1396,  1.5226,  1.9492, -0.5938,\n",
       "            0.7727]]]),\n",
       " tensor([[[9.9057e-01, 5.8785e-03, 4.4457e-02, 1.9656e-02, 0.0000e+00],\n",
       "          [7.7059e-04, 1.0608e+00, 3.3891e-03, 3.6190e-02, 9.9337e-03],\n",
       "          [8.0982e-02, 4.7095e-02, 7.4547e-01, 1.2453e-01, 1.1303e-01],\n",
       "          [1.7573e-02, 2.4683e-01, 6.1122e-02, 5.5050e-01, 2.3508e-01],\n",
       "          [6.2924e-02, 9.4332e-02, 7.7243e-02, 3.2731e-01, 5.4930e-01]],\n",
       " \n",
       "         [[0.0000e+00, 3.1440e-02, 3.0283e-01, 6.7405e-02, 1.6605e-01],\n",
       "          [4.2741e-02, 6.8040e-01, 5.4594e-02, 2.6424e-01, 6.9143e-02],\n",
       "          [7.2719e-02, 9.6434e-03, 9.8833e-01, 9.2186e-03, 3.1202e-02],\n",
       "          [1.1818e-01, 3.4079e-01, 6.7310e-02, 5.3324e-01, 5.1586e-02],\n",
       "          [5.0875e-02, 1.5583e-02, 3.9811e-02, 9.0146e-03, 9.9583e-01]]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 512                    # æ¨¡å‹ç»´åº¦\n",
    "    n_heads: int = 8                  # æ³¨æ„åŠ›å¤´æ•°\n",
    "    dropout: float = 0.1              # Dropout æ¯”ç‡\n",
    "    max_seq_len: int = 2048           # æœ€å¤§åºåˆ—é•¿åº¦\n",
    "\n",
    "# ä¿®å¤ MultiHeadAttention ç±»ä¸­çš„ä¸€ä¸ªå°é”™è¯¯\n",
    "'''å¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—æ¨¡å—'''\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelArgs, is_causal=False):\n",
    "        # æ„é€ å‡½æ•°\n",
    "        # args: é…ç½®å¯¹è±¡\n",
    "        super().__init__()\n",
    "        # éšè—å±‚ç»´åº¦å¿…é¡»æ˜¯å¤´æ•°çš„æ•´æ•°å€ï¼Œå› ä¸ºåé¢æˆ‘ä»¬ä¼šå°†è¾“å…¥æ‹†æˆå¤´æ•°ä¸ªçŸ©é˜µ\n",
    "        assert args.dim % args.n_heads == 0\n",
    "        # æ¨¡å‹å¹¶è¡Œå¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸º1ã€‚\n",
    "        model_parallel_size = 1\n",
    "        # æœ¬åœ°è®¡ç®—å¤´æ•°ï¼Œç­‰äºæ€»å¤´æ•°é™¤ä»¥æ¨¡å‹å¹¶è¡Œå¤„ç†å¤§å°ã€‚\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        # æ¯ä¸ªå¤´çš„ç»´åº¦ï¼Œç­‰äºæ¨¡å‹ç»´åº¦é™¤ä»¥å¤´çš„æ€»æ•°ã€‚\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        # ä¿å­˜æ˜¯å¦ä½¿ç”¨å› æœæ©ç \n",
    "        self.is_causal = is_causal\n",
    "\n",
    "        # Wq, Wk, Wv å‚æ•°çŸ©é˜µï¼Œæ¯ä¸ªå‚æ•°çŸ©é˜µä¸º n_embd x n_embd\n",
    "        # è¿™é‡Œé€šè¿‡ä¸‰ä¸ªç»„åˆçŸ©é˜µæ¥ä»£æ›¿äº†nä¸ªå‚æ•°çŸ©é˜µçš„ç»„åˆï¼Œå…¶é€»è¾‘åœ¨äºçŸ©é˜µå†…ç§¯å†æ‹¼æ¥å…¶å®ç­‰åŒäºæ‹¼æ¥çŸ©é˜µå†å†…ç§¯ï¼Œ\n",
    "        # ä¸ç†è§£çš„è¯»è€…å¯ä»¥è‡ªè¡Œæ¨¡æ‹Ÿä¸€ä¸‹ï¼Œæ¯ä¸€ä¸ªçº¿æ€§å±‚å…¶å®ç›¸å½“äºnä¸ªå‚æ•°çŸ©é˜µçš„æ‹¼æ¥\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        # è¾“å‡ºæƒé‡çŸ©é˜µï¼Œç»´åº¦ä¸º dim x n_embdï¼ˆhead_dim = n_embeds / n_headsï¼‰\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "        # æ³¨æ„åŠ›çš„ dropout\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        # æ®‹å·®è¿æ¥çš„ dropout\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "         \n",
    "        # åˆ›å»ºä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µï¼Œç”¨äºé®è”½æœªæ¥ä¿¡æ¯\n",
    "        # æ³¨æ„ï¼Œå› ä¸ºæ˜¯å¤šå¤´æ³¨æ„åŠ›ï¼ŒMask çŸ©é˜µæ¯”ä¹‹å‰æˆ‘ä»¬å®šä¹‰çš„å¤šä¸€ä¸ªç»´åº¦\n",
    "        if is_causal:\n",
    "           mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))\n",
    "           mask = torch.triu(mask, diagonal=1)\n",
    "           # æ³¨å†Œä¸ºæ¨¡å‹çš„ç¼“å†²åŒº\n",
    "           self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
    "\n",
    "        # è·å–æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦ï¼Œ[batch_size, seq_len, dim]\n",
    "        bsz, seqlen, _ = q.shape\n",
    "\n",
    "        # è®¡ç®—æŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰ã€å€¼ï¼ˆVï¼‰,è¾“å…¥é€šè¿‡å‚æ•°çŸ©é˜µå±‚ï¼Œç»´åº¦ä¸º (B, T, n_embed) x (n_embed, n_embed) -> (B, T, n_embed)\n",
    "        xq, xk, xv = self.wq(q), self.wk(k), self.wv(v)\n",
    "        print(f\"After linear transformation:\")\n",
    "        print(f\"  xq shape: {xq.shape}\")\n",
    "        print(f\"  xk shape: {xk.shape}\")\n",
    "        print(f\"  xv shape: {xv.shape}\")\n",
    "\n",
    "        # å°† Qã€Kã€V æ‹†åˆ†æˆå¤šå¤´ï¼Œç»´åº¦ä¸º (B, T, n_head, C // n_head)ï¼Œç„¶åäº¤æ¢ç»´åº¦ï¼Œå˜æˆ (B, n_head, T, C // n_head)\n",
    "        # å› ä¸ºåœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­æˆ‘ä»¬æ˜¯å–äº†åä¸¤ä¸ªç»´åº¦å‚ä¸è®¡ç®—\n",
    "        # ä¸ºä»€ä¹ˆè¦å…ˆæŒ‰B*T*n_head*C//n_headå±•å¼€å†äº’æ¢1ã€2ç»´åº¦è€Œä¸æ˜¯ç›´æ¥æŒ‰æ³¨æ„åŠ›è¾“å…¥å±•å¼€ï¼Œæ˜¯å› ä¸ºviewçš„å±•å¼€æ–¹å¼æ˜¯ç›´æ¥æŠŠè¾“å…¥å…¨éƒ¨æ’å¼€ï¼Œ\n",
    "        # ç„¶åæŒ‰è¦æ±‚æ„é€ ï¼Œå¯ä»¥å‘ç°åªæœ‰ä¸Šè¿°æ“ä½œèƒ½å¤Ÿå®ç°æˆ‘ä»¬å°†æ¯ä¸ªå¤´å¯¹åº”éƒ¨åˆ†å–å‡ºæ¥çš„ç›®æ ‡\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        print(f\"After view (reshape):\")\n",
    "        print(f\"  xq shape: {xq.shape}\")\n",
    "        print(f\"  xk shape: {xk.shape}\")\n",
    "        print(f\"  xv shape: {xv.shape}\")\n",
    "        \n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "        print(f\"After transpose:\")\n",
    "        print(f\"  xq shape: {xq.shape}\")\n",
    "        print(f\"  xk shape: {xk.shape}\")\n",
    "        print(f\"  xv shape: {xv.shape}\")\n",
    "\n",
    "\n",
    "        # æ³¨æ„åŠ›è®¡ç®—\n",
    "        # è®¡ç®— QK^T / sqrt(d_k)ï¼Œç»´åº¦ä¸º (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        print(f\"Attention scores shape: {scores.shape}\")\n",
    "        \n",
    "        # æ©ç è‡ªæ³¨æ„åŠ›å¿…é¡»æœ‰æ³¨æ„åŠ›æ©ç \n",
    "        if self.is_causal:\n",
    "            assert hasattr(self, 'mask')\n",
    "            # è¿™é‡Œæˆªå–åˆ°åºåˆ—é•¿åº¦ï¼Œå› ä¸ºæœ‰äº›åºåˆ—å¯èƒ½æ¯” max_seq_len çŸ­\n",
    "            scores = scores + self.mask[:, :, :seqlen, :seqlen]\n",
    "            print(f\"After applying causal mask\")\n",
    "            \n",
    "        # è®¡ç®— softmaxï¼Œç»´åº¦ä¸º (B, nh, T, T)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        # åš Dropout\n",
    "        scores = self.attn_dropout(scores)\n",
    "        print(f\"Attention weights shape: {scores.shape}\")\n",
    "        \n",
    "        # V * Scoreï¼Œç»´åº¦ä¸º(B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        output = torch.matmul(scores, xv)\n",
    "        print(f\"After attention output shape: {output.shape}\")\n",
    "\n",
    "        # æ¢å¤æ—¶é—´ç»´åº¦å¹¶åˆå¹¶å¤´ã€‚\n",
    "        # å°†å¤šå¤´çš„ç»“æœæ‹¼æ¥èµ·æ¥, å…ˆäº¤æ¢ç»´åº¦ä¸º (B, T, n_head, C // n_head)ï¼Œå†æ‹¼æ¥æˆ (B, T, n_head * C // n_head)\n",
    "        # contiguous å‡½æ•°ç”¨äºé‡æ–°å¼€è¾Ÿä¸€å—æ–°å†…å­˜å­˜å‚¨ï¼Œå› ä¸ºPytorchè®¾ç½®å…ˆtransposeå†viewä¼šæŠ¥é”™ï¼Œ\n",
    "        # å› ä¸ºviewç›´æ¥åŸºäºåº•å±‚å­˜å‚¨å¾—åˆ°ï¼Œç„¶è€Œtransposeå¹¶ä¸ä¼šæ”¹å˜åº•å±‚å­˜å‚¨ï¼Œå› æ­¤éœ€è¦é¢å¤–å­˜å‚¨\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        print(f\"After merging heads shape: {output.shape}\")\n",
    "\n",
    "        # æœ€ç»ˆæŠ•å½±å›æ®‹å·®æµã€‚\n",
    "        output = self.wo(output)\n",
    "        output = self.resid_dropout(output)\n",
    "        print(f\"Final output shape: {output.shape}\")\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ¼”ç¤º\n",
      "==================================================\n",
      "é…ç½®å‚æ•°:\n",
      "  æ¨¡å‹ç»´åº¦ (dim): 512\n",
      "  æ³¨æ„åŠ›å¤´æ•° (n_heads): 8\n",
      "  æ¯ä¸ªå¤´çš„ç»´åº¦ (head_dim): 64\n",
      "  dropoutç‡: 0.1\n",
      "\n",
      "åˆ›å»ºä¸¤ä¸ªå¤šå¤´æ³¨æ„åŠ›æ¨¡å—:\n",
      "  - æ™®é€šå¤šå¤´æ³¨æ„åŠ› (åŒå‘)\n",
      "  - å› æœå¤šå¤´æ³¨æ„åŠ› (å•å‘ï¼Œç”¨äºè§£ç å™¨)\n",
      "\n",
      "è¾“å…¥æ•°æ®:\n",
      "  è¾“å…¥å½¢çŠ¶: torch.Size([2, 10, 512])\n",
      "  å«ä¹‰: [batch_size=2, seq_len=10, dim=512]\n",
      "\n",
      "==============================\n",
      "æ™®é€šå¤šå¤´æ³¨æ„åŠ› (è‡ªæ³¨æ„åŠ›)\n",
      "==============================\n",
      "After linear transformation:\n",
      "  xq shape: torch.Size([2, 10, 512])\n",
      "  xk shape: torch.Size([2, 10, 512])\n",
      "  xv shape: torch.Size([2, 10, 512])\n",
      "After view (reshape):\n",
      "  xq shape: torch.Size([2, 10, 8, 64])\n",
      "  xk shape: torch.Size([2, 10, 8, 64])\n",
      "  xv shape: torch.Size([2, 10, 8, 64])\n",
      "After transpose:\n",
      "  xq shape: torch.Size([2, 8, 10, 64])\n",
      "  xk shape: torch.Size([2, 8, 10, 64])\n",
      "  xv shape: torch.Size([2, 8, 10, 64])\n",
      "Attention scores shape: torch.Size([2, 8, 10, 10])\n",
      "Attention weights shape: torch.Size([2, 8, 10, 10])\n",
      "After attention output shape: torch.Size([2, 8, 10, 64])\n",
      "After merging heads shape: torch.Size([2, 10, 512])\n",
      "Final output shape: torch.Size([2, 10, 512])\n",
      "\\nè¾“å‡ºç»“æœ:\n",
      "  è¾“å‡ºå½¢çŠ¶: torch.Size([2, 10, 512])\n",
      "  è¾“å‡ºæ˜¯å¦ç­‰äºè¾“å…¥å½¢çŠ¶: True\n",
      "\n",
      "==============================\n",
      "å› æœå¤šå¤´æ³¨æ„åŠ› (å¸¦æ©ç )\n",
      "==============================\n",
      "After linear transformation:\n",
      "  xq shape: torch.Size([2, 10, 512])\n",
      "  xk shape: torch.Size([2, 10, 512])\n",
      "  xv shape: torch.Size([2, 10, 512])\n",
      "After view (reshape):\n",
      "  xq shape: torch.Size([2, 10, 8, 64])\n",
      "  xk shape: torch.Size([2, 10, 8, 64])\n",
      "  xv shape: torch.Size([2, 10, 8, 64])\n",
      "After transpose:\n",
      "  xq shape: torch.Size([2, 8, 10, 64])\n",
      "  xk shape: torch.Size([2, 8, 10, 64])\n",
      "  xv shape: torch.Size([2, 8, 10, 64])\n",
      "Attention scores shape: torch.Size([2, 8, 10, 10])\n",
      "After applying causal mask\n",
      "Attention weights shape: torch.Size([2, 8, 10, 10])\n",
      "After attention output shape: torch.Size([2, 8, 10, 64])\n",
      "After merging heads shape: torch.Size([2, 10, 512])\n",
      "Final output shape: torch.Size([2, 10, 512])\n",
      "\\nè¾“å‡ºç»“æœ:\n",
      "  è¾“å‡ºå½¢çŠ¶: torch.Size([2, 10, 512])\n",
      "  è¾“å‡ºæ˜¯å¦ç­‰äºè¾“å…¥å½¢çŠ¶: True\n",
      "\n",
      "==============================\n",
      "è¾“å‡ºæ¯”è¾ƒ\n",
      "==============================\n",
      "æ™®é€šæ³¨æ„åŠ›ä¸å› æœæ³¨æ„åŠ›è¾“å‡ºçš„å¹³å‡ç»å¯¹å·®å¼‚: 0.167131\n",
      "(å·®å¼‚åº”è¯¥å¾ˆå¤§ï¼Œå› ä¸ºå› æœæ³¨æ„åŠ›åªèƒ½çœ‹åˆ°è¿‡å»çš„ä¿¡æ¯)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-3.5882e-03,  3.4845e-01, -1.5955e-01,  ..., -5.2484e-02,\n",
       "           -5.0627e-02, -4.3429e-02],\n",
       "          [-4.2136e-02,  2.9440e-01, -2.5072e-01,  ..., -6.8263e-02,\n",
       "            6.0080e-02,  9.8534e-03],\n",
       "          [-4.6872e-02,  2.9721e-01, -1.5375e-01,  ..., -3.9070e-02,\n",
       "           -1.0439e-02, -1.0417e-02],\n",
       "          ...,\n",
       "          [ 2.0015e-02,  3.5448e-01, -2.0549e-01,  ..., -6.1390e-02,\n",
       "           -4.2314e-02, -4.7186e-05],\n",
       "          [-1.0270e-01,  3.2186e-01, -2.1751e-01,  ..., -4.9039e-02,\n",
       "            1.4546e-02,  4.3820e-02],\n",
       "          [-3.0241e-02,  2.6220e-01, -2.4302e-01,  ..., -1.0419e-01,\n",
       "            5.5745e-02, -6.3002e-02]],\n",
       " \n",
       "         [[-2.1459e-02,  3.7747e-02, -5.3677e-02,  ...,  1.2905e-01,\n",
       "           -2.2530e-01,  1.3839e-01],\n",
       "          [-1.1947e-01,  6.3499e-02, -1.5479e-01,  ...,  8.2109e-02,\n",
       "           -2.2436e-01,  1.0165e-01],\n",
       "          [-4.5769e-02,  8.1421e-02, -1.5156e-01,  ...,  9.5839e-02,\n",
       "           -2.2262e-01,  1.0562e-01],\n",
       "          ...,\n",
       "          [-5.1230e-02,  7.6579e-02, -2.1101e-02,  ...,  1.3839e-01,\n",
       "           -2.4152e-01,  7.9099e-02],\n",
       "          [-6.0678e-02,  6.7656e-02, -8.0606e-02,  ...,  1.2799e-01,\n",
       "           -2.0220e-01,  1.8050e-01],\n",
       "          [-7.3476e-02,  1.0921e-01, -8.7793e-02,  ...,  1.5827e-01,\n",
       "           -1.6783e-01,  8.7738e-02]]]),\n",
       " tensor([[[ 0.2968, -0.6895,  0.0992,  ..., -0.0096, -0.1886, -0.1755],\n",
       "          [ 0.2990, -0.1890, -0.1468,  ...,  0.0517, -0.0698, -0.1215],\n",
       "          [ 0.2263, -0.0433, -0.2526,  ...,  0.2171, -0.0802, -0.2265],\n",
       "          ...,\n",
       "          [ 0.1037, -0.0130, -0.2102,  ...,  0.2062, -0.0918,  0.0769],\n",
       "          [ 0.1417,  0.0124, -0.2317,  ...,  0.1996, -0.0641,  0.1701],\n",
       "          [ 0.1257, -0.1130, -0.2520,  ...,  0.0516, -0.0790,  0.1909]],\n",
       " \n",
       "         [[ 0.8599,  0.2112, -0.0909,  ...,  0.1156, -0.2621,  0.5401],\n",
       "          [ 0.4825,  0.1367, -0.0473,  ..., -0.0093, -0.2930,  0.3162],\n",
       "          [ 0.5302,  0.0890, -0.2057,  ..., -0.1416, -0.1877,  0.2160],\n",
       "          ...,\n",
       "          [ 0.2764,  0.0302,  0.0328,  ...,  0.0305,  0.0386,  0.1637],\n",
       "          [ 0.2677, -0.1140, -0.0871,  ..., -0.0559, -0.0019,  0.1379],\n",
       "          [ 0.2162, -0.0943, -0.1128,  ...,  0.0596,  0.0670,  0.1619]]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multi_head_attention_demo():\n",
    "    \"\"\"\n",
    "    å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å®Œæ•´æ¼”ç¤º\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ¼”ç¤º\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. åˆ›å»ºé…ç½®å‚æ•°\n",
    "    args = ModelArgs(\n",
    "        dim=512,         # æ¨¡å‹ç»´åº¦\n",
    "        n_heads=8,       # 8ä¸ªæ³¨æ„åŠ›å¤´\n",
    "        dropout=0.1,     # dropoutç‡\n",
    "        max_seq_len=2048 # æœ€å¤§åºåˆ—é•¿åº¦\n",
    "    )\n",
    "    \n",
    "    print(f\"é…ç½®å‚æ•°:\")\n",
    "    print(f\"  æ¨¡å‹ç»´åº¦ (dim): {args.dim}\")\n",
    "    print(f\"  æ³¨æ„åŠ›å¤´æ•° (n_heads): {args.n_heads}\")\n",
    "    print(f\"  æ¯ä¸ªå¤´çš„ç»´åº¦ (head_dim): {args.dim // args.n_heads}\")\n",
    "    print(f\"  dropoutç‡: {args.dropout}\")\n",
    "    print()\n",
    "    \n",
    "    # 2. åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›æ¨¡å—\n",
    "    # æˆ‘ä»¬åˆ›å»ºä¸¤ä¸ªç‰ˆæœ¬ï¼šä¸€ä¸ªæ™®é€šçš„ï¼Œä¸€ä¸ªå¸¦å› æœæ©ç çš„\n",
    "    print(\"åˆ›å»ºä¸¤ä¸ªå¤šå¤´æ³¨æ„åŠ›æ¨¡å—:\")\n",
    "    mha_normal = MultiHeadAttention(args, is_causal=False)\n",
    "    mha_causal = MultiHeadAttention(args, is_causal=True)\n",
    "    print(\"  - æ™®é€šå¤šå¤´æ³¨æ„åŠ› (åŒå‘)\")\n",
    "    print(\"  - å› æœå¤šå¤´æ³¨æ„åŠ› (å•å‘ï¼Œç”¨äºè§£ç å™¨)\")\n",
    "    print()\n",
    "    \n",
    "    # 3. åˆ›å»ºè¾“å…¥æ•°æ®\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    \n",
    "    # åˆ›å»ºéšæœºè¾“å…¥å¼ é‡\n",
    "    x = torch.randn(batch_size, seq_len, args.dim)\n",
    "    print(f\"è¾“å…¥æ•°æ®:\")\n",
    "    print(f\"  è¾“å…¥å½¢çŠ¶: {x.shape}\")\n",
    "    print(f\"  å«ä¹‰: [batch_size={batch_size}, seq_len={seq_len}, dim={args.dim}]\")\n",
    "    print()\n",
    "    \n",
    "    # 4. æ¼”ç¤ºæ™®é€šå¤šå¤´æ³¨æ„åŠ› (è‡ªæ³¨æ„åŠ›)\n",
    "    print(\"=\" * 30)\n",
    "    print(\"æ™®é€šå¤šå¤´æ³¨æ„åŠ› (è‡ªæ³¨æ„åŠ›)\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ä»¥é¿å…éšæœºæ€§\n",
    "    mha_normal.eval()\n",
    "    with torch.no_grad():\n",
    "        output_normal = mha_normal(x, x, x)  # è‡ªæ³¨æ„åŠ›ï¼šQ=K=V=x\n",
    "        \n",
    "    print(f\"\\\\nè¾“å‡ºç»“æœ:\")\n",
    "    print(f\"  è¾“å‡ºå½¢çŠ¶: {output_normal.shape}\")\n",
    "    print(f\"  è¾“å‡ºæ˜¯å¦ç­‰äºè¾“å…¥å½¢çŠ¶: {output_normal.shape == x.shape}\")\n",
    "    print()\n",
    "    \n",
    "    # 5. æ¼”ç¤ºå› æœå¤šå¤´æ³¨æ„åŠ›\n",
    "    print(\"=\" * 30)\n",
    "    print(\"å› æœå¤šå¤´æ³¨æ„åŠ› (å¸¦æ©ç )\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    mha_causal.eval()\n",
    "    with torch.no_grad():\n",
    "        output_causal = mha_causal(x, x, x)  # å› æœè‡ªæ³¨æ„åŠ›\n",
    "        \n",
    "    print(f\"\\\\nè¾“å‡ºç»“æœ:\")\n",
    "    print(f\"  è¾“å‡ºå½¢çŠ¶: {output_causal.shape}\")\n",
    "    print(f\"  è¾“å‡ºæ˜¯å¦ç­‰äºè¾“å…¥å½¢çŠ¶: {output_causal.shape == x.shape}\")\n",
    "    print()\n",
    "    \n",
    "    # 6. æ¯”è¾ƒä¸¤ä¸ªè¾“å‡º\n",
    "    print(\"=\" * 30)\n",
    "    print(\"è¾“å‡ºæ¯”è¾ƒ\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # è®¡ç®—è¾“å‡ºçš„å·®å¼‚\n",
    "    diff = torch.abs(output_normal - output_causal).mean()\n",
    "    print(f\"æ™®é€šæ³¨æ„åŠ›ä¸å› æœæ³¨æ„åŠ›è¾“å‡ºçš„å¹³å‡ç»å¯¹å·®å¼‚: {diff:.6f}\")\n",
    "    print(\"(å·®å¼‚åº”è¯¥å¾ˆå¤§ï¼Œå› ä¸ºå› æœæ³¨æ„åŠ›åªèƒ½çœ‹åˆ°è¿‡å»çš„ä¿¡æ¯)\")\n",
    "    \n",
    "    return output_normal, output_causal\n",
    "\n",
    "# è¿è¡Œæ¼”ç¤º\n",
    "multi_head_attention_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "è¯¦ç»†ç»´åº¦å˜åŒ–æ¼”ç¤º\n",
      "============================================================\n",
      "å°è§„æ¨¡é…ç½®å‚æ•°:\n",
      "  æ¨¡å‹ç»´åº¦ (dim): 64\n",
      "  æ³¨æ„åŠ›å¤´æ•° (n_heads): 4\n",
      "  æ¯ä¸ªå¤´çš„ç»´åº¦ (head_dim): 16\n",
      "\n",
      "è¾“å…¥æ•°æ®:\n",
      "  è¾“å…¥å½¢çŠ¶: torch.Size([1, 3, 64])\n",
      "  å«ä¹‰: [batch_size=1, seq_len=3, dim=64]\n",
      "  è¿™è¡¨ç¤ºï¼š1ä¸ªæ ·æœ¬ï¼Œ3ä¸ªè¯ï¼Œæ¯ä¸ªè¯64ç»´ç‰¹å¾\n",
      "\n",
      "========================================\n",
      "é€æ­¥æ‰§è¡Œå‰å‘ä¼ æ’­...\n",
      "========================================\n",
      "After linear transformation:\n",
      "  xq shape: torch.Size([1, 3, 64])\n",
      "  xk shape: torch.Size([1, 3, 64])\n",
      "  xv shape: torch.Size([1, 3, 64])\n",
      "After view (reshape):\n",
      "  xq shape: torch.Size([1, 3, 4, 16])\n",
      "  xk shape: torch.Size([1, 3, 4, 16])\n",
      "  xv shape: torch.Size([1, 3, 4, 16])\n",
      "After transpose:\n",
      "  xq shape: torch.Size([1, 4, 3, 16])\n",
      "  xk shape: torch.Size([1, 4, 3, 16])\n",
      "  xv shape: torch.Size([1, 4, 3, 16])\n",
      "Attention scores shape: torch.Size([1, 4, 3, 3])\n",
      "Attention weights shape: torch.Size([1, 4, 3, 3])\n",
      "After attention output shape: torch.Size([1, 4, 3, 16])\n",
      "After merging heads shape: torch.Size([1, 3, 64])\n",
      "Final output shape: torch.Size([1, 3, 64])\n",
      "\\n========================================\n",
      "æ€»ç»“\n",
      "========================================\n",
      "æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: torch.Size([1, 3, 64])\n",
      "è¾“å…¥è¾“å‡ºå½¢çŠ¶ä¸€è‡´: True\n",
      "è¿™æ„å‘³ç€æ¯ä¸ªè¯ä»ç„¶æ˜¯64ç»´ç‰¹å¾ï¼Œä½†ç°åœ¨åŒ…å«äº†å…¶ä»–è¯çš„ä¿¡æ¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1248, -0.4088, -0.1043, -0.1492, -0.1853, -0.1101, -0.0196,\n",
       "          -0.1588, -0.3738,  0.1675, -0.1530,  0.2170, -0.2289,  0.2016,\n",
       "           0.1170, -0.0964,  0.2748, -0.1827,  0.0985,  0.0850, -0.0833,\n",
       "           0.0658,  0.2059, -0.0165,  0.0487, -0.2841, -0.1550, -0.0300,\n",
       "           0.2595,  0.2782, -0.2102,  0.0638, -0.1863,  0.1755,  0.0742,\n",
       "           0.0574,  0.1980, -0.0122, -0.1676,  0.1484,  0.2180, -0.3638,\n",
       "           0.2751, -0.0568,  0.0623,  0.1186, -0.4737,  0.0195, -0.0804,\n",
       "          -0.1568,  0.2000,  0.1656, -0.0141, -0.1310,  0.0199, -0.0208,\n",
       "          -0.0608,  0.3282,  0.2732, -0.0519, -0.0837,  0.0963, -0.2658,\n",
       "          -0.1464],\n",
       "         [ 0.1496, -0.3596, -0.1726, -0.1457, -0.0202, -0.1257, -0.0626,\n",
       "          -0.2295, -0.3345,  0.2137, -0.2111,  0.0920, -0.1474,  0.2324,\n",
       "          -0.0257, -0.0202,  0.3253, -0.1405,  0.1100, -0.0922, -0.1377,\n",
       "           0.0540,  0.2066, -0.0022, -0.0961, -0.3093, -0.2066, -0.0564,\n",
       "           0.2395,  0.1950, -0.2124,  0.0709, -0.1287,  0.1223,  0.0484,\n",
       "           0.0862,  0.1778, -0.1013, -0.0725,  0.0510,  0.1539, -0.3173,\n",
       "           0.3033,  0.0046,  0.0632,  0.1564, -0.3630, -0.0772, -0.2274,\n",
       "          -0.1879,  0.2207,  0.2712, -0.1637, -0.0256,  0.0306, -0.1711,\n",
       "          -0.0779,  0.3288,  0.3063,  0.1163, -0.0967,  0.0704, -0.2757,\n",
       "          -0.1488],\n",
       "         [ 0.0823, -0.3671, -0.1969, -0.2487, -0.0743, -0.1486, -0.1499,\n",
       "          -0.1867, -0.3895,  0.2331, -0.2279,  0.0590, -0.1441,  0.2552,\n",
       "          -0.0307, -0.0769,  0.2446, -0.0753,  0.0862, -0.1203, -0.1632,\n",
       "           0.0617,  0.2425,  0.0137,  0.0227, -0.3594, -0.2321,  0.0096,\n",
       "           0.2849,  0.2600, -0.1448,  0.0883, -0.1078,  0.0479, -0.0029,\n",
       "           0.0893,  0.1025, -0.0568, -0.0530,  0.0266,  0.1854, -0.3491,\n",
       "           0.4324,  0.1125,  0.0306,  0.0994, -0.4280, -0.0492, -0.1397,\n",
       "          -0.1153,  0.1986,  0.2914, -0.2357, -0.0965,  0.0797, -0.1708,\n",
       "          -0.1198,  0.2441,  0.3305,  0.1231,  0.0757,  0.0222, -0.2933,\n",
       "          -0.1406]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detailed_dimension_demo():\n",
    "    \"\"\"\n",
    "    è¯¦ç»†çš„ç»´åº¦å˜åŒ–æ¼”ç¤º - ä½¿ç”¨å°è§„æ¨¡æ•°æ®ä¾¿äºç†è§£\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"è¯¦ç»†ç»´åº¦å˜åŒ–æ¼”ç¤º\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ä½¿ç”¨å°è§„æ¨¡å‚æ•°ä¾¿äºç†è§£\n",
    "    args = ModelArgs(\n",
    "        dim=64,          # å°çš„æ¨¡å‹ç»´åº¦\n",
    "        n_heads=4,       # 4ä¸ªæ³¨æ„åŠ›å¤´\n",
    "        dropout=0.0,     # ä¸ä½¿ç”¨dropoutä¾¿äºæ¼”ç¤º\n",
    "        max_seq_len=10   # å°çš„æœ€å¤§åºåˆ—é•¿åº¦\n",
    "    )\n",
    "    \n",
    "    print(f\"å°è§„æ¨¡é…ç½®å‚æ•°:\")\n",
    "    print(f\"  æ¨¡å‹ç»´åº¦ (dim): {args.dim}\")\n",
    "    print(f\"  æ³¨æ„åŠ›å¤´æ•° (n_heads): {args.n_heads}\")\n",
    "    print(f\"  æ¯ä¸ªå¤´çš„ç»´åº¦ (head_dim): {args.dim // args.n_heads}\")\n",
    "    print()\n",
    "    \n",
    "    # åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›æ¨¡å—\n",
    "    mha = MultiHeadAttention(args, is_causal=False)\n",
    "    mha.eval()\n",
    "    \n",
    "    # åˆ›å»ºå°è§„æ¨¡è¾“å…¥æ•°æ®\n",
    "    batch_size = 1  # å•ä¸ªæ‰¹æ¬¡\n",
    "    seq_len = 3     # 3ä¸ªè¯çš„åºåˆ—\n",
    "    \n",
    "    x = torch.randn(batch_size, seq_len, args.dim)\n",
    "    print(f\"è¾“å…¥æ•°æ®:\")\n",
    "    print(f\"  è¾“å…¥å½¢çŠ¶: {x.shape}\")\n",
    "    print(f\"  å«ä¹‰: [batch_size={batch_size}, seq_len={seq_len}, dim={args.dim}]\")\n",
    "    print(f\"  è¿™è¡¨ç¤ºï¼š1ä¸ªæ ·æœ¬ï¼Œ3ä¸ªè¯ï¼Œæ¯ä¸ªè¯64ç»´ç‰¹å¾\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=\" * 40)\n",
    "    print(\"é€æ­¥æ‰§è¡Œå‰å‘ä¼ æ’­...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = mha(x, x, x)  # è‡ªæ³¨æ„åŠ›\n",
    "        \n",
    "    print(\"\\\\n\" + \"=\" * 40)\n",
    "    print(\"æ€»ç»“\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: {output.shape}\")\n",
    "    print(f\"è¾“å…¥è¾“å‡ºå½¢çŠ¶ä¸€è‡´: {output.shape == x.shape}\")\n",
    "    print(f\"è¿™æ„å‘³ç€æ¯ä¸ªè¯ä»ç„¶æ˜¯64ç»´ç‰¹å¾ï¼Œä½†ç°åœ¨åŒ…å«äº†å…¶ä»–è¯çš„ä¿¡æ¯\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "# è¿è¡Œè¯¦ç»†æ¼”ç¤º\n",
    "detailed_dimension_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "éšè—ç»´åº¦ï¼ˆHidden Dimensionï¼‰è¯¦è§£\n",
      "============================================================\n",
      "1. åŸºæœ¬æ¦‚å¿µï¼š\n",
      "   éšè—ç»´åº¦ = æ¯ä¸ªè¯/token çš„ç‰¹å¾å‘é‡çš„é•¿åº¦\n",
      "   æ›´å¤§çš„éšè—ç»´åº¦ = æ›´ä¸°å¯Œçš„è¡¨ç¤ºèƒ½åŠ›\n",
      "\n",
      "2. ä¸åŒéšè—ç»´åº¦çš„å¯¹æ¯”ï¼š\n",
      "   å°éšè—ç»´åº¦ (dim=8): torch.Size([1, 4, 8])\n",
      "   å¤§éšè—ç»´åº¦ (dim=512): torch.Size([1, 4, 512])\n",
      "\n",
      "   è§£é‡Šï¼š\n",
      "   - å°ç»´åº¦ï¼šæ¯ä¸ªè¯ç”¨ 8 ä¸ªæ•°å­—è¡¨ç¤º\n",
      "   - å¤§ç»´åº¦ï¼šæ¯ä¸ªè¯ç”¨ 512 ä¸ªæ•°å­—è¡¨ç¤º\n",
      "   - æ›´å¤šæ•°å­— = æ›´ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤º\n",
      "\n",
      "3. éšè—ç»´åº¦ä¸å¤šå¤´æ³¨æ„åŠ›çš„å…³ç³»ï¼š\n",
      "   æ ¸å¿ƒåŸç†ï¼šéšè—ç»´åº¦å¿…é¡»èƒ½è¢«å¤´æ•°æ•´é™¤\n",
      "\n",
      "   é…ç½®: dim=64, n_heads=8\n",
      "   -> æ¯ä¸ªå¤´çš„ç»´åº¦ = 64 Ã· 8 = 8\n",
      "   -> æ„ä¹‰ï¼šå°† 64 ç»´ç‰¹å¾åˆ†æˆ 8 ä¸ª 8 ç»´çš„å­ç©ºé—´\n",
      "\n",
      "   é…ç½®: dim=512, n_heads=8\n",
      "   -> æ¯ä¸ªå¤´çš„ç»´åº¦ = 512 Ã· 8 = 64\n",
      "   -> æ„ä¹‰ï¼šå°† 512 ç»´ç‰¹å¾åˆ†æˆ 8 ä¸ª 64 ç»´çš„å­ç©ºé—´\n",
      "\n",
      "   é…ç½®: dim=512, n_heads=16\n",
      "   -> æ¯ä¸ªå¤´çš„ç»´åº¦ = 512 Ã· 16 = 32\n",
      "   -> æ„ä¹‰ï¼šå°† 512 ç»´ç‰¹å¾åˆ†æˆ 16 ä¸ª 32 ç»´çš„å­ç©ºé—´\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.4122,  0.5732, -0.3689, -0.6788, -1.4379,  1.1635, -0.6507,\n",
       "            1.2775],\n",
       "          [ 0.5263,  1.2405,  0.5946,  0.4822,  0.9842, -0.2387, -0.5439,\n",
       "            0.8796],\n",
       "          [ 1.4528,  0.1149, -0.1737, -0.0449, -0.4003,  0.5686, -0.1289,\n",
       "           -0.4169],\n",
       "          [-0.3138, -0.4388,  0.7239,  0.3780,  1.5564,  0.5695,  2.5011,\n",
       "           -2.2259]]]),\n",
       " tensor([[[ 1.4372, -0.3388, -0.9061,  ..., -1.2761, -0.3965,  0.3438],\n",
       "          [ 0.9279, -0.2351,  1.1933,  ...,  2.1610, -1.4266, -0.0313],\n",
       "          [-0.5602,  0.5114,  0.1775,  ...,  0.2276, -2.6811,  0.0305],\n",
       "          [-2.2060,  0.4727, -2.2430,  ...,  0.2253,  2.7395,  0.2348]]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def explain_hidden_dimension():\n",
    "    \"\"\"\n",
    "    è¯¦ç»†è§£é‡Šéšè—ç»´åº¦çš„æ¦‚å¿µå’Œä½œç”¨\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"éšè—ç»´åº¦ï¼ˆHidden Dimensionï¼‰è¯¦è§£\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. åŸºæœ¬æ¦‚å¿µæ¼”ç¤º\n",
    "    print(\"1. åŸºæœ¬æ¦‚å¿µï¼š\")\n",
    "    print(\"   éšè—ç»´åº¦ = æ¯ä¸ªè¯/token çš„ç‰¹å¾å‘é‡çš„é•¿åº¦\")\n",
    "    print(\"   æ›´å¤§çš„éšè—ç»´åº¦ = æ›´ä¸°å¯Œçš„è¡¨ç¤ºèƒ½åŠ›\")\n",
    "    print()\n",
    "    \n",
    "    # åˆ›å»ºä¸åŒéšè—ç»´åº¦çš„ä¾‹å­\n",
    "    batch_size = 1\n",
    "    seq_len = 4  # 4ä¸ªè¯çš„å¥å­\n",
    "    \n",
    "    # å°éšè—ç»´åº¦\n",
    "    small_dim = 8\n",
    "    small_features = torch.randn(batch_size, seq_len, small_dim)\n",
    "    \n",
    "    # å¤§éšè—ç»´åº¦\n",
    "    large_dim = 512\n",
    "    large_features = torch.randn(batch_size, seq_len, large_dim)\n",
    "    \n",
    "    print(\"2. ä¸åŒéšè—ç»´åº¦çš„å¯¹æ¯”ï¼š\")\n",
    "    print(f\"   å°éšè—ç»´åº¦ (dim={small_dim}): {small_features.shape}\")\n",
    "    print(f\"   å¤§éšè—ç»´åº¦ (dim={large_dim}): {large_features.shape}\")\n",
    "    print()\n",
    "    print(\"   è§£é‡Šï¼š\")\n",
    "    print(f\"   - å°ç»´åº¦ï¼šæ¯ä¸ªè¯ç”¨ {small_dim} ä¸ªæ•°å­—è¡¨ç¤º\")\n",
    "    print(f\"   - å¤§ç»´åº¦ï¼šæ¯ä¸ªè¯ç”¨ {large_dim} ä¸ªæ•°å­—è¡¨ç¤º\")\n",
    "    print(\"   - æ›´å¤šæ•°å­— = æ›´ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤º\")\n",
    "    print()\n",
    "    \n",
    "    # 3. éšè—ç»´åº¦ä¸å¤šå¤´æ³¨æ„åŠ›çš„å…³ç³»\n",
    "    print(\"3. éšè—ç»´åº¦ä¸å¤šå¤´æ³¨æ„åŠ›çš„å…³ç³»ï¼š\")\n",
    "    print(\"   æ ¸å¿ƒåŸç†ï¼šéšè—ç»´åº¦å¿…é¡»èƒ½è¢«å¤´æ•°æ•´é™¤\")\n",
    "    print()\n",
    "    \n",
    "    # æ¼”ç¤ºä¸åŒçš„é…ç½®\n",
    "    configs = [\n",
    "        (64, 8),    # dim=64, n_heads=8\n",
    "        (512, 8),   # dim=512, n_heads=8\n",
    "        (512, 16),  # dim=512, n_heads=16\n",
    "    ]\n",
    "    \n",
    "    for dim, n_heads in configs:\n",
    "        head_dim = dim // n_heads\n",
    "        print(f\"   é…ç½®: dim={dim}, n_heads={n_heads}\")\n",
    "        print(f\"   -> æ¯ä¸ªå¤´çš„ç»´åº¦ = {dim} Ã· {n_heads} = {head_dim}\")\n",
    "        print(f\"   -> æ„ä¹‰ï¼šå°† {dim} ç»´ç‰¹å¾åˆ†æˆ {n_heads} ä¸ª {head_dim} ç»´çš„å­ç©ºé—´\")\n",
    "        print()\n",
    "    \n",
    "    return small_features, large_features\n",
    "\n",
    "explain_hidden_dimension()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_dimension_splitting():\n",
    "    \"\"\"\n",
    "    æ¼”ç¤ºéšè—ç»´åº¦å¦‚ä½•åœ¨å¤šå¤´æ³¨æ„åŠ›ä¸­è¢«åˆ†å‰²\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"éšè—ç»´åº¦åˆ†å‰²è¿‡ç¨‹è¯¦è§£\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # è®¾ç½®å‚æ•°\n",
    "    batch_size = 1\n",
    "    seq_len = 3\n",
    "    dim = 12  # ä½¿ç”¨å°çš„ç»´åº¦ä¾¿äºç†è§£\n",
    "    n_heads = 3\n",
    "    head_dim = dim // n_heads\n",
    "    \n",
    "    print(f\"é…ç½®å‚æ•°ï¼š\")\n",
    "    print(f\"  batch_size: {batch_size}\")\n",
    "    print(f\"  seq_len: {seq_len} (3ä¸ªè¯)\")\n",
    "    print(f\"  dim: {dim} (éšè—ç»´åº¦)\")\n",
    "    print(f\"  n_heads: {n_heads} (æ³¨æ„åŠ›å¤´æ•°)\")\n",
    "    print(f\"  head_dim: {head_dim} (æ¯ä¸ªå¤´çš„ç»´åº¦)\")\n",
    "    print()\n",
    "    \n",
    "    # 1. åˆ›å»ºè¾“å…¥æ•°æ®\n",
    "    x = torch.randn(batch_size, seq_len, dim)\n",
    "    print(\"1. è¾“å…¥æ•°æ®ï¼š\")\n",
    "    print(f\"   å½¢çŠ¶: {x.shape}\")\n",
    "    print(f\"   å«ä¹‰: {batch_size} ä¸ªæ ·æœ¬, {seq_len} ä¸ªè¯, æ¯ä¸ªè¯ {dim} ç»´ç‰¹å¾\")\n",
    "    print()\n",
    "    \n",
    "    # 2. æ¨¡æ‹Ÿçº¿æ€§å˜æ¢ (Q, K, V)\n",
    "    # è¿™é‡Œæˆ‘ä»¬åªå±•ç¤ºQçš„å˜æ¢è¿‡ç¨‹\n",
    "    Wq = torch.randn(dim, dim)  # Qçš„æƒé‡çŸ©é˜µ\n",
    "    \n",
    "    # è®¡ç®—Q\n",
    "    Q = torch.matmul(x, Wq)  # (1, 3, 12) Ã— (12, 12) = (1, 3, 12)\n",
    "    print(\"2. çº¿æ€§å˜æ¢åçš„Qï¼š\")\n",
    "    print(f\"   Qå½¢çŠ¶: {Q.shape}\")\n",
    "    print(f\"   è¿™ä»ç„¶æ˜¯ {seq_len} ä¸ªè¯ï¼Œæ¯ä¸ªè¯ {dim} ç»´ç‰¹å¾\")\n",
    "    print()\n",
    "    \n",
    "    # 3. é‡å¡‘ä¸ºå¤šå¤´å½¢å¼\n",
    "    Q_reshaped = Q.view(batch_size, seq_len, n_heads, head_dim)\n",
    "    print(\"3. é‡å¡‘ä¸ºå¤šå¤´å½¢å¼ï¼š\")\n",
    "    print(f\"   Q_reshapedå½¢çŠ¶: {Q_reshaped.shape}\")\n",
    "    print(f\"   å«ä¹‰: {batch_size} ä¸ªæ ·æœ¬, {seq_len} ä¸ªè¯, {n_heads} ä¸ªå¤´, æ¯ä¸ªå¤´ {head_dim} ç»´\")\n",
    "    print()\n",
    "    \n",
    "    # 4. è¯¦ç»†æŸ¥çœ‹åˆ†å‰²ç»“æœ\n",
    "    print(\"4. è¯¦ç»†æŸ¥çœ‹æ¯ä¸ªå¤´çš„æ•°æ®ï¼š\")\n",
    "    for head in range(n_heads):\n",
    "        head_data = Q_reshaped[0, :, head, :]  # ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œæ‰€æœ‰è¯ï¼Œç¬¬headä¸ªå¤´\n",
    "        print(f\"   å¤´ {head}: å½¢çŠ¶ {head_data.shape}\")\n",
    "        print(f\"   -> åŒ…å« {seq_len} ä¸ªè¯ï¼Œæ¯ä¸ªè¯ {head_dim} ç»´ç‰¹å¾\")\n",
    "        print(f\"   -> åŸå§‹ç‰¹å¾çš„ç¬¬ {head*head_dim} åˆ°ç¬¬ {(head+1)*head_dim-1} ç»´\")\n",
    "        print()\n",
    "    \n",
    "    # 5. è½¬ç½®å‡†å¤‡è®¡ç®—\n",
    "    Q_transposed = Q_reshaped.transpose(1, 2)\n",
    "    print(\"5. è½¬ç½®åç”¨äºæ³¨æ„åŠ›è®¡ç®—ï¼š\")\n",
    "    print(f\"   Q_transposedå½¢çŠ¶: {Q_transposed.shape}\")\n",
    "    print(f\"   å«ä¹‰: {batch_size} ä¸ªæ ·æœ¬, {n_heads} ä¸ªå¤´, {seq_len} ä¸ªè¯, æ¯ä¸ªå¤´ {head_dim} ç»´\")\n",
    "    print()\n",
    "    \n",
    "    # 6. å±•ç¤ºåŸå§‹ç‰¹å¾å¦‚ä½•è¢«åˆ†å‰²\n",
    "    print(\"6. åŸå§‹ç‰¹å¾åˆ†å‰²ç¤ºä¾‹ï¼š\")\n",
    "    print(\"   å‡è®¾åŸå§‹12ç»´ç‰¹å¾è¡¨ç¤ºï¼š[è¯­æ³•, è¯­ä¹‰, ä½ç½®, æƒ…æ„Ÿ, ä¸»é¢˜, è¯­è°ƒ, æ—¶æ€, è¯­æ€, è¯æ€§, ä¾èµ–, å…±æŒ‡, è¯­éŸ³]\")\n",
    "    print(\"   åˆ†å‰²åï¼š\")\n",
    "    print(\"   - å¤´0: [è¯­æ³•, è¯­ä¹‰, ä½ç½®, æƒ…æ„Ÿ] (ç»´åº¦ 0-3)\")\n",
    "    print(\"   - å¤´1: [ä¸»é¢˜, è¯­è°ƒ, æ—¶æ€, è¯­æ€] (ç»´åº¦ 4-7)\")\n",
    "    print(\"   - å¤´2: [è¯æ€§, ä¾èµ–, å…±æŒ‡, è¯­éŸ³] (ç»´åº¦ 8-11)\")\n",
    "    print(\"   æ¯ä¸ªå¤´ä¸“æ³¨äºä¸åŒçš„è¯­è¨€ç‰¹å¾ï¼\")\n",
    "    print()\n",
    "    \n",
    "    return Q_reshaped, Q_transposed\n",
    "\n",
    "demonstrate_dimension_splitting()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_hidden_dimensions():\n",
    "    \"\"\"\n",
    "    æ¯”è¾ƒä¸åŒéšè—ç»´åº¦å¯¹æ¨¡å‹çš„å½±å“\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ä¸åŒéšè—ç»´åº¦çš„å½±å“å¯¹æ¯”\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # æµ‹è¯•ä¸åŒçš„éšè—ç»´åº¦é…ç½®\n",
    "    configs = [\n",
    "        {\"dim\": 64, \"n_heads\": 4, \"name\": \"å°å‹æ¨¡å‹\"},\n",
    "        {\"dim\": 256, \"n_heads\": 8, \"name\": \"ä¸­å‹æ¨¡å‹\"},\n",
    "        {\"dim\": 512, \"n_heads\": 8, \"name\": \"å¤§å‹æ¨¡å‹\"},\n",
    "    ]\n",
    "    \n",
    "    batch_size = 1\n",
    "    seq_len = 5\n",
    "    \n",
    "    print(\"æµ‹è¯•é…ç½®ï¼š\")\n",
    "    print(f\"  è¾“å…¥: batch_size={batch_size}, seq_len={seq_len}\")\n",
    "    print()\n",
    "    \n",
    "    for i, config in enumerate(configs):\n",
    "        print(f\"{i+1}. {config['name']}:\")\n",
    "        print(f\"   éšè—ç»´åº¦: {config['dim']}\")\n",
    "        print(f\"   æ³¨æ„åŠ›å¤´æ•°: {config['n_heads']}\")\n",
    "        print(f\"   æ¯ä¸ªå¤´ç»´åº¦: {config['dim'] // config['n_heads']}\")\n",
    "        \n",
    "        # è®¡ç®—å‚æ•°æ•°é‡\n",
    "        dim = config['dim']\n",
    "        n_heads = config['n_heads']\n",
    "        \n",
    "        # Q, K, V çº¿æ€§å±‚å‚æ•°\n",
    "        qkv_params = 3 * dim * dim  # 3ä¸ªçº¿æ€§å±‚ï¼Œæ¯ä¸ª dimÃ—dim\n",
    "        # è¾“å‡ºçº¿æ€§å±‚å‚æ•°\n",
    "        output_params = dim * dim\n",
    "        total_params = qkv_params + output_params\n",
    "        \n",
    "        print(f\"   å‚æ•°æ•°é‡: {total_params:,}\")\n",
    "        print(f\"   å†…å­˜å ç”¨: ~{total_params * 4 / 1024:.1f} KB (float32)\")\n",
    "        \n",
    "        # åˆ›å»ºæ¨¡å‹\n",
    "        args = ModelArgs(dim=dim, n_heads=n_heads, dropout=0.0)\n",
    "        model = MultiHeadAttention(args, is_causal=False)\n",
    "        \n",
    "        # è®¡ç®—è¾“å‡º\n",
    "        x = torch.randn(batch_size, seq_len, dim)\n",
    "        with torch.no_grad():\n",
    "            output = model(x, x, x)\n",
    "        \n",
    "        print(f\"   è¾“å…¥å½¢çŠ¶: {x.shape}\")\n",
    "        print(f\"   è¾“å‡ºå½¢çŠ¶: {output.shape}\")\n",
    "        print(f\"   è¡¨ç¤ºèƒ½åŠ›: {dim}ç»´å‘é‡å¯ä»¥è¡¨ç¤º 2^{dim} ç§ä¸åŒçš„æ¨¡å¼\")\n",
    "        print()\n",
    "    \n",
    "    # å±•ç¤ºéšè—ç»´åº¦é€‰æ‹©çš„æƒè¡¡\n",
    "    print(\"=\" * 40)\n",
    "    print(\"éšè—ç»´åº¦é€‰æ‹©çš„æƒè¡¡\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"ğŸ”º æ›´å¤§çš„éšè—ç»´åº¦ (ä¼˜ç‚¹):\")\n",
    "    print(\"  âœ“ æ›´å¼ºçš„è¡¨ç¤ºèƒ½åŠ›\")\n",
    "    print(\"  âœ“ èƒ½æ•è·æ›´å¤æ‚çš„è¯­è¨€æ¨¡å¼\")\n",
    "    print(\"  âœ“ æ›´å¥½çš„æ€§èƒ½è¡¨ç°\")\n",
    "    print(\"  âœ“ æ›´å¤§çš„æ¨¡å‹å®¹é‡\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ”» æ›´å¤§çš„éšè—ç»´åº¦ (ç¼ºç‚¹):\")\n",
    "    print(\"  âœ— éœ€è¦æ›´å¤šå†…å­˜\")\n",
    "    print(\"  âœ— è®¡ç®—æˆæœ¬æ›´é«˜\")\n",
    "    print(\"  âœ— å®¹æ˜“è¿‡æ‹Ÿåˆ\")\n",
    "    print(\"  âœ— è®­ç»ƒæ—¶é—´æ›´é•¿\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ¯ å®é™…åº”ç”¨ä¸­çš„é€‰æ‹©:\")\n",
    "    print(\"  â€¢ GPT-2 small: dim=768\")\n",
    "    print(\"  â€¢ GPT-2 medium: dim=1024\")\n",
    "    print(\"  â€¢ GPT-2 large: dim=1280\")\n",
    "    print(\"  â€¢ GPT-3: dim=12288\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ’¡ é€‰æ‹©å»ºè®®:\")\n",
    "    print(\"  1. ä»å°æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥å¢åŠ ç»´åº¦\")\n",
    "    print(\"  2. æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©åˆé€‚çš„ç»´åº¦\")\n",
    "    print(\"  3. è€ƒè™‘è®¡ç®—èµ„æºå’Œæ—¶é—´é™åˆ¶\")\n",
    "    print(\"  4. ä½¿ç”¨éªŒè¯é›†æ¥é€‰æ‹©æœ€ä¼˜ç»´åº¦\")\n",
    "\n",
    "compare_hidden_dimensions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_dimension_complete_flow():\n",
    "    \"\"\"\n",
    "    å±•ç¤ºéšè—ç»´åº¦åœ¨å¤šå¤´æ³¨æ„åŠ›ä¸­çš„å®Œæ•´æµç¨‹\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"éšè—ç»´åº¦åœ¨å¤šå¤´æ³¨æ„åŠ›ä¸­çš„å®Œæ•´æµç¨‹\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # ä½¿ç”¨å®¹æ˜“ç†è§£çš„å°å‚æ•°\n",
    "    dim = 16\n",
    "    n_heads = 4\n",
    "    head_dim = dim // n_heads\n",
    "    seq_len = 3\n",
    "    \n",
    "    print(\"ğŸ“‹ æµç¨‹æ¦‚è§ˆ:\")\n",
    "    print(f\"   åŸå§‹è¾“å…¥: æ¯ä¸ªè¯ {dim} ç»´ç‰¹å¾\")\n",
    "    print(f\"   åˆ†å‰²: åˆ†æˆ {n_heads} ä¸ªå¤´ï¼Œæ¯ä¸ªå¤´ {head_dim} ç»´\")\n",
    "    print(f\"   è®¡ç®—: æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›\")\n",
    "    print(f\"   åˆå¹¶: å°† {n_heads} ä¸ªå¤´çš„ç»“æœåˆå¹¶å› {dim} ç»´\")\n",
    "    print()\n",
    "    \n",
    "    # åˆ›å»ºè¾“å…¥æ•°æ®\n",
    "    x = torch.randn(1, seq_len, dim)\n",
    "    print(f\"ğŸ”¸ æ­¥éª¤1: è¾“å…¥æ•°æ®\")\n",
    "    print(f\"   å½¢çŠ¶: {x.shape}\")\n",
    "    print(f\"   å«ä¹‰: 3ä¸ªè¯ï¼Œæ¯ä¸ªè¯16ç»´ç‰¹å¾å‘é‡\")\n",
    "    print()\n",
    "    \n",
    "    # æ¨¡æ‹Ÿæƒé‡çŸ©é˜µ\n",
    "    W_q = torch.randn(dim, dim)\n",
    "    W_k = torch.randn(dim, dim)\n",
    "    W_v = torch.randn(dim, dim)\n",
    "    W_o = torch.randn(dim, dim)\n",
    "    \n",
    "    print(f\"ğŸ”¸ æ­¥éª¤2: çº¿æ€§å˜æ¢ (Q, K, V)\")\n",
    "    Q = torch.matmul(x, W_q)\n",
    "    K = torch.matmul(x, W_k)\n",
    "    V = torch.matmul(x, W_v)\n",
    "    print(f\"   Qå½¢çŠ¶: {Q.shape} (æŸ¥è¯¢)\")\n",
    "    print(f\"   Kå½¢çŠ¶: {K.shape} (é”®)\")\n",
    "    print(f\"   Vå½¢çŠ¶: {V.shape} (å€¼)\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"ğŸ”¸ æ­¥éª¤3: é‡å¡‘ä¸ºå¤šå¤´å½¢å¼\")\n",
    "    Q_multi = Q.view(1, seq_len, n_heads, head_dim).transpose(1, 2)\n",
    "    K_multi = K.view(1, seq_len, n_heads, head_dim).transpose(1, 2)\n",
    "    V_multi = V.view(1, seq_len, n_heads, head_dim).transpose(1, 2)\n",
    "    print(f\"   Q_multiå½¢çŠ¶: {Q_multi.shape}\")\n",
    "    print(f\"   å«ä¹‰: 1ä¸ªæ ·æœ¬, {n_heads}ä¸ªå¤´, {seq_len}ä¸ªè¯, æ¯ä¸ªå¤´{head_dim}ç»´\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"ğŸ”¸ æ­¥éª¤4: æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›\")\n",
    "    attention_outputs = []\n",
    "    for head in range(n_heads):\n",
    "        q_head = Q_multi[:, head, :, :]  # (1, 3, 4)\n",
    "        k_head = K_multi[:, head, :, :]  # (1, 3, 4)\n",
    "        v_head = V_multi[:, head, :, :]  # (1, 3, 4)\n",
    "        \n",
    "        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n",
    "        scores = torch.matmul(q_head, k_head.transpose(-2, -1)) / math.sqrt(head_dim)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v_head)\n",
    "        \n",
    "        attention_outputs.append(attn_output)\n",
    "        print(f\"   å¤´{head}: {q_head.shape} -> {attn_output.shape}\")\n",
    "    \n",
    "    print(f\"   æ¯ä¸ªå¤´éƒ½äº§ç”Ÿäº† {seq_len} ä¸ªè¯çš„ {head_dim} ç»´è¡¨ç¤º\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"ğŸ”¸ æ­¥éª¤5: åˆå¹¶å¤šå¤´ç»“æœ\")\n",
    "    # å°†æ‰€æœ‰å¤´çš„è¾“å‡ºåˆå¹¶\n",
    "    multi_head_output = torch.stack(attention_outputs, dim=1)  # (1, 4, 3, 4)\n",
    "    print(f\"   åˆå¹¶åå½¢çŠ¶: {multi_head_output.shape}\")\n",
    "    \n",
    "    # é‡å¡‘å›åŸå§‹å½¢å¼\n",
    "    merged_output = multi_head_output.transpose(1, 2).contiguous().view(1, seq_len, dim)\n",
    "    print(f\"   é‡å¡‘åå½¢çŠ¶: {merged_output.shape}\")\n",
    "    print(f\"   å«ä¹‰: æ¢å¤åˆ° {seq_len} ä¸ªè¯ï¼Œæ¯ä¸ªè¯ {dim} ç»´ç‰¹å¾\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"ğŸ”¸ æ­¥éª¤6: æœ€ç»ˆçº¿æ€§å˜æ¢\")\n",
    "    final_output = torch.matmul(merged_output, W_o)\n",
    "    print(f\"   æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: {final_output.shape}\")\n",
    "    print(f\"   è¿™ä¸è¾“å…¥å½¢çŠ¶å®Œå…¨ä¸€è‡´ï¼\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"ğŸ¯ éšè—ç»´åº¦çš„å…³é”®ä½œç”¨æ€»ç»“\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"1. ğŸ“Š ä¿¡æ¯å®¹é‡:\")\n",
    "    print(f\"   â€¢ {dim}ç»´å‘é‡å¯ä»¥ç¼–ç ä¸°å¯Œçš„è¯­è¨€ä¿¡æ¯\")\n",
    "    print(f\"   â€¢ æ›´å¤§çš„ç»´åº¦ = æ›´å¼ºçš„è¡¨ç¤ºèƒ½åŠ›\")\n",
    "    print()\n",
    "    \n",
    "    print(\"2. ğŸ”„ å¹¶è¡Œå¤„ç†:\")\n",
    "    print(f\"   â€¢ å°†{dim}ç»´ç‰¹å¾åˆ†å‰²æˆ{n_heads}ä¸ª{head_dim}ç»´å­ç©ºé—´\")\n",
    "    print(f\"   â€¢ æ¯ä¸ªå¤´ä¸“æ³¨äºä¸åŒçš„è¯­è¨€ç‰¹å¾\")\n",
    "    print(f\"   â€¢ å¹¶è¡Œè®¡ç®—æé«˜æ•ˆç‡\")\n",
    "    print()\n",
    "    \n",
    "    print(\"3. ğŸ¨ ç‰¹å¾å¤šæ ·æ€§:\")\n",
    "    print(\"   â€¢ ä¸åŒçš„å¤´å¯èƒ½å­¦ä¹ ï¼š\")\n",
    "    print(\"     - å¤´0: è¯­æ³•å…³ç³» (ä¸»è°“å®¾)\")\n",
    "    print(\"     - å¤´1: è¯­ä¹‰ç›¸ä¼¼æ€§\")\n",
    "    print(\"     - å¤´2: ä½ç½®ä¿¡æ¯\")\n",
    "    print(\"     - å¤´3: æƒ…æ„Ÿå€¾å‘\")\n",
    "    print()\n",
    "    \n",
    "    print(\"4. ğŸ”§ è®¾è®¡çº¦æŸ:\")\n",
    "    print(f\"   â€¢ dim å¿…é¡»èƒ½è¢« n_heads æ•´é™¤\")\n",
    "    print(f\"   â€¢ å½“å‰: {dim} Ã· {n_heads} = {head_dim} âœ“\")\n",
    "    print(f\"   â€¢ è¿™ç¡®ä¿äº†å‡åŒ€çš„ç‰¹å¾åˆ†é…\")\n",
    "    print()\n",
    "    \n",
    "    print(\"5. ğŸ’¡ å®é™…æ„ä¹‰:\")\n",
    "    print(\"   â€¢ è¾“å…¥è¾“å‡ºç»´åº¦ä¿æŒä¸€è‡´\")\n",
    "    print(\"   â€¢ å¯ä»¥å åŠ å¤šä¸ªæ³¨æ„åŠ›å±‚\")\n",
    "    print(\"   â€¢ å…¼å®¹æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–\")\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "hidden_dimension_complete_flow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_weights():\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ï¼Œå±•ç¤ºå› æœæ©ç çš„æ•ˆæœ\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–æ¼”ç¤º\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # åˆ›å»ºä¸€ä¸ªç®€å•çš„å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ç”¨äºå¯è§†åŒ–\n",
    "    class SimpleMultiHeadAttention(nn.Module):\n",
    "        def __init__(self, dim, n_heads, is_causal=False):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            self.n_heads = n_heads\n",
    "            self.head_dim = dim // n_heads\n",
    "            self.is_causal = is_causal\n",
    "            \n",
    "            self.wq = nn.Linear(dim, dim, bias=False)\n",
    "            self.wk = nn.Linear(dim, dim, bias=False)\n",
    "            self.wv = nn.Linear(dim, dim, bias=False)\n",
    "            self.wo = nn.Linear(dim, dim, bias=False)\n",
    "            \n",
    "            if is_causal:\n",
    "                # åˆ›å»ºå› æœæ©ç \n",
    "                max_len = 10\n",
    "                mask = torch.full((max_len, max_len), float('-inf'))\n",
    "                mask = torch.triu(mask, diagonal=1)\n",
    "                self.register_buffer('mask', mask)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            B, T, C = x.shape\n",
    "            \n",
    "            # è®¡ç®— Q, K, V\n",
    "            q = self.wq(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "            k = self.wk(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "            v = self.wv(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "            \n",
    "            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n",
    "            scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "            \n",
    "            # åº”ç”¨å› æœæ©ç \n",
    "            if self.is_causal:\n",
    "                scores = scores + self.mask[:T, :T]\n",
    "            \n",
    "            # è®¡ç®—æ³¨æ„åŠ›æƒé‡\n",
    "            attn_weights = F.softmax(scores, dim=-1)\n",
    "            \n",
    "            # åº”ç”¨æ³¨æ„åŠ›æƒé‡\n",
    "            out = torch.matmul(attn_weights, v)\n",
    "            out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "            out = self.wo(out)\n",
    "            \n",
    "            return out, attn_weights\n",
    "    \n",
    "    # åˆ›å»ºä¸¤ä¸ªæ¨¡å‹ï¼šæ™®é€šçš„å’Œå› æœçš„\n",
    "    dim = 32\n",
    "    n_heads = 2\n",
    "    seq_len = 5\n",
    "    \n",
    "    model_normal = SimpleMultiHeadAttention(dim, n_heads, is_causal=False)\n",
    "    model_causal = SimpleMultiHeadAttention(dim, n_heads, is_causal=True)\n",
    "    \n",
    "    # åˆ›å»ºè¾“å…¥æ•°æ®\n",
    "    x = torch.randn(1, seq_len, dim)\n",
    "    \n",
    "    print(f\"è¾“å…¥æ•°æ®å½¢çŠ¶: {x.shape}\")\n",
    "    print(f\"æ¨¡å‹é…ç½®: dim={dim}, n_heads={n_heads}, seq_len={seq_len}\")\n",
    "    print()\n",
    "    \n",
    "    # è¿è¡Œä¸¤ä¸ªæ¨¡å‹\n",
    "    with torch.no_grad():\n",
    "        output_normal, attn_normal = model_normal(x)\n",
    "        output_causal, attn_causal = model_causal(x)\n",
    "    \n",
    "    print(\"æ™®é€šå¤šå¤´æ³¨æ„åŠ›æƒé‡ (ç¬¬ä¸€ä¸ªå¤´):\")\n",
    "    print(\"è¡Œï¼šæŸ¥è¯¢ä½ç½®ï¼Œåˆ—ï¼šé”®ä½ç½®\")\n",
    "    print(\"æ•°å€¼è¡¨ç¤ºæ¯ä¸ªæŸ¥è¯¢ä½ç½®å¯¹æ¯ä¸ªé”®ä½ç½®çš„æ³¨æ„åŠ›æƒé‡\")\n",
    "    print(\"-\" * 50)\n",
    "    normal_weights = attn_normal[0, 0].numpy()  # ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œç¬¬ä¸€ä¸ªå¤´\n",
    "    for i in range(seq_len):\n",
    "        row_str = \" \".join([f\"{val:.3f}\" for val in normal_weights[i]])\n",
    "        print(f\"ä½ç½® {i}: [{row_str}]\")\n",
    "    print()\n",
    "    \n",
    "    print(\"å› æœå¤šå¤´æ³¨æ„åŠ›æƒé‡ (ç¬¬ä¸€ä¸ªå¤´):\")\n",
    "    print(\"æ³¨æ„ï¼šä¸Šä¸‰è§’éƒ¨åˆ†åº”è¯¥å…¨ä¸º0ï¼ˆæˆ–å¾ˆå°ï¼‰ï¼Œå› ä¸ºä¸èƒ½çœ‹åˆ°æœªæ¥ä¿¡æ¯\")\n",
    "    print(\"-\" * 50)\n",
    "    causal_weights = attn_causal[0, 0].numpy()  # ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œç¬¬ä¸€ä¸ªå¤´\n",
    "    for i in range(seq_len):\n",
    "        row_str = \" \".join([f\"{val:.3f}\" for val in causal_weights[i]])\n",
    "        print(f\"ä½ç½® {i}: [{row_str}]\")\n",
    "    print()\n",
    "    \n",
    "    # éªŒè¯å› æœæ©ç çš„æ•ˆæœ\n",
    "    print(\"éªŒè¯å› æœæ©ç æ•ˆæœ:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # æ£€æŸ¥ä¸Šä¸‰è§’éƒ¨åˆ†æ˜¯å¦ä¸º0\n",
    "    upper_triangle_sum = 0\n",
    "    for i in range(seq_len):\n",
    "        for j in range(i+1, seq_len):\n",
    "            upper_triangle_sum += causal_weights[i, j]\n",
    "    \n",
    "    print(f\"å› æœæ³¨æ„åŠ›æƒé‡ä¸Šä¸‰è§’éƒ¨åˆ†çš„å’Œ: {upper_triangle_sum:.6f}\")\n",
    "    print(\"(åº”è¯¥æ¥è¿‘0ï¼Œè¡¨ç¤ºæœªæ¥ä¿¡æ¯è¢«å±è”½)\")\n",
    "    \n",
    "    # æ£€æŸ¥æ¯è¡Œçš„å’Œæ˜¯å¦ä¸º1\n",
    "    print(\"\\\\næ¯è¡Œæ³¨æ„åŠ›æƒé‡çš„å’Œ (åº”è¯¥éƒ½çº¦ç­‰äº1):\")\n",
    "    for i in range(seq_len):\n",
    "        row_sum = causal_weights[i].sum()\n",
    "        print(f\"  ä½ç½® {i}: {row_sum:.6f}\")\n",
    "    \n",
    "    return attn_normal, attn_causal\n",
    "\n",
    "# è¿è¡Œå¯è§†åŒ–æ¼”ç¤º\n",
    "visualize_attention_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ModelArgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''å¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—æ¨¡å—'''\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMultiHeadAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: ModelArgs, is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# æ„é€ å‡½æ•°\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# args: é…ç½®å¯¹è±¡\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m, in \u001b[0;36mMultiHeadAttention\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMultiHeadAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: ModelArgs, is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# æ„é€ å‡½æ•°\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# args: é…ç½®å¯¹è±¡\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# éšè—å±‚ç»´åº¦å¿…é¡»æ˜¯å¤´æ•°çš„æ•´æ•°å€ï¼Œå› ä¸ºåé¢æˆ‘ä»¬ä¼šå°†è¾“å…¥æ‹†æˆå¤´æ•°ä¸ªçŸ©é˜µ\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ModelArgs' is not defined"
     ]
    }
   ],
   "source": [
    "#define the ModelArgs Type\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    \"\"\"æ¨¡å‹é…ç½®å‚æ•°ç±»\"\"\"\n",
    "    dim: int = 512                    # æ¨¡å‹ç»´åº¦\n",
    "    n_heads: int = 8                  # æ³¨æ„åŠ›å¤´æ•°\n",
    "    n_layers: int = 6                 # å±‚æ•°\n",
    "    vocab_size: int = 50000           # è¯æ±‡è¡¨å¤§å°\n",
    "    max_seq_len: int = 2048           # æœ€å¤§åºåˆ—é•¿åº¦\n",
    "    dropout: float = 0.1              # Dropout æ¯”ç‡\n",
    "    bias: bool = False                # æ˜¯å¦ä½¿ç”¨åç½®\n",
    "\n",
    "\n",
    "'''å¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—æ¨¡å—'''\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelArgs, is_causal=False):\n",
    "        # æ„é€ å‡½æ•°\n",
    "        # args: é…ç½®å¯¹è±¡\n",
    "        super().__init__()\n",
    "        # éšè—å±‚ç»´åº¦å¿…é¡»æ˜¯å¤´æ•°çš„æ•´æ•°å€ï¼Œå› ä¸ºåé¢æˆ‘ä»¬ä¼šå°†è¾“å…¥æ‹†æˆå¤´æ•°ä¸ªçŸ©é˜µ\n",
    "        assert args.dim % args.n_heads == 0\n",
    "        # æ¨¡å‹å¹¶è¡Œå¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸º1ã€‚\n",
    "        model_parallel_size = 1\n",
    "        # æœ¬åœ°è®¡ç®—å¤´æ•°ï¼Œç­‰äºæ€»å¤´æ•°é™¤ä»¥æ¨¡å‹å¹¶è¡Œå¤„ç†å¤§å°ã€‚\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        # æ¯ä¸ªå¤´çš„ç»´åº¦ï¼Œç­‰äºæ¨¡å‹ç»´åº¦é™¤ä»¥å¤´çš„æ€»æ•°ã€‚\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        # Wq, Wk, Wv å‚æ•°çŸ©é˜µï¼Œæ¯ä¸ªå‚æ•°çŸ©é˜µä¸º n_embd x n_embd\n",
    "        # è¿™é‡Œé€šè¿‡ä¸‰ä¸ªç»„åˆçŸ©é˜µæ¥ä»£æ›¿äº†nä¸ªå‚æ•°çŸ©é˜µçš„ç»„åˆï¼Œå…¶é€»è¾‘åœ¨äºçŸ©é˜µå†…ç§¯å†æ‹¼æ¥å…¶å®ç­‰åŒäºæ‹¼æ¥çŸ©é˜µå†å†…ç§¯ï¼Œ\n",
    "        # ä¸ç†è§£çš„è¯»è€…å¯ä»¥è‡ªè¡Œæ¨¡æ‹Ÿä¸€ä¸‹ï¼Œæ¯ä¸€ä¸ªçº¿æ€§å±‚å…¶å®ç›¸å½“äºnä¸ªå‚æ•°çŸ©é˜µçš„æ‹¼æ¥\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        # è¾“å‡ºæƒé‡çŸ©é˜µï¼Œç»´åº¦ä¸º dim x n_embdï¼ˆhead_dim = n_embeds / n_headsï¼‰\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "        # æ³¨æ„åŠ›çš„ dropout\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        # æ®‹å·®è¿æ¥çš„ dropout\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "         \n",
    "        # åˆ›å»ºä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µï¼Œç”¨äºé®è”½æœªæ¥ä¿¡æ¯\n",
    "        # æ³¨æ„ï¼Œå› ä¸ºæ˜¯å¤šå¤´æ³¨æ„åŠ›ï¼ŒMask çŸ©é˜µæ¯”ä¹‹å‰æˆ‘ä»¬å®šä¹‰çš„å¤šä¸€ä¸ªç»´åº¦\n",
    "        if is_causal:\n",
    "           mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))\n",
    "           mask = torch.triu(mask, diagonal=1)\n",
    "           # æ³¨å†Œä¸ºæ¨¡å‹çš„ç¼“å†²åŒº\n",
    "           self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
    "\n",
    "        # è·å–æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦ï¼Œ[batch_size, seq_len, dim]\n",
    "        bsz, seqlen, _ = q.shape\n",
    "\n",
    "        # è®¡ç®—æŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰ã€å€¼ï¼ˆVï¼‰,è¾“å…¥é€šè¿‡å‚æ•°çŸ©é˜µå±‚ï¼Œç»´åº¦ä¸º (B, T, n_embed) x (n_embed, n_embed) -> (B, T, n_embed)\n",
    "        xq, xk, xv = self.wq(q), self.wk(k), self.wv(v)\n",
    "\n",
    "        # å°† Qã€Kã€V æ‹†åˆ†æˆå¤šå¤´ï¼Œç»´åº¦ä¸º (B, T, n_head, C // n_head)ï¼Œç„¶åäº¤æ¢ç»´åº¦ï¼Œå˜æˆ (B, n_head, T, C // n_head)\n",
    "        # å› ä¸ºåœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­æˆ‘ä»¬æ˜¯å–äº†åä¸¤ä¸ªç»´åº¦å‚ä¸è®¡ç®—\n",
    "        # ä¸ºä»€ä¹ˆè¦å…ˆæŒ‰B*T*n_head*C//n_headå±•å¼€å†äº’æ¢1ã€2ç»´åº¦è€Œä¸æ˜¯ç›´æ¥æŒ‰æ³¨æ„åŠ›è¾“å…¥å±•å¼€ï¼Œæ˜¯å› ä¸ºviewçš„å±•å¼€æ–¹å¼æ˜¯ç›´æ¥æŠŠè¾“å…¥å…¨éƒ¨æ’å¼€ï¼Œ\n",
    "        # ç„¶åæŒ‰è¦æ±‚æ„é€ ï¼Œå¯ä»¥å‘ç°åªæœ‰ä¸Šè¿°æ“ä½œèƒ½å¤Ÿå®ç°æˆ‘ä»¬å°†æ¯ä¸ªå¤´å¯¹åº”éƒ¨åˆ†å–å‡ºæ¥çš„ç›®æ ‡\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "\n",
    "\n",
    "        # æ³¨æ„åŠ›è®¡ç®—\n",
    "        # è®¡ç®— QK^T / sqrt(d_k)ï¼Œç»´åº¦ä¸º (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        # æ©ç è‡ªæ³¨æ„åŠ›å¿…é¡»æœ‰æ³¨æ„åŠ›æ©ç \n",
    "        if self.is_causal:\n",
    "            assert hasattr(self, 'mask')\n",
    "            # è¿™é‡Œæˆªå–åˆ°åºåˆ—é•¿åº¦ï¼Œå› ä¸ºæœ‰äº›åºåˆ—å¯èƒ½æ¯” max_seq_len çŸ­\n",
    "            scores = scores + self.mask[:, :, :seqlen, :seqlen]\n",
    "        # è®¡ç®— softmaxï¼Œç»´åº¦ä¸º (B, nh, T, T)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        # åš Dropout\n",
    "        scores = self.attn_dropout(scores)\n",
    "        # V * Scoreï¼Œç»´åº¦ä¸º(B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        output = torch.matmul(scores, xv)\n",
    "\n",
    "        # æ¢å¤æ—¶é—´ç»´åº¦å¹¶åˆå¹¶å¤´ã€‚\n",
    "        # å°†å¤šå¤´çš„ç»“æœæ‹¼æ¥èµ·æ¥, å…ˆäº¤æ¢ç»´åº¦ä¸º (B, T, n_head, C // n_head)ï¼Œå†æ‹¼æ¥æˆ (B, T, n_head * C // n_head)\n",
    "        # contiguous å‡½æ•°ç”¨äºé‡æ–°å¼€è¾Ÿä¸€å—æ–°å†…å­˜å­˜å‚¨ï¼Œå› ä¸ºPytorchè®¾ç½®å…ˆtransposeå†viewä¼šæŠ¥é”™ï¼Œ\n",
    "        # å› ä¸ºviewç›´æ¥åŸºäºåº•å±‚å­˜å‚¨å¾—åˆ°ï¼Œç„¶è€Œtransposeå¹¶ä¸ä¼šæ”¹å˜åº•å±‚å­˜å‚¨ï¼Œå› æ­¤éœ€è¦é¢å¤–å­˜å‚¨\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        # æœ€ç»ˆæŠ•å½±å›æ®‹å·®æµã€‚\n",
    "        output = self.wo(output)\n",
    "        output = self.resid_dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
