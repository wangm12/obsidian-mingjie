{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ModelArgs configuration class\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    \"\"\"模型配置参数类\"\"\"\n",
    "    dim: int = 512                    # 模型维度\n",
    "    n_heads: int = 8                  # 注意力头数\n",
    "    n_layers: int = 6                 # 层数\n",
    "    vocab_size: int = 50000           # 词汇表大小\n",
    "    max_seq_len: int = 2048           # 最大序列长度\n",
    "    dropout: float = 0.1              # Dropout 比率\n",
    "    bias: bool = False                # 是否使用偏置\n",
    "\n",
    "print(\"ModelArgs class defined successfully!\")\n",
    "print(\"Available parameters:\")\n",
    "for field_name in ModelArgs.__dataclass_fields__:\n",
    "    field = ModelArgs.__dataclass_fields__[field_name]\n",
    "    print(f\"  {field_name}: {field.type.__name__} = {field.default}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''注意力计算函数'''\n",
    "def attention(query, key, value, dropout=None):\n",
    "    '''\n",
    "    计算\"缩放点积注意力\"。\n",
    "\n",
    "    参数:\n",
    "    query: 查询张量，形状为 (..., seq_len_q, d_k)\n",
    "    key: 键张量，形状为 (..., seq_len_k, d_k)\n",
    "    value: 值张量，形状为 (..., seq_len_v, d_v)，通常 seq_len_k == seq_len_v\n",
    "    dropout: Dropout 层，可选\n",
    "\n",
    "    返回:\n",
    "    加权后的值张量，形状为 (..., seq_len_q, d_v)\n",
    "    注意力权重张量，形状为 (..., seq_len_q, seq_len_k)\n",
    "    '''\n",
    "    # 获取键向量的维度 d_k\n",
    "    d_k = query.size(-1)\n",
    "    print(f\"d_k: {d_k}\")\n",
    "    \n",
    "    # 计算 Q 与 K 的转置的点积，然后缩放\n",
    "    # 注意力得分: scores = Q × K^T / √d_k\n",
    "    # scores 的形状: (..., seq_len_q, seq_len_k)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # 对 scores 的最后一个维度应用 softmax，得到注意力权重\n",
    "    # p_attn 的形状: (..., seq_len_q, seq_len_k)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    \n",
    "    # (可选) 应用 dropout\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "        \n",
    "    # 将注意力权重 p_attn 与 V 相乘，得到加权的输出\n",
    "    # output 的形状: (..., seq_len_q, d_v)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to execute\n",
    "def attention_test():\n",
    "    # 设定超参数\n",
    "    batch_size = 2      # 批处理大小\n",
    "    seq_len = 5         # 序列长度\n",
    "    d_k = 8             # Query 和 Key 的维度\n",
    "    d_v = 10            # Value 的维度 (可以和 d_k 不同)\n",
    "    dropout_rate = 0.1  # Dropout 比率\n",
    "\n",
    "    # 1. 实例化 Dropout 层\n",
    "    dropout_layer = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    # 2. 创建模拟的 Q, K, V 张量\n",
    "    # 形状: (batch_size, seq_len, dimension)\n",
    "    query = torch.randn(batch_size, seq_len, d_k)\n",
    "    key = torch.randn(batch_size, seq_len, d_k)\n",
    "    value = torch.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "    print(\"--- 输入张量形状 ---\")\n",
    "    print(f\"Query (Q) shape: {query.shape}\")\n",
    "    print(f\"Key (K) shape:   {key.shape}\")\n",
    "    print(f\"Value (V) shape: {value.shape}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # 3. 调用 attention 函数\n",
    "    # 将 dropout_layer 作为参数传入\n",
    "    output, attention_weights = attention(query, key, value, dropout=dropout_layer)\n",
    "\n",
    "    # 4. 打印输出结果的形状\n",
    "    print(\"\\n--- 输出结果形状 ---\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention Weights shape: {attention_weights.shape}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # 5. 检查输出内容的含义\n",
    "    print(\"\\n--- 输出内容检查 ---\")\n",
    "    print(\"Output[0, 0, :]:\\n\", output[0, 0, :])\n",
    "    print(\"\\n第一个样本的注意力权重 (部分):\\n\", attention_weights[0].round(decimals=4))\n",
    "    # 检查注意力权重的和是否为1 (在最后一个维度上)\n",
    "    sum_of_weights = attention_weights[0, 0, :].sum()\n",
    "    print(f\"\\n第一行注意力权重的和: {sum_of_weights:.4f} (应约等于 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 输入张量形状 ---\n",
      "Query (Q) shape: torch.Size([2, 5, 8])\n",
      "Key (K) shape:   torch.Size([2, 5, 8])\n",
      "Value (V) shape: torch.Size([2, 5, 10])\n",
      "--------------------\n",
      "d_k: 8\n",
      "\n",
      "--- 输出结果形状 ---\n",
      "Output shape: torch.Size([2, 5, 10])\n",
      "Attention Weights shape: torch.Size([2, 5, 5])\n",
      "--------------------\n",
      "\n",
      "--- 输出内容检查 ---\n",
      "Output[0, 0, :]:\n",
      " tensor([ 0.9657,  0.2202, -0.5204, -0.5764, -1.5707, -0.3773, -0.9463,  1.3893,\n",
      "         1.2438,  0.4634])\n",
      "\n",
      "第一个样本的注意力权重 (部分):\n",
      " tensor([[0.0000, 0.1387, 0.0465, 0.0915, 0.7841],\n",
      "        [0.3416, 0.0135, 0.0407, 0.6249, 0.0904],\n",
      "        [0.1342, 0.0799, 0.0927, 0.2342, 0.5700],\n",
      "        [0.2875, 0.0857, 0.0203, 0.3677, 0.3498],\n",
      "        [0.2472, 0.2715, 0.1490, 0.3754, 0.0680]])\n",
      "\n",
      "第一行注意力权重的和: 1.0608 (应约等于 1.0)\n"
     ]
    }
   ],
   "source": [
    "attention_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention_test():\n",
    "    # self attention 是可以将相同的 Q, K, V 传入attention机制里\n",
    "    batch_size = 2      # 批处理大小\n",
    "    seq_len = 5         # 序列长度\n",
    "    d_k = 8             # Query 和 Key 的维度，因为我们是自注意力，因此这也是Value的维度\n",
    "    dropout_rate = 0.1  # Dropout 比率\n",
    "\n",
    "    # 1. 实例化 Dropout 层\n",
    "    dropout_layer = nn.Dropout(p=dropout_rate)\n",
    "    \n",
    "    x = torch.randn(batch_size, seq_len, d_k)\n",
    "    print(f\"x.shape: {x.shape}\")\n",
    "    \n",
    "    # 2. 计算自注意力\n",
    "    output, p_attn = attention(x, x, x, dropout_layer)\n",
    "    \n",
    "    # 3. 打印输出形状\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {p_attn.shape}\")\n",
    "    \n",
    "    return output, p_attn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([2, 5, 8])\n",
      "d_k: 8\n",
      "Output shape: torch.Size([2, 5, 8])\n",
      "Attention weights shape: torch.Size([2, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.6683,  0.8487,  0.3628, -1.3097, -0.8950,  0.6976, -1.5617,\n",
       "           -0.6750],\n",
       "          [ 2.4377,  0.0092, -0.2172,  1.1283,  0.5985, -2.2264,  1.5915,\n",
       "           -0.6868],\n",
       "          [-0.8538,  0.0929,  0.2378, -0.3035,  1.2549,  0.0255,  0.4164,\n",
       "            0.3867],\n",
       "          [ 1.5358,  0.2174,  0.0970, -0.2330,  1.0490, -0.1617,  1.1028,\n",
       "            0.4111],\n",
       "          [ 1.0493,  0.3808, -0.1211, -0.8172,  0.7783,  0.2532,  0.8740,\n",
       "            0.6752]],\n",
       " \n",
       "         [[-0.4442, -0.1237, -0.1255, -0.0936,  0.2974,  0.1481, -0.6693,\n",
       "            0.5486],\n",
       "          [-0.4099,  0.6186,  0.3617, -0.8760, -0.7344, -0.4572,  0.2512,\n",
       "           -0.3626],\n",
       "          [-1.2514, -0.2184, -1.0676,  0.4270,  0.5719, -0.3268, -2.2355,\n",
       "            1.5484],\n",
       "          [-0.4894,  0.0703,  0.0202, -0.4927, -0.7036, -0.4252,  0.3350,\n",
       "           -0.3648],\n",
       "          [-0.2227, -0.7650,  0.9707, -1.1396,  1.5226,  1.9492, -0.5938,\n",
       "            0.7727]]]),\n",
       " tensor([[[9.9057e-01, 5.8785e-03, 4.4457e-02, 1.9656e-02, 0.0000e+00],\n",
       "          [7.7059e-04, 1.0608e+00, 3.3891e-03, 3.6190e-02, 9.9337e-03],\n",
       "          [8.0982e-02, 4.7095e-02, 7.4547e-01, 1.2453e-01, 1.1303e-01],\n",
       "          [1.7573e-02, 2.4683e-01, 6.1122e-02, 5.5050e-01, 2.3508e-01],\n",
       "          [6.2924e-02, 9.4332e-02, 7.7243e-02, 3.2731e-01, 5.4930e-01]],\n",
       " \n",
       "         [[0.0000e+00, 3.1440e-02, 3.0283e-01, 6.7405e-02, 1.6605e-01],\n",
       "          [4.2741e-02, 6.8040e-01, 5.4594e-02, 2.6424e-01, 6.9143e-02],\n",
       "          [7.2719e-02, 9.6434e-03, 9.8833e-01, 9.2186e-03, 3.1202e-02],\n",
       "          [1.1818e-01, 3.4079e-01, 6.7310e-02, 5.3324e-01, 5.1586e-02],\n",
       "          [5.0875e-02, 1.5583e-02, 3.9811e-02, 9.0146e-03, 9.9583e-01]]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 512                    # 模型维度\n",
    "    n_heads: int = 8                  # 注意力头数\n",
    "    dropout: float = 0.1              # Dropout 比率\n",
    "    max_seq_len: int = 2048           # 最大序列长度\n",
    "\n",
    "# 修复 MultiHeadAttention 类中的一个小错误\n",
    "'''多头自注意力计算模块'''\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelArgs, is_causal=False):\n",
    "        # 构造函数\n",
    "        # args: 配置对象\n",
    "        super().__init__()\n",
    "        # 隐藏层维度必须是头数的整数倍，因为后面我们会将输入拆成头数个矩阵\n",
    "        assert args.dim % args.n_heads == 0\n",
    "        # 模型并行处理大小，默认为1。\n",
    "        model_parallel_size = 1\n",
    "        # 本地计算头数，等于总头数除以模型并行处理大小。\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        # 每个头的维度，等于模型维度除以头的总数。\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        # 保存是否使用因果掩码\n",
    "        self.is_causal = is_causal\n",
    "\n",
    "        # Wq, Wk, Wv 参数矩阵，每个参数矩阵为 n_embd x n_embd\n",
    "        # 这里通过三个组合矩阵来代替了n个参数矩阵的组合，其逻辑在于矩阵内积再拼接其实等同于拼接矩阵再内积，\n",
    "        # 不理解的读者可以自行模拟一下，每一个线性层其实相当于n个参数矩阵的拼接\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        # 输出权重矩阵，维度为 dim x n_embd（head_dim = n_embeds / n_heads）\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "        # 注意力的 dropout\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        # 残差连接的 dropout\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "         \n",
    "        # 创建一个上三角矩阵，用于遮蔽未来信息\n",
    "        # 注意，因为是多头注意力，Mask 矩阵比之前我们定义的多一个维度\n",
    "        if is_causal:\n",
    "           mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))\n",
    "           mask = torch.triu(mask, diagonal=1)\n",
    "           # 注册为模型的缓冲区\n",
    "           self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
    "\n",
    "        # 获取批次大小和序列长度，[batch_size, seq_len, dim]\n",
    "        bsz, seqlen, _ = q.shape\n",
    "\n",
    "        # 计算查询（Q）、键（K）、值（V）,输入通过参数矩阵层，维度为 (B, T, n_embed) x (n_embed, n_embed) -> (B, T, n_embed)\n",
    "        xq, xk, xv = self.wq(q), self.wk(k), self.wv(v)\n",
    "        print(f\"After linear transformation:\")\n",
    "        print(f\"  xq shape: {xq.shape}\")\n",
    "        print(f\"  xk shape: {xk.shape}\")\n",
    "        print(f\"  xv shape: {xv.shape}\")\n",
    "\n",
    "        # 将 Q、K、V 拆分成多头，维度为 (B, T, n_head, C // n_head)，然后交换维度，变成 (B, n_head, T, C // n_head)\n",
    "        # 因为在注意力计算中我们是取了后两个维度参与计算\n",
    "        # 为什么要先按B*T*n_head*C//n_head展开再互换1、2维度而不是直接按注意力输入展开，是因为view的展开方式是直接把输入全部排开，\n",
    "        # 然后按要求构造，可以发现只有上述操作能够实现我们将每个头对应部分取出来的目标\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        print(f\"After view (reshape):\")\n",
    "        print(f\"  xq shape: {xq.shape}\")\n",
    "        print(f\"  xk shape: {xk.shape}\")\n",
    "        print(f\"  xv shape: {xv.shape}\")\n",
    "        \n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "        print(f\"After transpose:\")\n",
    "        print(f\"  xq shape: {xq.shape}\")\n",
    "        print(f\"  xk shape: {xk.shape}\")\n",
    "        print(f\"  xv shape: {xv.shape}\")\n",
    "\n",
    "\n",
    "        # 注意力计算\n",
    "        # 计算 QK^T / sqrt(d_k)，维度为 (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        print(f\"Attention scores shape: {scores.shape}\")\n",
    "        \n",
    "        # 掩码自注意力必须有注意力掩码\n",
    "        if self.is_causal:\n",
    "            assert hasattr(self, 'mask')\n",
    "            # 这里截取到序列长度，因为有些序列可能比 max_seq_len 短\n",
    "            scores = scores + self.mask[:, :, :seqlen, :seqlen]\n",
    "            print(f\"After applying causal mask\")\n",
    "            \n",
    "        # 计算 softmax，维度为 (B, nh, T, T)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        # 做 Dropout\n",
    "        scores = self.attn_dropout(scores)\n",
    "        print(f\"Attention weights shape: {scores.shape}\")\n",
    "        \n",
    "        # V * Score，维度为(B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        output = torch.matmul(scores, xv)\n",
    "        print(f\"After attention output shape: {output.shape}\")\n",
    "\n",
    "        # 恢复时间维度并合并头。\n",
    "        # 将多头的结果拼接起来, 先交换维度为 (B, T, n_head, C // n_head)，再拼接成 (B, T, n_head * C // n_head)\n",
    "        # contiguous 函数用于重新开辟一块新内存存储，因为Pytorch设置先transpose再view会报错，\n",
    "        # 因为view直接基于底层存储得到，然而transpose并不会改变底层存储，因此需要额外存储\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        print(f\"After merging heads shape: {output.shape}\")\n",
    "\n",
    "        # 最终投影回残差流。\n",
    "        output = self.wo(output)\n",
    "        output = self.resid_dropout(output)\n",
    "        print(f\"Final output shape: {output.shape}\")\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "多头注意力机制演示\n",
      "==================================================\n",
      "配置参数:\n",
      "  模型维度 (dim): 512\n",
      "  注意力头数 (n_heads): 8\n",
      "  每个头的维度 (head_dim): 64\n",
      "  dropout率: 0.1\n",
      "\n",
      "创建两个多头注意力模块:\n",
      "  - 普通多头注意力 (双向)\n",
      "  - 因果多头注意力 (单向，用于解码器)\n",
      "\n",
      "输入数据:\n",
      "  输入形状: torch.Size([2, 10, 512])\n",
      "  含义: [batch_size=2, seq_len=10, dim=512]\n",
      "\n",
      "==============================\n",
      "普通多头注意力 (自注意力)\n",
      "==============================\n",
      "After linear transformation:\n",
      "  xq shape: torch.Size([2, 10, 512])\n",
      "  xk shape: torch.Size([2, 10, 512])\n",
      "  xv shape: torch.Size([2, 10, 512])\n",
      "After view (reshape):\n",
      "  xq shape: torch.Size([2, 10, 8, 64])\n",
      "  xk shape: torch.Size([2, 10, 8, 64])\n",
      "  xv shape: torch.Size([2, 10, 8, 64])\n",
      "After transpose:\n",
      "  xq shape: torch.Size([2, 8, 10, 64])\n",
      "  xk shape: torch.Size([2, 8, 10, 64])\n",
      "  xv shape: torch.Size([2, 8, 10, 64])\n",
      "Attention scores shape: torch.Size([2, 8, 10, 10])\n",
      "Attention weights shape: torch.Size([2, 8, 10, 10])\n",
      "After attention output shape: torch.Size([2, 8, 10, 64])\n",
      "After merging heads shape: torch.Size([2, 10, 512])\n",
      "Final output shape: torch.Size([2, 10, 512])\n",
      "\\n输出结果:\n",
      "  输出形状: torch.Size([2, 10, 512])\n",
      "  输出是否等于输入形状: True\n",
      "\n",
      "==============================\n",
      "因果多头注意力 (带掩码)\n",
      "==============================\n",
      "After linear transformation:\n",
      "  xq shape: torch.Size([2, 10, 512])\n",
      "  xk shape: torch.Size([2, 10, 512])\n",
      "  xv shape: torch.Size([2, 10, 512])\n",
      "After view (reshape):\n",
      "  xq shape: torch.Size([2, 10, 8, 64])\n",
      "  xk shape: torch.Size([2, 10, 8, 64])\n",
      "  xv shape: torch.Size([2, 10, 8, 64])\n",
      "After transpose:\n",
      "  xq shape: torch.Size([2, 8, 10, 64])\n",
      "  xk shape: torch.Size([2, 8, 10, 64])\n",
      "  xv shape: torch.Size([2, 8, 10, 64])\n",
      "Attention scores shape: torch.Size([2, 8, 10, 10])\n",
      "After applying causal mask\n",
      "Attention weights shape: torch.Size([2, 8, 10, 10])\n",
      "After attention output shape: torch.Size([2, 8, 10, 64])\n",
      "After merging heads shape: torch.Size([2, 10, 512])\n",
      "Final output shape: torch.Size([2, 10, 512])\n",
      "\\n输出结果:\n",
      "  输出形状: torch.Size([2, 10, 512])\n",
      "  输出是否等于输入形状: True\n",
      "\n",
      "==============================\n",
      "输出比较\n",
      "==============================\n",
      "普通注意力与因果注意力输出的平均绝对差异: 0.167131\n",
      "(差异应该很大，因为因果注意力只能看到过去的信息)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-3.5882e-03,  3.4845e-01, -1.5955e-01,  ..., -5.2484e-02,\n",
       "           -5.0627e-02, -4.3429e-02],\n",
       "          [-4.2136e-02,  2.9440e-01, -2.5072e-01,  ..., -6.8263e-02,\n",
       "            6.0080e-02,  9.8534e-03],\n",
       "          [-4.6872e-02,  2.9721e-01, -1.5375e-01,  ..., -3.9070e-02,\n",
       "           -1.0439e-02, -1.0417e-02],\n",
       "          ...,\n",
       "          [ 2.0015e-02,  3.5448e-01, -2.0549e-01,  ..., -6.1390e-02,\n",
       "           -4.2314e-02, -4.7186e-05],\n",
       "          [-1.0270e-01,  3.2186e-01, -2.1751e-01,  ..., -4.9039e-02,\n",
       "            1.4546e-02,  4.3820e-02],\n",
       "          [-3.0241e-02,  2.6220e-01, -2.4302e-01,  ..., -1.0419e-01,\n",
       "            5.5745e-02, -6.3002e-02]],\n",
       " \n",
       "         [[-2.1459e-02,  3.7747e-02, -5.3677e-02,  ...,  1.2905e-01,\n",
       "           -2.2530e-01,  1.3839e-01],\n",
       "          [-1.1947e-01,  6.3499e-02, -1.5479e-01,  ...,  8.2109e-02,\n",
       "           -2.2436e-01,  1.0165e-01],\n",
       "          [-4.5769e-02,  8.1421e-02, -1.5156e-01,  ...,  9.5839e-02,\n",
       "           -2.2262e-01,  1.0562e-01],\n",
       "          ...,\n",
       "          [-5.1230e-02,  7.6579e-02, -2.1101e-02,  ...,  1.3839e-01,\n",
       "           -2.4152e-01,  7.9099e-02],\n",
       "          [-6.0678e-02,  6.7656e-02, -8.0606e-02,  ...,  1.2799e-01,\n",
       "           -2.0220e-01,  1.8050e-01],\n",
       "          [-7.3476e-02,  1.0921e-01, -8.7793e-02,  ...,  1.5827e-01,\n",
       "           -1.6783e-01,  8.7738e-02]]]),\n",
       " tensor([[[ 0.2968, -0.6895,  0.0992,  ..., -0.0096, -0.1886, -0.1755],\n",
       "          [ 0.2990, -0.1890, -0.1468,  ...,  0.0517, -0.0698, -0.1215],\n",
       "          [ 0.2263, -0.0433, -0.2526,  ...,  0.2171, -0.0802, -0.2265],\n",
       "          ...,\n",
       "          [ 0.1037, -0.0130, -0.2102,  ...,  0.2062, -0.0918,  0.0769],\n",
       "          [ 0.1417,  0.0124, -0.2317,  ...,  0.1996, -0.0641,  0.1701],\n",
       "          [ 0.1257, -0.1130, -0.2520,  ...,  0.0516, -0.0790,  0.1909]],\n",
       " \n",
       "         [[ 0.8599,  0.2112, -0.0909,  ...,  0.1156, -0.2621,  0.5401],\n",
       "          [ 0.4825,  0.1367, -0.0473,  ..., -0.0093, -0.2930,  0.3162],\n",
       "          [ 0.5302,  0.0890, -0.2057,  ..., -0.1416, -0.1877,  0.2160],\n",
       "          ...,\n",
       "          [ 0.2764,  0.0302,  0.0328,  ...,  0.0305,  0.0386,  0.1637],\n",
       "          [ 0.2677, -0.1140, -0.0871,  ..., -0.0559, -0.0019,  0.1379],\n",
       "          [ 0.2162, -0.0943, -0.1128,  ...,  0.0596,  0.0670,  0.1619]]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multi_head_attention_demo():\n",
    "    \"\"\"\n",
    "    多头注意力机制的完整演示\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"多头注意力机制演示\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. 创建配置参数\n",
    "    args = ModelArgs(\n",
    "        dim=512,         # 模型维度\n",
    "        n_heads=8,       # 8个注意力头\n",
    "        dropout=0.1,     # dropout率\n",
    "        max_seq_len=2048 # 最大序列长度\n",
    "    )\n",
    "    \n",
    "    print(f\"配置参数:\")\n",
    "    print(f\"  模型维度 (dim): {args.dim}\")\n",
    "    print(f\"  注意力头数 (n_heads): {args.n_heads}\")\n",
    "    print(f\"  每个头的维度 (head_dim): {args.dim // args.n_heads}\")\n",
    "    print(f\"  dropout率: {args.dropout}\")\n",
    "    print()\n",
    "    \n",
    "    # 2. 创建多头注意力模块\n",
    "    # 我们创建两个版本：一个普通的，一个带因果掩码的\n",
    "    print(\"创建两个多头注意力模块:\")\n",
    "    mha_normal = MultiHeadAttention(args, is_causal=False)\n",
    "    mha_causal = MultiHeadAttention(args, is_causal=True)\n",
    "    print(\"  - 普通多头注意力 (双向)\")\n",
    "    print(\"  - 因果多头注意力 (单向，用于解码器)\")\n",
    "    print()\n",
    "    \n",
    "    # 3. 创建输入数据\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    \n",
    "    # 创建随机输入张量\n",
    "    x = torch.randn(batch_size, seq_len, args.dim)\n",
    "    print(f\"输入数据:\")\n",
    "    print(f\"  输入形状: {x.shape}\")\n",
    "    print(f\"  含义: [batch_size={batch_size}, seq_len={seq_len}, dim={args.dim}]\")\n",
    "    print()\n",
    "    \n",
    "    # 4. 演示普通多头注意力 (自注意力)\n",
    "    print(\"=\" * 30)\n",
    "    print(\"普通多头注意力 (自注意力)\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # 设置为评估模式以避免随机性\n",
    "    mha_normal.eval()\n",
    "    with torch.no_grad():\n",
    "        output_normal = mha_normal(x, x, x)  # 自注意力：Q=K=V=x\n",
    "        \n",
    "    print(f\"\\\\n输出结果:\")\n",
    "    print(f\"  输出形状: {output_normal.shape}\")\n",
    "    print(f\"  输出是否等于输入形状: {output_normal.shape == x.shape}\")\n",
    "    print()\n",
    "    \n",
    "    # 5. 演示因果多头注意力\n",
    "    print(\"=\" * 30)\n",
    "    print(\"因果多头注意力 (带掩码)\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    mha_causal.eval()\n",
    "    with torch.no_grad():\n",
    "        output_causal = mha_causal(x, x, x)  # 因果自注意力\n",
    "        \n",
    "    print(f\"\\\\n输出结果:\")\n",
    "    print(f\"  输出形状: {output_causal.shape}\")\n",
    "    print(f\"  输出是否等于输入形状: {output_causal.shape == x.shape}\")\n",
    "    print()\n",
    "    \n",
    "    # 6. 比较两个输出\n",
    "    print(\"=\" * 30)\n",
    "    print(\"输出比较\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # 计算输出的差异\n",
    "    diff = torch.abs(output_normal - output_causal).mean()\n",
    "    print(f\"普通注意力与因果注意力输出的平均绝对差异: {diff:.6f}\")\n",
    "    print(\"(差异应该很大，因为因果注意力只能看到过去的信息)\")\n",
    "    \n",
    "    return output_normal, output_causal\n",
    "\n",
    "# 运行演示\n",
    "multi_head_attention_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "详细维度变化演示\n",
      "============================================================\n",
      "小规模配置参数:\n",
      "  模型维度 (dim): 64\n",
      "  注意力头数 (n_heads): 4\n",
      "  每个头的维度 (head_dim): 16\n",
      "\n",
      "输入数据:\n",
      "  输入形状: torch.Size([1, 3, 64])\n",
      "  含义: [batch_size=1, seq_len=3, dim=64]\n",
      "  这表示：1个样本，3个词，每个词64维特征\n",
      "\n",
      "========================================\n",
      "逐步执行前向传播...\n",
      "========================================\n",
      "After linear transformation:\n",
      "  xq shape: torch.Size([1, 3, 64])\n",
      "  xk shape: torch.Size([1, 3, 64])\n",
      "  xv shape: torch.Size([1, 3, 64])\n",
      "After view (reshape):\n",
      "  xq shape: torch.Size([1, 3, 4, 16])\n",
      "  xk shape: torch.Size([1, 3, 4, 16])\n",
      "  xv shape: torch.Size([1, 3, 4, 16])\n",
      "After transpose:\n",
      "  xq shape: torch.Size([1, 4, 3, 16])\n",
      "  xk shape: torch.Size([1, 4, 3, 16])\n",
      "  xv shape: torch.Size([1, 4, 3, 16])\n",
      "Attention scores shape: torch.Size([1, 4, 3, 3])\n",
      "Attention weights shape: torch.Size([1, 4, 3, 3])\n",
      "After attention output shape: torch.Size([1, 4, 3, 16])\n",
      "After merging heads shape: torch.Size([1, 3, 64])\n",
      "Final output shape: torch.Size([1, 3, 64])\n",
      "\\n========================================\n",
      "总结\n",
      "========================================\n",
      "最终输出形状: torch.Size([1, 3, 64])\n",
      "输入输出形状一致: True\n",
      "这意味着每个词仍然是64维特征，但现在包含了其他词的信息\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1248, -0.4088, -0.1043, -0.1492, -0.1853, -0.1101, -0.0196,\n",
       "          -0.1588, -0.3738,  0.1675, -0.1530,  0.2170, -0.2289,  0.2016,\n",
       "           0.1170, -0.0964,  0.2748, -0.1827,  0.0985,  0.0850, -0.0833,\n",
       "           0.0658,  0.2059, -0.0165,  0.0487, -0.2841, -0.1550, -0.0300,\n",
       "           0.2595,  0.2782, -0.2102,  0.0638, -0.1863,  0.1755,  0.0742,\n",
       "           0.0574,  0.1980, -0.0122, -0.1676,  0.1484,  0.2180, -0.3638,\n",
       "           0.2751, -0.0568,  0.0623,  0.1186, -0.4737,  0.0195, -0.0804,\n",
       "          -0.1568,  0.2000,  0.1656, -0.0141, -0.1310,  0.0199, -0.0208,\n",
       "          -0.0608,  0.3282,  0.2732, -0.0519, -0.0837,  0.0963, -0.2658,\n",
       "          -0.1464],\n",
       "         [ 0.1496, -0.3596, -0.1726, -0.1457, -0.0202, -0.1257, -0.0626,\n",
       "          -0.2295, -0.3345,  0.2137, -0.2111,  0.0920, -0.1474,  0.2324,\n",
       "          -0.0257, -0.0202,  0.3253, -0.1405,  0.1100, -0.0922, -0.1377,\n",
       "           0.0540,  0.2066, -0.0022, -0.0961, -0.3093, -0.2066, -0.0564,\n",
       "           0.2395,  0.1950, -0.2124,  0.0709, -0.1287,  0.1223,  0.0484,\n",
       "           0.0862,  0.1778, -0.1013, -0.0725,  0.0510,  0.1539, -0.3173,\n",
       "           0.3033,  0.0046,  0.0632,  0.1564, -0.3630, -0.0772, -0.2274,\n",
       "          -0.1879,  0.2207,  0.2712, -0.1637, -0.0256,  0.0306, -0.1711,\n",
       "          -0.0779,  0.3288,  0.3063,  0.1163, -0.0967,  0.0704, -0.2757,\n",
       "          -0.1488],\n",
       "         [ 0.0823, -0.3671, -0.1969, -0.2487, -0.0743, -0.1486, -0.1499,\n",
       "          -0.1867, -0.3895,  0.2331, -0.2279,  0.0590, -0.1441,  0.2552,\n",
       "          -0.0307, -0.0769,  0.2446, -0.0753,  0.0862, -0.1203, -0.1632,\n",
       "           0.0617,  0.2425,  0.0137,  0.0227, -0.3594, -0.2321,  0.0096,\n",
       "           0.2849,  0.2600, -0.1448,  0.0883, -0.1078,  0.0479, -0.0029,\n",
       "           0.0893,  0.1025, -0.0568, -0.0530,  0.0266,  0.1854, -0.3491,\n",
       "           0.4324,  0.1125,  0.0306,  0.0994, -0.4280, -0.0492, -0.1397,\n",
       "          -0.1153,  0.1986,  0.2914, -0.2357, -0.0965,  0.0797, -0.1708,\n",
       "          -0.1198,  0.2441,  0.3305,  0.1231,  0.0757,  0.0222, -0.2933,\n",
       "          -0.1406]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detailed_dimension_demo():\n",
    "    \"\"\"\n",
    "    详细的维度变化演示 - 使用小规模数据便于理解\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"详细维度变化演示\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 使用小规模参数便于理解\n",
    "    args = ModelArgs(\n",
    "        dim=64,          # 小的模型维度\n",
    "        n_heads=4,       # 4个注意力头\n",
    "        dropout=0.0,     # 不使用dropout便于演示\n",
    "        max_seq_len=10   # 小的最大序列长度\n",
    "    )\n",
    "    \n",
    "    print(f\"小规模配置参数:\")\n",
    "    print(f\"  模型维度 (dim): {args.dim}\")\n",
    "    print(f\"  注意力头数 (n_heads): {args.n_heads}\")\n",
    "    print(f\"  每个头的维度 (head_dim): {args.dim // args.n_heads}\")\n",
    "    print()\n",
    "    \n",
    "    # 创建多头注意力模块\n",
    "    mha = MultiHeadAttention(args, is_causal=False)\n",
    "    mha.eval()\n",
    "    \n",
    "    # 创建小规模输入数据\n",
    "    batch_size = 1  # 单个批次\n",
    "    seq_len = 3     # 3个词的序列\n",
    "    \n",
    "    x = torch.randn(batch_size, seq_len, args.dim)\n",
    "    print(f\"输入数据:\")\n",
    "    print(f\"  输入形状: {x.shape}\")\n",
    "    print(f\"  含义: [batch_size={batch_size}, seq_len={seq_len}, dim={args.dim}]\")\n",
    "    print(f\"  这表示：1个样本，3个词，每个词64维特征\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=\" * 40)\n",
    "    print(\"逐步执行前向传播...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = mha(x, x, x)  # 自注意力\n",
    "        \n",
    "    print(\"\\\\n\" + \"=\" * 40)\n",
    "    print(\"总结\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"最终输出形状: {output.shape}\")\n",
    "    print(f\"输入输出形状一致: {output.shape == x.shape}\")\n",
    "    print(f\"这意味着每个词仍然是64维特征，但现在包含了其他词的信息\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 运行详细演示\n",
    "detailed_dimension_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "隐藏维度（Hidden Dimension）详解\n",
      "============================================================\n",
      "1. 基本概念：\n",
      "   隐藏维度 = 每个词/token 的特征向量的长度\n",
      "   更大的隐藏维度 = 更丰富的表示能力\n",
      "\n",
      "2. 不同隐藏维度的对比：\n",
      "   小隐藏维度 (dim=8): torch.Size([1, 4, 8])\n",
      "   大隐藏维度 (dim=512): torch.Size([1, 4, 512])\n",
      "\n",
      "   解释：\n",
      "   - 小维度：每个词用 8 个数字表示\n",
      "   - 大维度：每个词用 512 个数字表示\n",
      "   - 更多数字 = 更丰富的语义表示\n",
      "\n",
      "3. 隐藏维度与多头注意力的关系：\n",
      "   核心原理：隐藏维度必须能被头数整除\n",
      "\n",
      "   配置: dim=64, n_heads=8\n",
      "   -> 每个头的维度 = 64 ÷ 8 = 8\n",
      "   -> 意义：将 64 维特征分成 8 个 8 维的子空间\n",
      "\n",
      "   配置: dim=512, n_heads=8\n",
      "   -> 每个头的维度 = 512 ÷ 8 = 64\n",
      "   -> 意义：将 512 维特征分成 8 个 64 维的子空间\n",
      "\n",
      "   配置: dim=512, n_heads=16\n",
      "   -> 每个头的维度 = 512 ÷ 16 = 32\n",
      "   -> 意义：将 512 维特征分成 16 个 32 维的子空间\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.4122,  0.5732, -0.3689, -0.6788, -1.4379,  1.1635, -0.6507,\n",
       "            1.2775],\n",
       "          [ 0.5263,  1.2405,  0.5946,  0.4822,  0.9842, -0.2387, -0.5439,\n",
       "            0.8796],\n",
       "          [ 1.4528,  0.1149, -0.1737, -0.0449, -0.4003,  0.5686, -0.1289,\n",
       "           -0.4169],\n",
       "          [-0.3138, -0.4388,  0.7239,  0.3780,  1.5564,  0.5695,  2.5011,\n",
       "           -2.2259]]]),\n",
       " tensor([[[ 1.4372, -0.3388, -0.9061,  ..., -1.2761, -0.3965,  0.3438],\n",
       "          [ 0.9279, -0.2351,  1.1933,  ...,  2.1610, -1.4266, -0.0313],\n",
       "          [-0.5602,  0.5114,  0.1775,  ...,  0.2276, -2.6811,  0.0305],\n",
       "          [-2.2060,  0.4727, -2.2430,  ...,  0.2253,  2.7395,  0.2348]]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def explain_hidden_dimension():\n",
    "    \"\"\"\n",
    "    详细解释隐藏维度的概念和作用\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"隐藏维度（Hidden Dimension）详解\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. 基本概念演示\n",
    "    print(\"1. 基本概念：\")\n",
    "    print(\"   隐藏维度 = 每个词/token 的特征向量的长度\")\n",
    "    print(\"   更大的隐藏维度 = 更丰富的表示能力\")\n",
    "    print()\n",
    "    \n",
    "    # 创建不同隐藏维度的例子\n",
    "    batch_size = 1\n",
    "    seq_len = 4  # 4个词的句子\n",
    "    \n",
    "    # 小隐藏维度\n",
    "    small_dim = 8\n",
    "    small_features = torch.randn(batch_size, seq_len, small_dim)\n",
    "    \n",
    "    # 大隐藏维度\n",
    "    large_dim = 512\n",
    "    large_features = torch.randn(batch_size, seq_len, large_dim)\n",
    "    \n",
    "    print(\"2. 不同隐藏维度的对比：\")\n",
    "    print(f\"   小隐藏维度 (dim={small_dim}): {small_features.shape}\")\n",
    "    print(f\"   大隐藏维度 (dim={large_dim}): {large_features.shape}\")\n",
    "    print()\n",
    "    print(\"   解释：\")\n",
    "    print(f\"   - 小维度：每个词用 {small_dim} 个数字表示\")\n",
    "    print(f\"   - 大维度：每个词用 {large_dim} 个数字表示\")\n",
    "    print(\"   - 更多数字 = 更丰富的语义表示\")\n",
    "    print()\n",
    "    \n",
    "    # 3. 隐藏维度与多头注意力的关系\n",
    "    print(\"3. 隐藏维度与多头注意力的关系：\")\n",
    "    print(\"   核心原理：隐藏维度必须能被头数整除\")\n",
    "    print()\n",
    "    \n",
    "    # 演示不同的配置\n",
    "    configs = [\n",
    "        (64, 8),    # dim=64, n_heads=8\n",
    "        (512, 8),   # dim=512, n_heads=8\n",
    "        (512, 16),  # dim=512, n_heads=16\n",
    "    ]\n",
    "    \n",
    "    for dim, n_heads in configs:\n",
    "        head_dim = dim // n_heads\n",
    "        print(f\"   配置: dim={dim}, n_heads={n_heads}\")\n",
    "        print(f\"   -> 每个头的维度 = {dim} ÷ {n_heads} = {head_dim}\")\n",
    "        print(f\"   -> 意义：将 {dim} 维特征分成 {n_heads} 个 {head_dim} 维的子空间\")\n",
    "        print()\n",
    "    \n",
    "    return small_features, large_features\n",
    "\n",
    "explain_hidden_dimension()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_dimension_splitting():\n",
    "    \"\"\"\n",
    "    演示隐藏维度如何在多头注意力中被分割\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"隐藏维度分割过程详解\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 设置参数\n",
    "    batch_size = 1\n",
    "    seq_len = 3\n",
    "    dim = 12  # 使用小的维度便于理解\n",
    "    n_heads = 3\n",
    "    head_dim = dim // n_heads\n",
    "    \n",
    "    print(f\"配置参数：\")\n",
    "    print(f\"  batch_size: {batch_size}\")\n",
    "    print(f\"  seq_len: {seq_len} (3个词)\")\n",
    "    print(f\"  dim: {dim} (隐藏维度)\")\n",
    "    print(f\"  n_heads: {n_heads} (注意力头数)\")\n",
    "    print(f\"  head_dim: {head_dim} (每个头的维度)\")\n",
    "    print()\n",
    "    \n",
    "    # 1. 创建输入数据\n",
    "    x = torch.randn(batch_size, seq_len, dim)\n",
    "    print(\"1. 输入数据：\")\n",
    "    print(f\"   形状: {x.shape}\")\n",
    "    print(f\"   含义: {batch_size} 个样本, {seq_len} 个词, 每个词 {dim} 维特征\")\n",
    "    print()\n",
    "    \n",
    "    # 2. 模拟线性变换 (Q, K, V)\n",
    "    # 这里我们只展示Q的变换过程\n",
    "    Wq = torch.randn(dim, dim)  # Q的权重矩阵\n",
    "    \n",
    "    # 计算Q\n",
    "    Q = torch.matmul(x, Wq)  # (1, 3, 12) × (12, 12) = (1, 3, 12)\n",
    "    print(\"2. 线性变换后的Q：\")\n",
    "    print(f\"   Q形状: {Q.shape}\")\n",
    "    print(f\"   这仍然是 {seq_len} 个词，每个词 {dim} 维特征\")\n",
    "    print()\n",
    "    \n",
    "    # 3. 重塑为多头形式\n",
    "    Q_reshaped = Q.view(batch_size, seq_len, n_heads, head_dim)\n",
    "    print(\"3. 重塑为多头形式：\")\n",
    "    print(f\"   Q_reshaped形状: {Q_reshaped.shape}\")\n",
    "    print(f\"   含义: {batch_size} 个样本, {seq_len} 个词, {n_heads} 个头, 每个头 {head_dim} 维\")\n",
    "    print()\n",
    "    \n",
    "    # 4. 详细查看分割结果\n",
    "    print(\"4. 详细查看每个头的数据：\")\n",
    "    for head in range(n_heads):\n",
    "        head_data = Q_reshaped[0, :, head, :]  # 第一个样本，所有词，第head个头\n",
    "        print(f\"   头 {head}: 形状 {head_data.shape}\")\n",
    "        print(f\"   -> 包含 {seq_len} 个词，每个词 {head_dim} 维特征\")\n",
    "        print(f\"   -> 原始特征的第 {head*head_dim} 到第 {(head+1)*head_dim-1} 维\")\n",
    "        print()\n",
    "    \n",
    "    # 5. 转置准备计算\n",
    "    Q_transposed = Q_reshaped.transpose(1, 2)\n",
    "    print(\"5. 转置后用于注意力计算：\")\n",
    "    print(f\"   Q_transposed形状: {Q_transposed.shape}\")\n",
    "    print(f\"   含义: {batch_size} 个样本, {n_heads} 个头, {seq_len} 个词, 每个头 {head_dim} 维\")\n",
    "    print()\n",
    "    \n",
    "    # 6. 展示原始特征如何被分割\n",
    "    print(\"6. 原始特征分割示例：\")\n",
    "    print(\"   假设原始12维特征表示：[语法, 语义, 位置, 情感, 主题, 语调, 时态, 语态, 词性, 依赖, 共指, 语音]\")\n",
    "    print(\"   分割后：\")\n",
    "    print(\"   - 头0: [语法, 语义, 位置, 情感] (维度 0-3)\")\n",
    "    print(\"   - 头1: [主题, 语调, 时态, 语态] (维度 4-7)\")\n",
    "    print(\"   - 头2: [词性, 依赖, 共指, 语音] (维度 8-11)\")\n",
    "    print(\"   每个头专注于不同的语言特征！\")\n",
    "    print()\n",
    "    \n",
    "    return Q_reshaped, Q_transposed\n",
    "\n",
    "demonstrate_dimension_splitting()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_hidden_dimensions():\n",
    "    \"\"\"\n",
    "    比较不同隐藏维度对模型的影响\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"不同隐藏维度的影响对比\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 测试不同的隐藏维度配置\n",
    "    configs = [\n",
    "        {\"dim\": 64, \"n_heads\": 4, \"name\": \"小型模型\"},\n",
    "        {\"dim\": 256, \"n_heads\": 8, \"name\": \"中型模型\"},\n",
    "        {\"dim\": 512, \"n_heads\": 8, \"name\": \"大型模型\"},\n",
    "    ]\n",
    "    \n",
    "    batch_size = 1\n",
    "    seq_len = 5\n",
    "    \n",
    "    print(\"测试配置：\")\n",
    "    print(f\"  输入: batch_size={batch_size}, seq_len={seq_len}\")\n",
    "    print()\n",
    "    \n",
    "    for i, config in enumerate(configs):\n",
    "        print(f\"{i+1}. {config['name']}:\")\n",
    "        print(f\"   隐藏维度: {config['dim']}\")\n",
    "        print(f\"   注意力头数: {config['n_heads']}\")\n",
    "        print(f\"   每个头维度: {config['dim'] // config['n_heads']}\")\n",
    "        \n",
    "        # 计算参数数量\n",
    "        dim = config['dim']\n",
    "        n_heads = config['n_heads']\n",
    "        \n",
    "        # Q, K, V 线性层参数\n",
    "        qkv_params = 3 * dim * dim  # 3个线性层，每个 dim×dim\n",
    "        # 输出线性层参数\n",
    "        output_params = dim * dim\n",
    "        total_params = qkv_params + output_params\n",
    "        \n",
    "        print(f\"   参数数量: {total_params:,}\")\n",
    "        print(f\"   内存占用: ~{total_params * 4 / 1024:.1f} KB (float32)\")\n",
    "        \n",
    "        # 创建模型\n",
    "        args = ModelArgs(dim=dim, n_heads=n_heads, dropout=0.0)\n",
    "        model = MultiHeadAttention(args, is_causal=False)\n",
    "        \n",
    "        # 计算输出\n",
    "        x = torch.randn(batch_size, seq_len, dim)\n",
    "        with torch.no_grad():\n",
    "            output = model(x, x, x)\n",
    "        \n",
    "        print(f\"   输入形状: {x.shape}\")\n",
    "        print(f\"   输出形状: {output.shape}\")\n",
    "        print(f\"   表示能力: {dim}维向量可以表示 2^{dim} 种不同的模式\")\n",
    "        print()\n",
    "    \n",
    "    # 展示隐藏维度选择的权衡\n",
    "    print(\"=\" * 40)\n",
    "    print(\"隐藏维度选择的权衡\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"🔺 更大的隐藏维度 (优点):\")\n",
    "    print(\"  ✓ 更强的表示能力\")\n",
    "    print(\"  ✓ 能捕获更复杂的语言模式\")\n",
    "    print(\"  ✓ 更好的性能表现\")\n",
    "    print(\"  ✓ 更大的模型容量\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🔻 更大的隐藏维度 (缺点):\")\n",
    "    print(\"  ✗ 需要更多内存\")\n",
    "    print(\"  ✗ 计算成本更高\")\n",
    "    print(\"  ✗ 容易过拟合\")\n",
    "    print(\"  ✗ 训练时间更长\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🎯 实际应用中的选择:\")\n",
    "    print(\"  • GPT-2 small: dim=768\")\n",
    "    print(\"  • GPT-2 medium: dim=1024\")\n",
    "    print(\"  • GPT-2 large: dim=1280\")\n",
    "    print(\"  • GPT-3: dim=12288\")\n",
    "    print()\n",
    "    \n",
    "    print(\"💡 选择建议:\")\n",
    "    print(\"  1. 从小模型开始，逐步增加维度\")\n",
    "    print(\"  2. 根据任务复杂度选择合适的维度\")\n",
    "    print(\"  3. 考虑计算资源和时间限制\")\n",
    "    print(\"  4. 使用验证集来选择最优维度\")\n",
    "\n",
    "compare_hidden_dimensions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_dimension_complete_flow():\n",
    "    \"\"\"\n",
    "    展示隐藏维度在多头注意力中的完整流程\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"隐藏维度在多头注意力中的完整流程\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 使用容易理解的小参数\n",
    "    dim = 16\n",
    "    n_heads = 4\n",
    "    head_dim = dim // n_heads\n",
    "    seq_len = 3\n",
    "    \n",
    "    print(\"📋 流程概览:\")\n",
    "    print(f\"   原始输入: 每个词 {dim} 维特征\")\n",
    "    print(f\"   分割: 分成 {n_heads} 个头，每个头 {head_dim} 维\")\n",
    "    print(f\"   计算: 每个头独立计算注意力\")\n",
    "    print(f\"   合并: 将 {n_heads} 个头的结果合并回 {dim} 维\")\n",
    "    print()\n",
    "    \n",
    "    # 创建输入数据\n",
    "    x = torch.randn(1, seq_len, dim)\n",
    "    print(f\"🔸 步骤1: 输入数据\")\n",
    "    print(f\"   形状: {x.shape}\")\n",
    "    print(f\"   含义: 3个词，每个词16维特征向量\")\n",
    "    print()\n",
    "    \n",
    "    # 模拟权重矩阵\n",
    "    W_q = torch.randn(dim, dim)\n",
    "    W_k = torch.randn(dim, dim)\n",
    "    W_v = torch.randn(dim, dim)\n",
    "    W_o = torch.randn(dim, dim)\n",
    "    \n",
    "    print(f\"🔸 步骤2: 线性变换 (Q, K, V)\")\n",
    "    Q = torch.matmul(x, W_q)\n",
    "    K = torch.matmul(x, W_k)\n",
    "    V = torch.matmul(x, W_v)\n",
    "    print(f\"   Q形状: {Q.shape} (查询)\")\n",
    "    print(f\"   K形状: {K.shape} (键)\")\n",
    "    print(f\"   V形状: {V.shape} (值)\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"🔸 步骤3: 重塑为多头形式\")\n",
    "    Q_multi = Q.view(1, seq_len, n_heads, head_dim).transpose(1, 2)\n",
    "    K_multi = K.view(1, seq_len, n_heads, head_dim).transpose(1, 2)\n",
    "    V_multi = V.view(1, seq_len, n_heads, head_dim).transpose(1, 2)\n",
    "    print(f\"   Q_multi形状: {Q_multi.shape}\")\n",
    "    print(f\"   含义: 1个样本, {n_heads}个头, {seq_len}个词, 每个头{head_dim}维\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"🔸 步骤4: 每个头独立计算注意力\")\n",
    "    attention_outputs = []\n",
    "    for head in range(n_heads):\n",
    "        q_head = Q_multi[:, head, :, :]  # (1, 3, 4)\n",
    "        k_head = K_multi[:, head, :, :]  # (1, 3, 4)\n",
    "        v_head = V_multi[:, head, :, :]  # (1, 3, 4)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(q_head, k_head.transpose(-2, -1)) / math.sqrt(head_dim)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v_head)\n",
    "        \n",
    "        attention_outputs.append(attn_output)\n",
    "        print(f\"   头{head}: {q_head.shape} -> {attn_output.shape}\")\n",
    "    \n",
    "    print(f\"   每个头都产生了 {seq_len} 个词的 {head_dim} 维表示\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"🔸 步骤5: 合并多头结果\")\n",
    "    # 将所有头的输出合并\n",
    "    multi_head_output = torch.stack(attention_outputs, dim=1)  # (1, 4, 3, 4)\n",
    "    print(f\"   合并后形状: {multi_head_output.shape}\")\n",
    "    \n",
    "    # 重塑回原始形式\n",
    "    merged_output = multi_head_output.transpose(1, 2).contiguous().view(1, seq_len, dim)\n",
    "    print(f\"   重塑后形状: {merged_output.shape}\")\n",
    "    print(f\"   含义: 恢复到 {seq_len} 个词，每个词 {dim} 维特征\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"🔸 步骤6: 最终线性变换\")\n",
    "    final_output = torch.matmul(merged_output, W_o)\n",
    "    print(f\"   最终输出形状: {final_output.shape}\")\n",
    "    print(f\"   这与输入形状完全一致！\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"🎯 隐藏维度的关键作用总结\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"1. 📊 信息容量:\")\n",
    "    print(f\"   • {dim}维向量可以编码丰富的语言信息\")\n",
    "    print(f\"   • 更大的维度 = 更强的表示能力\")\n",
    "    print()\n",
    "    \n",
    "    print(\"2. 🔄 并行处理:\")\n",
    "    print(f\"   • 将{dim}维特征分割成{n_heads}个{head_dim}维子空间\")\n",
    "    print(f\"   • 每个头专注于不同的语言特征\")\n",
    "    print(f\"   • 并行计算提高效率\")\n",
    "    print()\n",
    "    \n",
    "    print(\"3. 🎨 特征多样性:\")\n",
    "    print(\"   • 不同的头可能学习：\")\n",
    "    print(\"     - 头0: 语法关系 (主谓宾)\")\n",
    "    print(\"     - 头1: 语义相似性\")\n",
    "    print(\"     - 头2: 位置信息\")\n",
    "    print(\"     - 头3: 情感倾向\")\n",
    "    print()\n",
    "    \n",
    "    print(\"4. 🔧 设计约束:\")\n",
    "    print(f\"   • dim 必须能被 n_heads 整除\")\n",
    "    print(f\"   • 当前: {dim} ÷ {n_heads} = {head_dim} ✓\")\n",
    "    print(f\"   • 这确保了均匀的特征分配\")\n",
    "    print()\n",
    "    \n",
    "    print(\"5. 💡 实际意义:\")\n",
    "    print(\"   • 输入输出维度保持一致\")\n",
    "    print(\"   • 可以叠加多个注意力层\")\n",
    "    print(\"   • 兼容残差连接和层归一化\")\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "hidden_dimension_complete_flow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_weights():\n",
    "    \"\"\"\n",
    "    可视化注意力权重，展示因果掩码的效果\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"注意力权重可视化演示\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 创建一个简单的多头注意力模块用于可视化\n",
    "    class SimpleMultiHeadAttention(nn.Module):\n",
    "        def __init__(self, dim, n_heads, is_causal=False):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            self.n_heads = n_heads\n",
    "            self.head_dim = dim // n_heads\n",
    "            self.is_causal = is_causal\n",
    "            \n",
    "            self.wq = nn.Linear(dim, dim, bias=False)\n",
    "            self.wk = nn.Linear(dim, dim, bias=False)\n",
    "            self.wv = nn.Linear(dim, dim, bias=False)\n",
    "            self.wo = nn.Linear(dim, dim, bias=False)\n",
    "            \n",
    "            if is_causal:\n",
    "                # 创建因果掩码\n",
    "                max_len = 10\n",
    "                mask = torch.full((max_len, max_len), float('-inf'))\n",
    "                mask = torch.triu(mask, diagonal=1)\n",
    "                self.register_buffer('mask', mask)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            B, T, C = x.shape\n",
    "            \n",
    "            # 计算 Q, K, V\n",
    "            q = self.wq(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "            k = self.wk(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "            v = self.wv(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "            \n",
    "            # 计算注意力分数\n",
    "            scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "            \n",
    "            # 应用因果掩码\n",
    "            if self.is_causal:\n",
    "                scores = scores + self.mask[:T, :T]\n",
    "            \n",
    "            # 计算注意力权重\n",
    "            attn_weights = F.softmax(scores, dim=-1)\n",
    "            \n",
    "            # 应用注意力权重\n",
    "            out = torch.matmul(attn_weights, v)\n",
    "            out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "            out = self.wo(out)\n",
    "            \n",
    "            return out, attn_weights\n",
    "    \n",
    "    # 创建两个模型：普通的和因果的\n",
    "    dim = 32\n",
    "    n_heads = 2\n",
    "    seq_len = 5\n",
    "    \n",
    "    model_normal = SimpleMultiHeadAttention(dim, n_heads, is_causal=False)\n",
    "    model_causal = SimpleMultiHeadAttention(dim, n_heads, is_causal=True)\n",
    "    \n",
    "    # 创建输入数据\n",
    "    x = torch.randn(1, seq_len, dim)\n",
    "    \n",
    "    print(f\"输入数据形状: {x.shape}\")\n",
    "    print(f\"模型配置: dim={dim}, n_heads={n_heads}, seq_len={seq_len}\")\n",
    "    print()\n",
    "    \n",
    "    # 运行两个模型\n",
    "    with torch.no_grad():\n",
    "        output_normal, attn_normal = model_normal(x)\n",
    "        output_causal, attn_causal = model_causal(x)\n",
    "    \n",
    "    print(\"普通多头注意力权重 (第一个头):\")\n",
    "    print(\"行：查询位置，列：键位置\")\n",
    "    print(\"数值表示每个查询位置对每个键位置的注意力权重\")\n",
    "    print(\"-\" * 50)\n",
    "    normal_weights = attn_normal[0, 0].numpy()  # 第一个样本，第一个头\n",
    "    for i in range(seq_len):\n",
    "        row_str = \" \".join([f\"{val:.3f}\" for val in normal_weights[i]])\n",
    "        print(f\"位置 {i}: [{row_str}]\")\n",
    "    print()\n",
    "    \n",
    "    print(\"因果多头注意力权重 (第一个头):\")\n",
    "    print(\"注意：上三角部分应该全为0（或很小），因为不能看到未来信息\")\n",
    "    print(\"-\" * 50)\n",
    "    causal_weights = attn_causal[0, 0].numpy()  # 第一个样本，第一个头\n",
    "    for i in range(seq_len):\n",
    "        row_str = \" \".join([f\"{val:.3f}\" for val in causal_weights[i]])\n",
    "        print(f\"位置 {i}: [{row_str}]\")\n",
    "    print()\n",
    "    \n",
    "    # 验证因果掩码的效果\n",
    "    print(\"验证因果掩码效果:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # 检查上三角部分是否为0\n",
    "    upper_triangle_sum = 0\n",
    "    for i in range(seq_len):\n",
    "        for j in range(i+1, seq_len):\n",
    "            upper_triangle_sum += causal_weights[i, j]\n",
    "    \n",
    "    print(f\"因果注意力权重上三角部分的和: {upper_triangle_sum:.6f}\")\n",
    "    print(\"(应该接近0，表示未来信息被屏蔽)\")\n",
    "    \n",
    "    # 检查每行的和是否为1\n",
    "    print(\"\\\\n每行注意力权重的和 (应该都约等于1):\")\n",
    "    for i in range(seq_len):\n",
    "        row_sum = causal_weights[i].sum()\n",
    "        print(f\"  位置 {i}: {row_sum:.6f}\")\n",
    "    \n",
    "    return attn_normal, attn_causal\n",
    "\n",
    "# 运行可视化演示\n",
    "visualize_attention_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ModelArgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''多头自注意力计算模块'''\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMultiHeadAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: ModelArgs, is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# 构造函数\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# args: 配置对象\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m, in \u001b[0;36mMultiHeadAttention\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMultiHeadAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: ModelArgs, is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# 构造函数\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# args: 配置对象\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# 隐藏层维度必须是头数的整数倍，因为后面我们会将输入拆成头数个矩阵\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ModelArgs' is not defined"
     ]
    }
   ],
   "source": [
    "#define the ModelArgs Type\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    \"\"\"模型配置参数类\"\"\"\n",
    "    dim: int = 512                    # 模型维度\n",
    "    n_heads: int = 8                  # 注意力头数\n",
    "    n_layers: int = 6                 # 层数\n",
    "    vocab_size: int = 50000           # 词汇表大小\n",
    "    max_seq_len: int = 2048           # 最大序列长度\n",
    "    dropout: float = 0.1              # Dropout 比率\n",
    "    bias: bool = False                # 是否使用偏置\n",
    "\n",
    "\n",
    "'''多头自注意力计算模块'''\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelArgs, is_causal=False):\n",
    "        # 构造函数\n",
    "        # args: 配置对象\n",
    "        super().__init__()\n",
    "        # 隐藏层维度必须是头数的整数倍，因为后面我们会将输入拆成头数个矩阵\n",
    "        assert args.dim % args.n_heads == 0\n",
    "        # 模型并行处理大小，默认为1。\n",
    "        model_parallel_size = 1\n",
    "        # 本地计算头数，等于总头数除以模型并行处理大小。\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        # 每个头的维度，等于模型维度除以头的总数。\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        # Wq, Wk, Wv 参数矩阵，每个参数矩阵为 n_embd x n_embd\n",
    "        # 这里通过三个组合矩阵来代替了n个参数矩阵的组合，其逻辑在于矩阵内积再拼接其实等同于拼接矩阵再内积，\n",
    "        # 不理解的读者可以自行模拟一下，每一个线性层其实相当于n个参数矩阵的拼接\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        # 输出权重矩阵，维度为 dim x n_embd（head_dim = n_embeds / n_heads）\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "        # 注意力的 dropout\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        # 残差连接的 dropout\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "         \n",
    "        # 创建一个上三角矩阵，用于遮蔽未来信息\n",
    "        # 注意，因为是多头注意力，Mask 矩阵比之前我们定义的多一个维度\n",
    "        if is_causal:\n",
    "           mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))\n",
    "           mask = torch.triu(mask, diagonal=1)\n",
    "           # 注册为模型的缓冲区\n",
    "           self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
    "\n",
    "        # 获取批次大小和序列长度，[batch_size, seq_len, dim]\n",
    "        bsz, seqlen, _ = q.shape\n",
    "\n",
    "        # 计算查询（Q）、键（K）、值（V）,输入通过参数矩阵层，维度为 (B, T, n_embed) x (n_embed, n_embed) -> (B, T, n_embed)\n",
    "        xq, xk, xv = self.wq(q), self.wk(k), self.wv(v)\n",
    "\n",
    "        # 将 Q、K、V 拆分成多头，维度为 (B, T, n_head, C // n_head)，然后交换维度，变成 (B, n_head, T, C // n_head)\n",
    "        # 因为在注意力计算中我们是取了后两个维度参与计算\n",
    "        # 为什么要先按B*T*n_head*C//n_head展开再互换1、2维度而不是直接按注意力输入展开，是因为view的展开方式是直接把输入全部排开，\n",
    "        # 然后按要求构造，可以发现只有上述操作能够实现我们将每个头对应部分取出来的目标\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "\n",
    "\n",
    "        # 注意力计算\n",
    "        # 计算 QK^T / sqrt(d_k)，维度为 (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        # 掩码自注意力必须有注意力掩码\n",
    "        if self.is_causal:\n",
    "            assert hasattr(self, 'mask')\n",
    "            # 这里截取到序列长度，因为有些序列可能比 max_seq_len 短\n",
    "            scores = scores + self.mask[:, :, :seqlen, :seqlen]\n",
    "        # 计算 softmax，维度为 (B, nh, T, T)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        # 做 Dropout\n",
    "        scores = self.attn_dropout(scores)\n",
    "        # V * Score，维度为(B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        output = torch.matmul(scores, xv)\n",
    "\n",
    "        # 恢复时间维度并合并头。\n",
    "        # 将多头的结果拼接起来, 先交换维度为 (B, T, n_head, C // n_head)，再拼接成 (B, T, n_head * C // n_head)\n",
    "        # contiguous 函数用于重新开辟一块新内存存储，因为Pytorch设置先transpose再view会报错，\n",
    "        # 因为view直接基于底层存储得到，然而transpose并不会改变底层存储，因此需要额外存储\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        # 最终投影回残差流。\n",
    "        output = self.wo(output)\n",
    "        output = self.resid_dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
