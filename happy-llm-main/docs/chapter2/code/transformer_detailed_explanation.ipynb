{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Transformer æ¶æ„è¯¦ç»†è§£é‡Š\n",
        "\n",
        "æœ¬notebookå°†è¯¦ç»†è§£é‡ŠTransformeræ¨¡å‹çš„æ¯ä¸ªç»„ä»¶ï¼ŒåŒ…æ‹¬æ•°å­¦å…¬å¼ã€ä»£ç å®ç°å’Œå®é™…ä¾‹å­ã€‚\n",
        "\n",
        "## ç›®å½•\n",
        "1. [é…ç½®ç±» ModelArgs](#1-é…ç½®ç±»-modelargs)\n",
        "2. [å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ MultiHeadAttention](#2-å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶-multiheadattention)\n",
        "3. [å±‚å½’ä¸€åŒ– LayerNorm](#3-å±‚å½’ä¸€åŒ–-layernorm)\n",
        "4. [å‰é¦ˆç¥ç»ç½‘ç»œ MLP](#4-å‰é¦ˆç¥ç»ç½‘ç»œ-mlp)\n",
        "5. [ç¼–ç å™¨ç»„ä»¶](#5-ç¼–ç å™¨ç»„ä»¶)\n",
        "6. [è§£ç å™¨ç»„ä»¶](#6-è§£ç å™¨ç»„ä»¶)\n",
        "7. [ä½ç½®ç¼–ç  PositionalEncoding](#7-ä½ç½®ç¼–ç -positionalencoding)\n",
        "8. [å®Œæ•´çš„Transformeræ¨¡å‹](#8-å®Œæ•´çš„transformeræ¨¡å‹)\n",
        "9. [ç¤ºä¾‹è¿è¡Œ](#9-ç¤ºä¾‹è¿è¡Œ)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# å¯¼å…¥å¿…è¦çš„åº“\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mmath\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "from dataclasses import dataclass\n",
        "from transformers import BertTokenizer\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸï¼\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. é…ç½®ç±» ModelArgs\n",
        "\n",
        "é¦–å…ˆå®šä¹‰æ¨¡å‹çš„è¶…å‚æ•°é…ç½®ã€‚ä½¿ç”¨`@dataclass`è£…é¥°å™¨å¯ä»¥è‡ªåŠ¨ç”Ÿæˆåˆå§‹åŒ–æ–¹æ³•ã€‚\n",
        "\n",
        "è¿™ä¸ªé…ç½®ç±»åŒ…å«äº†Transformeræ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰è¶…å‚æ•°ï¼š\n",
        "- `n_embd`: åµŒå…¥ç»´åº¦ï¼Œå†³å®šäº†tokenå‘é‡çš„å¤§å°\n",
        "- `n_heads`: å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°ï¼Œé€šå¸¸ä¸º8æˆ–16\n",
        "- `dim`: æ¨¡å‹çš„éšè—ç»´åº¦\n",
        "- `dropout`: é˜²æ­¢è¿‡æ‹Ÿåˆçš„dropoutç‡\n",
        "- `max_seq_len`: èƒ½å¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦\n",
        "- `vocab_size`: è¯æ±‡è¡¨å¤§å°\n",
        "- `block_size`: å¤„ç†çš„å—å¤§å°\n",
        "- `n_layer`: Transformerå±‚æ•°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "    \"\"\"æ¨¡å‹é…ç½®å‚æ•°\"\"\"\n",
        "    n_embd: int       # åµŒå…¥ç»´åº¦ (Embedding dimension)\n",
        "    n_heads: int      # å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•° (Number of attention heads)  \n",
        "    dim: int          # æ¨¡å‹ç»´åº¦ (Model dimension)\n",
        "    dropout: float    # Dropoutç‡ (Dropout rate)\n",
        "    max_seq_len: int  # æœ€å¤§åºåˆ—é•¿åº¦ (Maximum sequence length)\n",
        "    vocab_size: int   # è¯æ±‡è¡¨å¤§å° (Vocabulary size)\n",
        "    block_size: int   # å—å¤§å° (Block size)\n",
        "    n_layer: int      # å±‚æ•° (Number of layers)\n",
        "\n",
        "# ç¤ºä¾‹é…ç½®\n",
        "args_example = ModelArgs(\n",
        "    n_embd=512,      # åµŒå…¥ç»´åº¦\n",
        "    n_heads=8,       # 8ä¸ªæ³¨æ„åŠ›å¤´\n",
        "    dim=512,         # æ¨¡å‹ç»´åº¦  \n",
        "    dropout=0.1,     # 10% dropout\n",
        "    max_seq_len=1024, # æœ€å¤§1024ä¸ªtoken\n",
        "    vocab_size=30000, # 3ä¸‡è¯æ±‡é‡\n",
        "    block_size=1024,  # å—å¤§å°\n",
        "    n_layer=6        # 6å±‚\n",
        ")\n",
        "\n",
        "print(f\"é…ç½®ç¤ºä¾‹: {args_example}\")\n",
        "print(f\"æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦: {args_example.dim // args_example.n_heads}\")\n",
        "print(f\"æ¨¡å‹æ€»å‚æ•°é‡ä¼°è®¡: ~{(args_example.vocab_size * args_example.n_embd + args_example.n_layer * 4 * args_example.dim**2) / 1e6:.1f}M\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ MultiHeadAttention\n",
        "\n",
        "å¤šå¤´æ³¨æ„åŠ›æ˜¯Transformerçš„æ ¸å¿ƒç»„ä»¶ã€‚å®ƒå…è®¸æ¨¡å‹åŒæ—¶å…³æ³¨åºåˆ—ä¸­ä¸åŒä½ç½®çš„ä¿¡æ¯ã€‚\n",
        "\n",
        "### æ•°å­¦å…¬å¼ï¼š\n",
        "\n",
        "**å•å¤´æ³¨æ„åŠ›å…¬å¼ï¼š**\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "**å¤šå¤´æ³¨æ„åŠ›å…¬å¼ï¼š**\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
        "\n",
        "å…¶ä¸­ï¼š\n",
        "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
        "\n",
        "### å…³é”®æ¦‚å¿µï¼š\n",
        "- **Q (Query)**: æŸ¥è¯¢çŸ©é˜µï¼Œå†³å®š\"æˆ‘è¦æ‰¾ä»€ä¹ˆ\"\n",
        "- **K (Key)**: é”®çŸ©é˜µï¼Œå†³å®š\"æˆ‘æœ‰ä»€ä¹ˆå¯ä»¥è¢«æ‰¾åˆ°\"\n",
        "- **V (Value)**: å€¼çŸ©é˜µï¼Œå†³å®š\"æ‰¾åˆ°åè¿”å›ä»€ä¹ˆä¿¡æ¯\"\n",
        "- **æ³¨æ„åŠ›åˆ†æ•°**: $\\frac{QK^T}{\\sqrt{d_k}}$ è®¡ç®—æŸ¥è¯¢å’Œé”®çš„ç›¸ä¼¼åº¦\n",
        "- **ç¼©æ”¾å› å­**: $\\sqrt{d_k}$ é˜²æ­¢softmaxè¿›å…¥é¥±å’ŒåŒºåŸŸ\n",
        "- **å¤šå¤´**: å¹¶è¡Œè®¡ç®—å¤šä¸ªä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ - é€è¡Œè¯¦ç»†è§£é‡Š\"\"\"\n",
        "    \n",
        "    def __init__(self, args: ModelArgs, is_causal=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        # ç¬¬17è¡Œï¼šæ£€æŸ¥ç»´åº¦èƒ½å¦è¢«å¤´æ•°æ•´é™¤\n",
        "        assert args.dim % args.n_heads == 0, f\"dim({args.dim})å¿…é¡»èƒ½è¢«n_heads({args.n_heads})æ•´é™¤\"\n",
        "        \n",
        "        # ç¬¬19-20è¡Œï¼šæ¨¡å‹å¹¶è¡Œç›¸å…³ï¼ˆè¿™é‡Œç®€åŒ–ä¸º1ï¼‰\n",
        "        model_parallel_size = 1\n",
        "        self.n_local_heads = args.n_heads // model_parallel_size\n",
        "        \n",
        "        # ç¬¬22è¡Œï¼šè®¡ç®—æ¯ä¸ªå¤´çš„ç»´åº¦\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "        \n",
        "        print(f\"å¤šå¤´æ³¨æ„åŠ›åˆå§‹åŒ–:\")\n",
        "        print(f\"  æ€»ç»´åº¦: {args.dim}\")\n",
        "        print(f\"  å¤´æ•°: {self.n_local_heads}\")  \n",
        "        print(f\"  æ¯ä¸ªå¤´ç»´åº¦: {self.head_dim}\")\n",
        "        \n",
        "        # ç¬¬29-31è¡Œï¼šQã€Kã€Vçš„æƒé‡çŸ©é˜µ\n",
        "        # æ³¨æ„ï¼šè¿™é‡Œç”¨ä¸€ä¸ªå¤§çŸ©é˜µä»£æ›¿å¤šä¸ªå°çŸ©é˜µï¼Œæé«˜æ•ˆç‡\n",
        "        self.wq = nn.Linear(args.n_embd, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wk = nn.Linear(args.n_embd, args.n_heads * self.head_dim, bias=False) \n",
        "        self.wv = nn.Linear(args.n_embd, args.n_heads * self.head_dim, bias=False)\n",
        "        \n",
        "        # ç¬¬33è¡Œï¼šè¾“å‡ºæŠ•å½±çŸ©é˜µW^O\n",
        "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
        "        \n",
        "        # ç¬¬35-37è¡Œï¼šDropoutå±‚\n",
        "        self.attn_dropout = nn.Dropout(args.dropout)\n",
        "        self.resid_dropout = nn.Dropout(args.dropout)\n",
        "        self.is_causal = is_causal\n",
        "        \n",
        "        # ç¬¬39-46è¡Œï¼šåˆ›å»ºå› æœæ©ç ï¼ˆç”¨äºè§£ç å™¨ï¼‰\n",
        "        if is_causal:\n",
        "            # åˆ›å»ºä¸Šä¸‰è§’æ©ç ï¼Œé˜²æ­¢çœ‹åˆ°\"æœªæ¥\"ä¿¡æ¯\n",
        "            mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))\n",
        "            mask = torch.triu(mask, diagonal=1)  # ä¸Šä¸‰è§’çŸ©é˜µ\n",
        "            self.register_buffer(\"mask\", mask)\n",
        "            print(f\"  åˆ›å»ºå› æœæ©ç : {mask.shape}\")\n",
        "    \n",
        "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
        "        \"\"\"å‰å‘ä¼ æ’­ - é€æ­¥è§£é‡Šæ³¨æ„åŠ›è®¡ç®—è¿‡ç¨‹\"\"\"\n",
        "        \n",
        "        # ç¬¬51è¡Œï¼šè·å–æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦\n",
        "        bsz, seqlen, _ = q.shape\n",
        "        print(f\"\\n=== å¤šå¤´æ³¨æ„åŠ›å‰å‘ä¼ æ’­ ===\")\n",
        "        print(f\"è¾“å…¥å½¢çŠ¶: batch={bsz}, seq_len={seqlen}, dim={q.shape[2]}\")\n",
        "        \n",
        "        # ç¬¬54è¡Œï¼šé€šè¿‡çº¿æ€§å±‚è®¡ç®—Qã€Kã€V\n",
        "        # [B, T, n_embd] -> [B, T, n_heads * head_dim]\n",
        "        xq, xk, xv = self.wq(q), self.wk(k), self.wv(v)\n",
        "        print(f\"QKVçº¿æ€§å˜æ¢å: {xq.shape}\")\n",
        "        \n",
        "        # ç¬¬59-62è¡Œï¼šé‡å¡‘ä¸ºå¤šå¤´æ ¼å¼\n",
        "        # [B, T, n_heads * head_dim] -> [B, T, n_heads, head_dim]  \n",
        "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
        "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
        "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
        "        \n",
        "        # ç¬¬63-65è¡Œï¼šè½¬ç½®åˆ°æ³¨æ„åŠ›è®¡ç®—æ ¼å¼\n",
        "        # [B, T, n_heads, head_dim] -> [B, n_heads, T, head_dim]\n",
        "        xq = xq.transpose(1, 2)\n",
        "        xk = xk.transpose(1, 2) \n",
        "        xv = xv.transpose(1, 2)\n",
        "        print(f\"å¤šå¤´é‡å¡‘å: {xq.shape}\")\n",
        "        \n",
        "        # ç¬¬68è¡Œï¼šè®¡ç®—æ³¨æ„åŠ›åˆ†æ•° QK^T/âˆšd_k\n",
        "        # [B, n_heads, T, head_dim] Ã— [B, n_heads, head_dim, T] -> [B, n_heads, T, T]\n",
        "        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "        print(f\"æ³¨æ„åŠ›åˆ†æ•°: {scores.shape}, ç¼©æ”¾å› å­: {math.sqrt(self.head_dim):.2f}\")\n",
        "        \n",
        "        # ç¬¬70-73è¡Œï¼šåº”ç”¨å› æœæ©ç ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
        "        if self.is_causal:\n",
        "            assert hasattr(self, 'mask')\n",
        "            scores = scores + self.mask[:, :, :seqlen, :seqlen]\n",
        "            print(f\"åº”ç”¨æ©ç ååˆ†æ•°èŒƒå›´: [{scores.min():.2f}, {scores.max():.2f}]\")\n",
        "        \n",
        "        # ç¬¬75-77è¡Œï¼šSoftmax + Dropout\n",
        "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "        scores = self.attn_dropout(scores)\n",
        "        \n",
        "        # ç¬¬79è¡Œï¼šæ³¨æ„åŠ›åŠ æƒæ±‚å’Œ\n",
        "        # [B, n_heads, T, T] Ã— [B, n_heads, T, head_dim] -> [B, n_heads, T, head_dim]\n",
        "        output = torch.matmul(scores, xv)\n",
        "        print(f\"æ³¨æ„åŠ›è¾“å‡º: {output.shape}\")\n",
        "        \n",
        "        # ç¬¬84è¡Œï¼šåˆå¹¶å¤šå¤´\n",
        "        # [B, n_heads, T, head_dim] -> [B, T, n_heads, head_dim] -> [B, T, n_heads*head_dim]\n",
        "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
        "        \n",
        "        # ç¬¬87-88è¡Œï¼šæœ€ç»ˆæŠ•å½±å’Œdropout\n",
        "        output = self.wo(output)\n",
        "        output = self.resid_dropout(output)\n",
        "        \n",
        "        print(f\"æœ€ç»ˆè¾“å‡º: {output.shape}\")\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶\n",
        "print(\"=== æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ ===\")\n",
        "\n",
        "# åˆ›å»ºå°å‹é…ç½®ç”¨äºæµ‹è¯•\n",
        "args_small = ModelArgs(\n",
        "    n_embd=64,      # åµŒå…¥ç»´åº¦\n",
        "    n_heads=4,      # 4ä¸ªæ³¨æ„åŠ›å¤´\n",
        "    dim=64,         # æ¨¡å‹ç»´åº¦\n",
        "    dropout=0.1,    # dropoutç‡\n",
        "    max_seq_len=128, # æœ€å¤§åºåˆ—é•¿åº¦\n",
        "    vocab_size=1000, # è¯æ±‡è¡¨å¤§å°\n",
        "    block_size=128,  # å—å¤§å°\n",
        "    n_layer=2       # å±‚æ•°\n",
        ")\n",
        "\n",
        "# åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›å±‚ï¼ˆç¼–ç å™¨ç‰ˆæœ¬ï¼Œæ— æ©ç ï¼‰\n",
        "print(\"\\n1. åˆ›å»ºç¼–ç å™¨ç‰ˆæœ¬çš„å¤šå¤´æ³¨æ„åŠ›:\")\n",
        "mha_encoder = MultiHeadAttention(args_small, is_causal=False)\n",
        "\n",
        "# åˆ›å»ºæµ‹è¯•è¾“å…¥\n",
        "batch_size, seq_len = 2, 8\n",
        "x = torch.randn(batch_size, seq_len, args_small.n_embd)\n",
        "print(f\"\\n2. åˆ›å»ºæµ‹è¯•è¾“å…¥: {x.shape}\")\n",
        "\n",
        "# å‰å‘ä¼ æ’­ï¼ˆè‡ªæ³¨æ„åŠ›ï¼šQ=K=Vï¼‰\n",
        "print(f\"\\n3. æ‰§è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—...\")\n",
        "with torch.no_grad():\n",
        "    output_encoder = mha_encoder(x, x, x)\n",
        "\n",
        "print(f\"\\nç¼–ç å™¨è‡ªæ³¨æ„åŠ›å®Œæˆï¼\")\n",
        "print(f\"è¾“å…¥: {x.shape} -> è¾“å‡º: {output_encoder.shape}\")\n",
        "\n",
        "# åˆ›å»ºè§£ç å™¨ç‰ˆæœ¬ï¼ˆå¸¦æ©ç ï¼‰\n",
        "print(f\"\\n4. åˆ›å»ºè§£ç å™¨ç‰ˆæœ¬çš„å¤šå¤´æ³¨æ„åŠ›:\")\n",
        "mha_decoder = MultiHeadAttention(args_small, is_causal=True)\n",
        "\n",
        "# æµ‹è¯•è§£ç å™¨ç‰ˆæœ¬\n",
        "print(f\"\\n5. æ‰§è¡Œæ©ç è‡ªæ³¨æ„åŠ›è®¡ç®—...\")\n",
        "with torch.no_grad():\n",
        "    output_decoder = mha_decoder(x, x, x)\n",
        "\n",
        "print(f\"\\nè§£ç å™¨æ©ç è‡ªæ³¨æ„åŠ›å®Œæˆï¼\")\n",
        "print(f\"è¾“å…¥: {x.shape} -> è¾“å‡º: {output_decoder.shape}\")\n",
        "\n",
        "print(f\"\\n=== å¤šå¤´æ³¨æ„åŠ›æµ‹è¯•å®Œæˆ ===\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. å®Œæ•´Transformerä»£ç è§£é‡Š\n",
        "\n",
        "ç°åœ¨è®©æˆ‘ä»¬åŠ è½½å¹¶è§£é‡Šå®Œæ•´çš„Transformerä»£ç ã€‚æˆ‘å°†é€ä¸ªç»„ä»¶è¿›è¡Œè¯¦ç»†è¯´æ˜ï¼š\n",
        "\n",
        "### å±‚å½’ä¸€åŒ– LayerNorm (ç¬¬95-110è¡Œ)\n",
        "å…¬å¼ï¼š$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta$$\n",
        "\n",
        "- å¯¹æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾ç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–\n",
        "- ä¸BatchNormä¸åŒï¼ŒLayerNormåœ¨ç‰¹å¾ç»´åº¦ä¸Šæ ‡å‡†åŒ–ï¼Œè€Œä¸æ˜¯æ‰¹æ¬¡ç»´åº¦\n",
        "\n",
        "### å‰é¦ˆç¥ç»ç½‘ç»œ MLP (ç¬¬112-128è¡Œ)  \n",
        "å…¬å¼ï¼š$$\\text{FFN}(x) = \\text{dropout}(W_2 \\cdot \\text{ReLU}(W_1 \\cdot x))$$\n",
        "\n",
        "- ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ\n",
        "- é€šå¸¸éšè—å±‚ç»´åº¦æ˜¯è¾“å…¥ç»´åº¦çš„4å€\n",
        "- ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°\n",
        "\n",
        "### ä½ç½®ç¼–ç  PositionalEncoding (ç¬¬246-278è¡Œ)\n",
        "å…¬å¼ï¼š\n",
        "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "\n",
        "- ä¸ºåºåˆ—æ·»åŠ ä½ç½®ä¿¡æ¯\n",
        "- ä½¿ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ï¼Œå…è®¸æ¨¡å‹å­¦ä¹ ç›¸å¯¹ä½ç½®å…³ç³»\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®Œæ•´çš„Transformerå®ç° - ä»åŸå§‹ä»£ç é€è¡Œè§£é‡Š\n",
        "\n",
        "# å…ˆå®šä¹‰å‰©ä½™çš„æ ¸å¿ƒç»„ä»¶\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    '''Layer Norm å±‚ - ç¬¬95-110è¡Œè¯¦è§£'''\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        # çº¿æ€§å˜æ¢å‚æ•° - å¯å­¦ä¹ çš„ç¼©æ”¾å’Œåç§»\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))    # Î³ (gamma) ç¼©æ”¾å‚æ•°\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))   # Î² (beta) åç§»å‚æ•°\n",
        "        self.eps = eps  # é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # åœ¨æœ€åä¸€ä¸ªç»´åº¦è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®\n",
        "        mean = x.mean(-1, keepdim=True)  # å‡å€¼ï¼š[batch, seq_len, 1]\n",
        "        std = x.std(-1, keepdim=True)    # æ ‡å‡†å·®ï¼š[batch, seq_len, 1]\n",
        "        # LayerNormå…¬å¼ï¼šÎ³ * (x-Î¼)/(Ïƒ+Îµ) + Î²\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    '''å‰é¦ˆç¥ç»ç½‘ç»œ - ç¬¬112-128è¡Œè¯¦è§£'''\n",
        "    def __init__(self, dim: int, hidden_dim: int, dropout: float):\n",
        "        super().__init__()\n",
        "        # ç¬¬ä¸€å±‚ï¼šç»´åº¦æ‰©å±• (é€šå¸¸ hidden_dim = 4 * dim)\n",
        "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        # ç¬¬äºŒå±‚ï¼šæ¢å¤åŸå§‹ç»´åº¦\n",
        "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
        "        # Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # FFNå…¬å¼ï¼šdropout(W2 * ReLU(W1 * x))\n",
        "        return self.dropout(self.w2(F.relu(self.w1(x))))\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    '''ä½ç½®ç¼–ç  - ç¬¬246-278è¡Œè¯¦è§£'''\n",
        "    def __init__(self, args):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=args.dropout)\n",
        "\n",
        "        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ [block_size, n_embd]\n",
        "        pe = torch.zeros(args.block_size, args.n_embd)\n",
        "        position = torch.arange(0, args.block_size).unsqueeze(1)  # [block_size, 1]\n",
        "        \n",
        "        # è®¡ç®—é¢‘ç‡é¡¹ï¼š10000^(-2i/d_model)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, args.n_embd, 2) * -(math.log(10000.0) / args.n_embd)\n",
        "        )\n",
        "        \n",
        "        # ä½ç½®ç¼–ç å…¬å¼ï¼š\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ä½ç½®ç”¨sin\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ä½ç½®ç”¨cos\n",
        "        \n",
        "        pe = pe.unsqueeze(0)  # æ·»åŠ batchç»´åº¦ï¼š[1, block_size, n_embd]\n",
        "        self.register_buffer(\"pe\", pe)  # æ³¨å†Œä¸ºç¼“å†²åŒºï¼ˆä¸æ›´æ–°æ¢¯åº¦ï¼‰\n",
        "\n",
        "    def forward(self, x):\n",
        "        # å°†ä½ç½®ç¼–ç åŠ åˆ°tokenåµŒå…¥ä¸Š\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "print(\"æ ¸å¿ƒç»„ä»¶å®šä¹‰å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç¼–ç å™¨å’Œè§£ç å™¨å®ç°\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"ç¼–ç å™¨å±‚ - ç¬¬131-147è¡Œè¯¦è§£\"\"\"\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        # Pre-LayerNormæ¶æ„ï¼šå…ˆå½’ä¸€åŒ–å†è®¡ç®—\n",
        "        self.attention_norm = LayerNorm(args.n_embd)\n",
        "        # ç¼–ç å™¨ä½¿ç”¨éå› æœï¼ˆåŒå‘ï¼‰æ³¨æ„åŠ›\n",
        "        self.attention = MultiHeadAttention(args, is_causal=False)\n",
        "        self.fnn_norm = LayerNorm(args.n_embd)\n",
        "        # FFNéšè—å±‚ç»´åº¦é€šå¸¸æ˜¯è¾“å…¥ç»´åº¦çš„4å€\n",
        "        self.feed_forward = MLP(args.dim, args.dim * 4, args.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ç¬¬ä¸€ä¸ªå­å±‚ï¼šå¤šå¤´è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥\n",
        "        # Pre-Normï¼šå…ˆLayerNormå†æ³¨æ„åŠ›\n",
        "        norm_x = self.attention_norm(x)\n",
        "        h = x + self.attention.forward(norm_x, norm_x, norm_x)  # æ®‹å·®è¿æ¥\n",
        "        \n",
        "        # ç¬¬äºŒä¸ªå­å±‚ï¼šå‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥  \n",
        "        norm_h = self.fnn_norm(h)\n",
        "        out = h + self.feed_forward.forward(norm_h)  # æ®‹å·®è¿æ¥\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    '''ç¼–ç å™¨ - ç¬¬149-160è¡Œè¯¦è§£'''\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "        # å †å Nä¸ªç¼–ç å™¨å±‚\n",
        "        self.layers = nn.ModuleList([EncoderLayer(args) for _ in range(args.n_layer)])\n",
        "        # æœ€ç»ˆçš„å±‚å½’ä¸€åŒ–\n",
        "        self.norm = LayerNorm(args.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # é€å±‚é€šè¿‡ç¼–ç å™¨\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.norm(x)  # æœ€ç»ˆå½’ä¸€åŒ–\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    '''è§£ç å™¨å±‚ - ç¬¬162-186è¡Œè¯¦è§£'''\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        # ç¬¬ä¸€ä¸ªæ³¨æ„åŠ›ï¼šæ©ç è‡ªæ³¨æ„åŠ›\n",
        "        self.attention_norm_1 = LayerNorm(args.n_embd)\n",
        "        self.mask_attention = MultiHeadAttention(args, is_causal=True)\n",
        "        \n",
        "        # ç¬¬äºŒä¸ªæ³¨æ„åŠ›ï¼šç¼–ç å™¨-è§£ç å™¨äº¤å‰æ³¨æ„åŠ›\n",
        "        self.attention_norm_2 = LayerNorm(args.n_embd)\n",
        "        self.attention = MultiHeadAttention(args, is_causal=False)\n",
        "        \n",
        "        # å‰é¦ˆç½‘ç»œ\n",
        "        self.ffn_norm = LayerNorm(args.n_embd)\n",
        "        self.feed_forward = MLP(args.dim, args.dim * 4, args.dropout)\n",
        "\n",
        "    def forward(self, x, enc_out):\n",
        "        # å­å±‚1ï¼šæ©ç è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥\n",
        "        norm_x1 = self.attention_norm_1(x)\n",
        "        x = x + self.mask_attention.forward(norm_x1, norm_x1, norm_x1)\n",
        "        \n",
        "        # å­å±‚2ï¼šç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ› + æ®‹å·®è¿æ¥\n",
        "        # Qæ¥è‡ªè§£ç å™¨ï¼ŒKå’ŒVæ¥è‡ªç¼–ç å™¨\n",
        "        norm_x2 = self.attention_norm_2(x)\n",
        "        h = x + self.attention.forward(norm_x2, enc_out, enc_out)\n",
        "        \n",
        "        # å­å±‚3ï¼šå‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥\n",
        "        norm_h = self.ffn_norm(h)\n",
        "        out = h + self.feed_forward.forward(norm_h)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    '''è§£ç å™¨ - ç¬¬188-199è¡Œè¯¦è§£'''\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "        # å †å Nä¸ªè§£ç å™¨å±‚\n",
        "        self.layers = nn.ModuleList([DecoderLayer(args) for _ in range(args.n_layer)])\n",
        "        # æœ€ç»ˆçš„å±‚å½’ä¸€åŒ–\n",
        "        self.norm = LayerNorm(args.n_embd)\n",
        "\n",
        "    def forward(self, x, enc_out):\n",
        "        # é€å±‚é€šè¿‡è§£ç å™¨ï¼Œæ¯å±‚éƒ½éœ€è¦ç¼–ç å™¨è¾“å‡º\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out)\n",
        "        return self.norm(x)\n",
        "\n",
        "print(\"ç¼–ç å™¨å’Œè§£ç å™¨å®šä¹‰å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®Œæ•´çš„Transformeræ¨¡å‹\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    '''å®Œæ•´çš„Transformeræ¨¡å‹ - ç¬¬280-349è¡Œè¯¦è§£'''\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        # éªŒè¯å¿…è¦å‚æ•°\n",
        "        assert args.vocab_size is not None, \"å¿…é¡»æŒ‡å®švocab_size\"\n",
        "        assert args.block_size is not None, \"å¿…é¡»æŒ‡å®šblock_size\"\n",
        "        \n",
        "        self.args = args\n",
        "        \n",
        "        # ä¸»è¦ç»„ä»¶ - ä½¿ç”¨ModuleDictç®¡ç†\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(args.vocab_size, args.n_embd),  # TokenåµŒå…¥\n",
        "            wpe=PositionalEncoding(args),                     # ä½ç½®ç¼–ç \n",
        "            drop=nn.Dropout(args.dropout),                    # Dropout\n",
        "            encoder=Encoder(args),                            # ç¼–ç å™¨\n",
        "            decoder=Decoder(args),                            # è§£ç å™¨\n",
        "        ))\n",
        "        \n",
        "        # è¯­è¨€å»ºæ¨¡å¤´ï¼šå°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨\n",
        "        self.lm_head = nn.Linear(args.n_embd, args.vocab_size, bias=False)\n",
        "        \n",
        "        # æƒé‡åˆå§‹åŒ–\n",
        "        self.apply(self._init_weights)\n",
        "        \n",
        "        # æ‰“å°å‚æ•°ç»Ÿè®¡\n",
        "        print(f\"æ¨¡å‹å‚æ•°æ•°é‡: {self.get_num_params()/1e6:.2f}M\")\n",
        "\n",
        "    def get_num_params(self, non_embedding=False):\n",
        "        \"\"\"ç»Ÿè®¡å‚æ•°æ•°é‡\"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wte.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"æƒé‡åˆå§‹åŒ– - ä½¿ç”¨æ­£æ€åˆ†å¸ƒ\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"å‰å‘ä¼ æ’­ - ç¬¬326-349è¡Œè¯¦è§£\n",
        "        \n",
        "        Args:\n",
        "            idx: è¾“å…¥tokenç´¢å¼• [batch_size, seq_len]\n",
        "            targets: ç›®æ ‡åºåˆ—ï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰ [batch_size, seq_len]\n",
        "        \n",
        "        Returns:\n",
        "            logits: è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ\n",
        "            loss: æŸå¤±å€¼ï¼ˆå¦‚æœæä¾›targetsï¼‰\n",
        "        \"\"\"\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        \n",
        "        # æ£€æŸ¥åºåˆ—é•¿åº¦é™åˆ¶\n",
        "        assert t <= self.args.block_size, f\"åºåˆ—é•¿åº¦{t}è¶…è¿‡æœ€å¤§é•¿åº¦{self.args.block_size}\"\n",
        "\n",
        "        print(f\"\\\\n=== Transformerå‰å‘ä¼ æ’­ ===\")\n",
        "        print(f\"è¾“å…¥idxå½¢çŠ¶: {idx.shape}\")\n",
        "\n",
        "        # 1. TokenåµŒå…¥ï¼šå°†tokenç´¢å¼•è½¬æ¢ä¸ºå‘é‡\n",
        "        tok_emb = self.transformer.wte(idx)  # [B, T, n_embd]\n",
        "        print(f\"TokenåµŒå…¥å: {tok_emb.shape}\")\n",
        "\n",
        "        # 2. ä½ç½®ç¼–ç ï¼šæ·»åŠ ä½ç½®ä¿¡æ¯\n",
        "        pos_emb = self.transformer.wpe(tok_emb)\n",
        "        print(f\"ä½ç½®ç¼–ç å: {pos_emb.shape}\")\n",
        "\n",
        "        # 3. Dropout\n",
        "        x = self.transformer.drop(pos_emb)\n",
        "        print(f\"Dropoutå: {x.shape}\")\n",
        "\n",
        "        # 4. ç¼–ç å™¨ï¼šç†è§£è¾“å…¥åºåˆ—\n",
        "        enc_out = self.transformer.encoder(x)\n",
        "        print(f\"ç¼–ç å™¨è¾“å‡º: {enc_out.shape}\")\n",
        "\n",
        "        # 5. è§£ç å™¨ï¼šç”Ÿæˆè¾“å‡ºåºåˆ—\n",
        "        x = self.transformer.decoder(x, enc_out)\n",
        "        print(f\"è§£ç å™¨è¾“å‡º: {x.shape}\")\n",
        "\n",
        "        if targets is not None:\n",
        "            # è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—æ‰€æœ‰ä½ç½®çš„æŸå¤±\n",
        "            logits = self.lm_head(x)  # [B, T, vocab_size]\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)), \n",
        "                targets.view(-1), \n",
        "                ignore_index=-1\n",
        "            )\n",
        "            print(f\"è®­ç»ƒæ¨¡å¼ - logits: {logits.shape}, loss: {loss.item():.4f}\")\n",
        "        else:\n",
        "            # æ¨ç†æ¨¡å¼ï¼šåªè®¡ç®—æœ€åä¸€ä¸ªä½ç½®\n",
        "            logits = self.lm_head(x[:, [-1], :])  # [B, 1, vocab_size]\n",
        "            loss = None\n",
        "            print(f\"æ¨ç†æ¨¡å¼ - logits: {logits.shape}\")\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "print(\"å®Œæ•´Transformeræ¨¡å‹å®šä¹‰å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®Œæ•´ç¤ºä¾‹è¿è¡Œ\n",
        "def main_demo():\n",
        "    \"\"\"ä¸»å‡½æ•°æ¼”ç¤º - åŸä»£ç ç¬¬351-375è¡Œè¯¦è§£\"\"\"\n",
        "    print(\"=\"*50)\n",
        "    print(\"   Transformeræ¨¡å‹å®Œæ•´æ¼”ç¤º\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # ç¬¬352è¡Œï¼šé…ç½®æ¨¡å‹å‚æ•°\n",
        "    args = ModelArgs(\n",
        "        n_embd=128,       # åµŒå…¥ç»´åº¦ï¼ˆç®€åŒ–ç‰ˆï¼ŒåŸä¸º100ï¼‰\n",
        "        n_heads=8,        # æ³¨æ„åŠ›å¤´æ•°ï¼ˆåŸä¸º10ï¼‰\n",
        "        dim=128,          # æ¨¡å‹ç»´åº¦ï¼ˆåŸä¸º100ï¼‰\n",
        "        dropout=0.1,      # Dropoutç‡\n",
        "        max_seq_len=512,  # æœ€å¤§åºåˆ—é•¿åº¦\n",
        "        vocab_size=21128, # è¯æ±‡è¡¨å¤§å°ï¼ˆBERTä¸­æ–‡ï¼‰\n",
        "        block_size=512,   # å—å¤§å°\n",
        "        n_layer=3         # å±‚æ•°ï¼ˆç®€åŒ–ç‰ˆï¼ŒåŸä¸º2ï¼‰\n",
        "    )\n",
        "    print(f\"æ¨¡å‹é…ç½®: {args}\")\n",
        "    \n",
        "    # ç¬¬353è¡Œï¼šæµ‹è¯•æ–‡æœ¬\n",
        "    text = \"æˆ‘å–œæ¬¢å¿«ä¹åœ°å­¦ä¹ å¤§æ¨¡å‹\"\n",
        "    print(f\"\\\\nè¾“å…¥æ–‡æœ¬: '{text}'\")\n",
        "    \n",
        "    # ç¬¬354-361è¡Œï¼šåˆ†è¯å¤„ç†\n",
        "    try:\n",
        "        # å°è¯•åŠ è½½BERTåˆ†è¯å™¨\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
        "        inputs_token = tokenizer(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            max_length=args.max_seq_len,\n",
        "            truncation=True,\n",
        "            padding='max_length'\n",
        "        )\n",
        "        inputs_id = inputs_token['input_ids']\n",
        "        args.vocab_size = tokenizer.vocab_size\n",
        "        \n",
        "        print(f\"åˆ†è¯æˆåŠŸ!\")\n",
        "        print(f\"  Token IDs (å‰20ä¸ª): {inputs_id[0][:20].tolist()}\")\n",
        "        print(f\"  å®é™…åºåˆ—é•¿åº¦: {(inputs_id[0] != 0).sum().item()}\")\n",
        "        print(f\"  è¯æ±‡è¡¨å¤§å°: {args.vocab_size}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"æ— æ³•åŠ è½½BERTåˆ†è¯å™¨: {e}\")\n",
        "        print(\"ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®...\")\n",
        "        # ä½¿ç”¨éšæœºæ•°æ®\n",
        "        inputs_id = torch.randint(1, 1000, (1, 20))  # é¿å…ä½¿ç”¨0ï¼ˆpaddingï¼‰\n",
        "        tokenizer = None\n",
        "        print(f\"  æ¨¡æ‹ŸToken IDs: {inputs_id[0].tolist()}\")\n",
        "    \n",
        "    # ç¬¬363è¡Œï¼šåˆ›å»ºTransformeræ¨¡å‹\n",
        "    print(f\"\\\\nåˆ›å»ºTransformeræ¨¡å‹...\")\n",
        "    transformer = Transformer(args)\n",
        "    \n",
        "    # ç¬¬364-365è¡Œï¼šæ¨ç†æ¨¡å¼\n",
        "    print(f\"\\\\n\" + \"=\"*30 + \" æ¨ç†æ¨¡å¼ \" + \"=\"*30)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = transformer.forward(inputs_id)\n",
        "    \n",
        "    # ç¬¬367-375è¡Œï¼šç»“æœåˆ†æ\n",
        "    print(f\"\\\\næ¨ç†ç»“æœåˆ†æ:\")\n",
        "    print(f\"  Logitså½¢çŠ¶: {logits.shape}\")\n",
        "    print(f\"  Logitsæ•°å€¼èŒƒå›´: [{logits.min():.3f}, {logits.max():.3f}]\")\n",
        "    \n",
        "    # è·å–æ¦‚ç‡æœ€é«˜çš„token\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    predicted_token_id = predicted_ids[0, 0].item()\n",
        "    print(f\"  é¢„æµ‹çš„ä¸‹ä¸€ä¸ªToken ID: {predicted_token_id}\")\n",
        "    \n",
        "    if tokenizer is not None:\n",
        "        try:\n",
        "            predicted_token = tokenizer.decode([predicted_token_id])\n",
        "            print(f\"  é¢„æµ‹çš„ä¸‹ä¸€ä¸ªToken: '{predicted_token}'\")\n",
        "        except:\n",
        "            print(f\"  æ— æ³•è§£ç Token ID: {predicted_token_id}\")\n",
        "    \n",
        "    # è®­ç»ƒæ¨¡å¼æ¼”ç¤º\n",
        "    print(f\"\\\\n\" + \"=\"*30 + \" è®­ç»ƒæ¨¡å¼ \" + \"=\"*30)\n",
        "    # åˆ›å»ºç›®æ ‡åºåˆ—ï¼ˆå‘å³åç§»ä¸€ä½ï¼‰\n",
        "    targets = torch.roll(inputs_id, shifts=-1, dims=1)\n",
        "    targets[:, -1] = -1  # æœ€åä¸€ä½è®¾ä¸ºignore_index\n",
        "    \n",
        "    print(f\"åˆ›å»ºè®­ç»ƒç›®æ ‡:\")\n",
        "    print(f\"  è¾“å…¥åºåˆ—: {inputs_id[0][:10].tolist()}...\")  \n",
        "    print(f\"  ç›®æ ‡åºåˆ—: {targets[0][:10].tolist()}...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        logits_train, loss_train = transformer.forward(inputs_id, targets)\n",
        "    \n",
        "    print(f\"\\\\nè®­ç»ƒç»“æœ:\")\n",
        "    print(f\"  è®­ç»ƒLogitså½¢çŠ¶: {logits_train.shape}\")\n",
        "    print(f\"  äº¤å‰ç†µæŸå¤±: {loss_train:.4f}\")\n",
        "    print(f\"  å›°æƒ‘åº¦: {torch.exp(loss_train):.2f}\")\n",
        "    \n",
        "    print(f\"\\\\n\" + \"=\"*50)\n",
        "    print(\"   æ¼”ç¤ºå®Œæˆï¼\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    return transformer, args, tokenizer\n",
        "\n",
        "# è¿è¡Œå®Œæ•´æ¼”ç¤º\n",
        "if __name__ == \"__main__\":\n",
        "    model, config, tokenizer = main_demo()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## æ€»ç»“ä¸å…³é”®å…¬å¼æ±‡æ€»\n",
        "\n",
        "### ğŸ¯ Transformeræ ¸å¿ƒæ€æƒ³\n",
        "Transformeræ˜¯ä¸€ç§åŸºäº**è‡ªæ³¨æ„åŠ›æœºåˆ¶**çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼Œå®Œå…¨æ‘’å¼ƒäº†å¾ªç¯å’Œå·ç§¯ç»“æ„ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ç›´æ¥å»ºæ¨¡åºåˆ—ä¸­ä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚\n",
        "\n",
        "### ğŸ“ å…³é”®æ•°å­¦å…¬å¼\n",
        "\n",
        "#### 1. å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
        "\n",
        "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
        "\n",
        "#### 2. å±‚å½’ä¸€åŒ– (Layer Normalization)\n",
        "$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta$$\n",
        "\n",
        "å…¶ä¸­ï¼š$\\mu = \\frac{1}{d}\\sum_{i=1}^{d} x_i$, $\\sigma^2 = \\frac{1}{d}\\sum_{i=1}^{d} (x_i - \\mu)^2$\n",
        "\n",
        "#### 3. ä½ç½®ç¼–ç  (Positional Encoding)\n",
        "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "\n",
        "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "\n",
        "#### 4. å‰é¦ˆç¥ç»ç½‘ç»œ (Feed Forward Network)\n",
        "$$\\text{FFN}(x) = \\text{dropout}(W_2 \\cdot \\text{ReLU}(W_1 \\cdot x + b_1) + b_2)$$\n",
        "\n",
        "### ğŸ—ï¸ æ¶æ„ç‰¹ç‚¹\n",
        "\n",
        "#### ç¼–ç å™¨ (Encoder)\n",
        "- **åŒå‘æ³¨æ„åŠ›**: å¯ä»¥çœ‹åˆ°æ•´ä¸ªè¾“å…¥åºåˆ—\n",
        "- **è‡ªæ³¨æ„åŠ›**: Q=K=Vï¼Œå­¦ä¹ è¾“å…¥åºåˆ—å†…éƒ¨çš„ä¾èµ–å…³ç³»\n",
        "- **æ®‹å·®è¿æ¥**: $\\text{output} = \\text{LayerNorm}(x + \\text{SubLayer}(x))$\n",
        "\n",
        "#### è§£ç å™¨ (Decoder)  \n",
        "- **æ©ç æ³¨æ„åŠ›**: åªèƒ½çœ‹åˆ°å½“å‰ä½ç½®ä¹‹å‰çš„ä¿¡æ¯\n",
        "- **äº¤å‰æ³¨æ„åŠ›**: Qæ¥è‡ªè§£ç å™¨ï¼ŒKå’ŒVæ¥è‡ªç¼–ç å™¨\n",
        "- **ä¸‰ä¸ªå­å±‚**: æ©ç è‡ªæ³¨æ„åŠ› â†’ äº¤å‰æ³¨æ„åŠ› â†’ å‰é¦ˆç½‘ç»œ\n",
        "\n",
        "### ğŸ’¡ å…³é”®åˆ›æ–°ç‚¹\n",
        "\n",
        "1. **å¹¶è¡ŒåŒ–**: ä¸åŒäºRNNçš„é¡ºåºè®¡ç®—ï¼ŒTransformerå¯ä»¥å¹¶è¡Œå¤„ç†æ‰€æœ‰ä½ç½®\n",
        "2. **é•¿è·ç¦»ä¾èµ–**: ç›´æ¥è®¡ç®—ä»»æ„ä¸¤ä¸ªä½ç½®çš„å…³æ³¨åº¦ï¼Œæœ‰æ•ˆå¤„ç†é•¿åºåˆ—\n",
        "3. **å¤šå¤´æœºåˆ¶**: å¹¶è¡Œè®¡ç®—å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ•è·ä¸åŒç±»å‹çš„ä¾èµ–å…³ç³»\n",
        "4. **æ®‹å·®è¿æ¥**: ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½¿å¾—å¯ä»¥å †å æ›´æ·±çš„ç½‘ç»œ\n",
        "5. **ä½ç½®ç¼–ç **: ä¸ºæ¨¡å‹æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼Œå¼¥è¡¥æ³¨æ„åŠ›æœºåˆ¶ç¼ºä¹ä½ç½®æ„ŸçŸ¥çš„ä¸è¶³\n",
        "\n",
        "### ğŸ”§ å®ç°ç»†èŠ‚\n",
        "\n",
        "- **ç¼©æ”¾å› å­**: $\\frac{1}{\\sqrt{d_k}}$ é˜²æ­¢softmaxè¿›å…¥é¥±å’ŒåŒºåŸŸ\n",
        "- **æ©ç æœºåˆ¶**: ä¸Šä¸‰è§’çŸ©é˜µæ©ç é˜²æ­¢è§£ç å™¨çœ‹åˆ°æœªæ¥ä¿¡æ¯  \n",
        "- **æƒé‡å…±äº«**: è¾“å…¥åµŒå…¥å’Œè¾“å‡ºæŠ•å½±å¯ä»¥å…±äº«æƒé‡\n",
        "- **å­¦ä¹ ç‡è°ƒåº¦**: ä½¿ç”¨warmupç­–ç•¥ï¼Œå…ˆå¢åå‡\n",
        "\n",
        "### ğŸ“Š å¤æ‚åº¦åˆ†æ\n",
        "\n",
        "| å±‚ç±»å‹ | åºåˆ—å¤æ‚åº¦ | å¹¶è¡Œåº¦ | æœ€çŸ­è·¯å¾„ |\n",
        "|--------|------------|--------|----------|\n",
        "| è‡ªæ³¨æ„åŠ› | $O(n^2 \\cdot d)$ | $O(1)$ | $O(1)$ |\n",
        "| å¾ªç¯ | $O(n \\cdot d^2)$ | $O(n)$ | $O(n)$ |\n",
        "| å·ç§¯ | $O(k \\cdot n \\cdot d^2)$ | $O(1)$ | $O(\\log_k(n))$ |\n",
        "\n",
        "å…¶ä¸­næ˜¯åºåˆ—é•¿åº¦ï¼Œdæ˜¯è¡¨ç¤ºç»´åº¦ï¼Œkæ˜¯å·ç§¯æ ¸å¤§å°ã€‚\n",
        "\n",
        "### ğŸŒŸ å½±å“åŠ›\n",
        "\n",
        "Transformeræ¶æ„å‚¬ç”Ÿäº†ï¼š\n",
        "- **BERT**: åŒå‘ç¼–ç å™¨ï¼Œæ“…é•¿ç†è§£ä»»åŠ¡\n",
        "- **GPT**: å•å‘è§£ç å™¨ï¼Œæ“…é•¿ç”Ÿæˆä»»åŠ¡  \n",
        "- **T5**: ç¼–ç å™¨-è§£ç å™¨ï¼Œç»Ÿä¸€çš„æ–‡æœ¬åˆ°æ–‡æœ¬æ¡†æ¶\n",
        "- **å¤§è¯­è¨€æ¨¡å‹**: ChatGPTã€GPT-4ç­‰é©å‘½æ€§åº”ç”¨\n",
        "\n",
        "è¿™ä¸ªå®ç°å±•ç¤ºäº†å®Œæ•´çš„ç¼–ç å™¨-è§£ç å™¨Transformeræ¶æ„ï¼Œæ˜¯ç†è§£ç°ä»£NLPæ¨¡å‹çš„é‡è¦åŸºç¡€ï¼\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
