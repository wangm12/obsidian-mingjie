{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Transformer 架构详细解释\n",
        "\n",
        "本notebook将详细解释Transformer模型的每个组件，包括数学公式、代码实现和实际例子。\n",
        "\n",
        "## 目录\n",
        "1. [配置类 ModelArgs](#1-配置类-modelargs)\n",
        "2. [多头注意力机制 MultiHeadAttention](#2-多头注意力机制-multiheadattention)\n",
        "3. [层归一化 LayerNorm](#3-层归一化-layernorm)\n",
        "4. [前馈神经网络 MLP](#4-前馈神经网络-mlp)\n",
        "5. [编码器组件](#5-编码器组件)\n",
        "6. [解码器组件](#6-解码器组件)\n",
        "7. [位置编码 PositionalEncoding](#7-位置编码-positionalencoding)\n",
        "8. [完整的Transformer模型](#8-完整的transformer模型)\n",
        "9. [示例运行](#9-示例运行)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 导入必要的库\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mmath\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "# 导入必要的库\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "from dataclasses import dataclass\n",
        "from transformers import BertTokenizer\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"所有库导入成功！\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. 配置类 ModelArgs\n",
        "\n",
        "首先定义模型的超参数配置。使用`@dataclass`装饰器可以自动生成初始化方法。\n",
        "\n",
        "这个配置类包含了Transformer模型所需的所有超参数：\n",
        "- `n_embd`: 嵌入维度，决定了token向量的大小\n",
        "- `n_heads`: 多头注意力的头数，通常为8或16\n",
        "- `dim`: 模型的隐藏维度\n",
        "- `dropout`: 防止过拟合的dropout率\n",
        "- `max_seq_len`: 能处理的最大序列长度\n",
        "- `vocab_size`: 词汇表大小\n",
        "- `block_size`: 处理的块大小\n",
        "- `n_layer`: Transformer层数\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "    \"\"\"模型配置参数\"\"\"\n",
        "    n_embd: int       # 嵌入维度 (Embedding dimension)\n",
        "    n_heads: int      # 多头注意力的头数 (Number of attention heads)  \n",
        "    dim: int          # 模型维度 (Model dimension)\n",
        "    dropout: float    # Dropout率 (Dropout rate)\n",
        "    max_seq_len: int  # 最大序列长度 (Maximum sequence length)\n",
        "    vocab_size: int   # 词汇表大小 (Vocabulary size)\n",
        "    block_size: int   # 块大小 (Block size)\n",
        "    n_layer: int      # 层数 (Number of layers)\n",
        "\n",
        "# 示例配置\n",
        "args_example = ModelArgs(\n",
        "    n_embd=512,      # 嵌入维度\n",
        "    n_heads=8,       # 8个注意力头\n",
        "    dim=512,         # 模型维度  \n",
        "    dropout=0.1,     # 10% dropout\n",
        "    max_seq_len=1024, # 最大1024个token\n",
        "    vocab_size=30000, # 3万词汇量\n",
        "    block_size=1024,  # 块大小\n",
        "    n_layer=6        # 6层\n",
        ")\n",
        "\n",
        "print(f\"配置示例: {args_example}\")\n",
        "print(f\"每个注意力头的维度: {args_example.dim // args_example.n_heads}\")\n",
        "print(f\"模型总参数量估计: ~{(args_example.vocab_size * args_example.n_embd + args_example.n_layer * 4 * args_example.dim**2) / 1e6:.1f}M\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. 多头注意力机制 MultiHeadAttention\n",
        "\n",
        "多头注意力是Transformer的核心组件。它允许模型同时关注序列中不同位置的信息。\n",
        "\n",
        "### 数学公式：\n",
        "\n",
        "**单头注意力公式：**\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "**多头注意力公式：**\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
        "\n",
        "其中：\n",
        "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
        "\n",
        "### 关键概念：\n",
        "- **Q (Query)**: 查询矩阵，决定\"我要找什么\"\n",
        "- **K (Key)**: 键矩阵，决定\"我有什么可以被找到\"\n",
        "- **V (Value)**: 值矩阵，决定\"找到后返回什么信息\"\n",
        "- **注意力分数**: $\\frac{QK^T}{\\sqrt{d_k}}$ 计算查询和键的相似度\n",
        "- **缩放因子**: $\\sqrt{d_k}$ 防止softmax进入饱和区域\n",
        "- **多头**: 并行计算多个不同的注意力模式\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"多头注意力机制 - 逐行详细解释\"\"\"\n",
        "    \n",
        "    def __init__(self, args: ModelArgs, is_causal=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        # 第17行：检查维度能否被头数整除\n",
        "        assert args.dim % args.n_heads == 0, f\"dim({args.dim})必须能被n_heads({args.n_heads})整除\"\n",
        "        \n",
        "        # 第19-20行：模型并行相关（这里简化为1）\n",
        "        model_parallel_size = 1\n",
        "        self.n_local_heads = args.n_heads // model_parallel_size\n",
        "        \n",
        "        # 第22行：计算每个头的维度\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "        \n",
        "        print(f\"多头注意力初始化:\")\n",
        "        print(f\"  总维度: {args.dim}\")\n",
        "        print(f\"  头数: {self.n_local_heads}\")  \n",
        "        print(f\"  每个头维度: {self.head_dim}\")\n",
        "        \n",
        "        # 第29-31行：Q、K、V的权重矩阵\n",
        "        # 注意：这里用一个大矩阵代替多个小矩阵，提高效率\n",
        "        self.wq = nn.Linear(args.n_embd, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wk = nn.Linear(args.n_embd, args.n_heads * self.head_dim, bias=False) \n",
        "        self.wv = nn.Linear(args.n_embd, args.n_heads * self.head_dim, bias=False)\n",
        "        \n",
        "        # 第33行：输出投影矩阵W^O\n",
        "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
        "        \n",
        "        # 第35-37行：Dropout层\n",
        "        self.attn_dropout = nn.Dropout(args.dropout)\n",
        "        self.resid_dropout = nn.Dropout(args.dropout)\n",
        "        self.is_causal = is_causal\n",
        "        \n",
        "        # 第39-46行：创建因果掩码（用于解码器）\n",
        "        if is_causal:\n",
        "            # 创建上三角掩码，防止看到\"未来\"信息\n",
        "            mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))\n",
        "            mask = torch.triu(mask, diagonal=1)  # 上三角矩阵\n",
        "            self.register_buffer(\"mask\", mask)\n",
        "            print(f\"  创建因果掩码: {mask.shape}\")\n",
        "    \n",
        "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
        "        \"\"\"前向传播 - 逐步解释注意力计算过程\"\"\"\n",
        "        \n",
        "        # 第51行：获取批次大小和序列长度\n",
        "        bsz, seqlen, _ = q.shape\n",
        "        print(f\"\\n=== 多头注意力前向传播 ===\")\n",
        "        print(f\"输入形状: batch={bsz}, seq_len={seqlen}, dim={q.shape[2]}\")\n",
        "        \n",
        "        # 第54行：通过线性层计算Q、K、V\n",
        "        # [B, T, n_embd] -> [B, T, n_heads * head_dim]\n",
        "        xq, xk, xv = self.wq(q), self.wk(k), self.wv(v)\n",
        "        print(f\"QKV线性变换后: {xq.shape}\")\n",
        "        \n",
        "        # 第59-62行：重塑为多头格式\n",
        "        # [B, T, n_heads * head_dim] -> [B, T, n_heads, head_dim]  \n",
        "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
        "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
        "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
        "        \n",
        "        # 第63-65行：转置到注意力计算格式\n",
        "        # [B, T, n_heads, head_dim] -> [B, n_heads, T, head_dim]\n",
        "        xq = xq.transpose(1, 2)\n",
        "        xk = xk.transpose(1, 2) \n",
        "        xv = xv.transpose(1, 2)\n",
        "        print(f\"多头重塑后: {xq.shape}\")\n",
        "        \n",
        "        # 第68行：计算注意力分数 QK^T/√d_k\n",
        "        # [B, n_heads, T, head_dim] × [B, n_heads, head_dim, T] -> [B, n_heads, T, T]\n",
        "        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "        print(f\"注意力分数: {scores.shape}, 缩放因子: {math.sqrt(self.head_dim):.2f}\")\n",
        "        \n",
        "        # 第70-73行：应用因果掩码（如果需要）\n",
        "        if self.is_causal:\n",
        "            assert hasattr(self, 'mask')\n",
        "            scores = scores + self.mask[:, :, :seqlen, :seqlen]\n",
        "            print(f\"应用掩码后分数范围: [{scores.min():.2f}, {scores.max():.2f}]\")\n",
        "        \n",
        "        # 第75-77行：Softmax + Dropout\n",
        "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "        scores = self.attn_dropout(scores)\n",
        "        \n",
        "        # 第79行：注意力加权求和\n",
        "        # [B, n_heads, T, T] × [B, n_heads, T, head_dim] -> [B, n_heads, T, head_dim]\n",
        "        output = torch.matmul(scores, xv)\n",
        "        print(f\"注意力输出: {output.shape}\")\n",
        "        \n",
        "        # 第84行：合并多头\n",
        "        # [B, n_heads, T, head_dim] -> [B, T, n_heads, head_dim] -> [B, T, n_heads*head_dim]\n",
        "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
        "        \n",
        "        # 第87-88行：最终投影和dropout\n",
        "        output = self.wo(output)\n",
        "        output = self.resid_dropout(output)\n",
        "        \n",
        "        print(f\"最终输出: {output.shape}\")\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 测试多头注意力机制\n",
        "print(\"=== 测试多头注意力机制 ===\")\n",
        "\n",
        "# 创建小型配置用于测试\n",
        "args_small = ModelArgs(\n",
        "    n_embd=64,      # 嵌入维度\n",
        "    n_heads=4,      # 4个注意力头\n",
        "    dim=64,         # 模型维度\n",
        "    dropout=0.1,    # dropout率\n",
        "    max_seq_len=128, # 最大序列长度\n",
        "    vocab_size=1000, # 词汇表大小\n",
        "    block_size=128,  # 块大小\n",
        "    n_layer=2       # 层数\n",
        ")\n",
        "\n",
        "# 创建多头注意力层（编码器版本，无掩码）\n",
        "print(\"\\n1. 创建编码器版本的多头注意力:\")\n",
        "mha_encoder = MultiHeadAttention(args_small, is_causal=False)\n",
        "\n",
        "# 创建测试输入\n",
        "batch_size, seq_len = 2, 8\n",
        "x = torch.randn(batch_size, seq_len, args_small.n_embd)\n",
        "print(f\"\\n2. 创建测试输入: {x.shape}\")\n",
        "\n",
        "# 前向传播（自注意力：Q=K=V）\n",
        "print(f\"\\n3. 执行自注意力计算...\")\n",
        "with torch.no_grad():\n",
        "    output_encoder = mha_encoder(x, x, x)\n",
        "\n",
        "print(f\"\\n编码器自注意力完成！\")\n",
        "print(f\"输入: {x.shape} -> 输出: {output_encoder.shape}\")\n",
        "\n",
        "# 创建解码器版本（带掩码）\n",
        "print(f\"\\n4. 创建解码器版本的多头注意力:\")\n",
        "mha_decoder = MultiHeadAttention(args_small, is_causal=True)\n",
        "\n",
        "# 测试解码器版本\n",
        "print(f\"\\n5. 执行掩码自注意力计算...\")\n",
        "with torch.no_grad():\n",
        "    output_decoder = mha_decoder(x, x, x)\n",
        "\n",
        "print(f\"\\n解码器掩码自注意力完成！\")\n",
        "print(f\"输入: {x.shape} -> 输出: {output_decoder.shape}\")\n",
        "\n",
        "print(f\"\\n=== 多头注意力测试完成 ===\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. 完整Transformer代码解释\n",
        "\n",
        "现在让我们加载并解释完整的Transformer代码。我将逐个组件进行详细说明：\n",
        "\n",
        "### 层归一化 LayerNorm (第95-110行)\n",
        "公式：$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta$$\n",
        "\n",
        "- 对每个样本的特征维度进行标准化\n",
        "- 与BatchNorm不同，LayerNorm在特征维度上标准化，而不是批次维度\n",
        "\n",
        "### 前馈神经网络 MLP (第112-128行)  \n",
        "公式：$$\\text{FFN}(x) = \\text{dropout}(W_2 \\cdot \\text{ReLU}(W_1 \\cdot x))$$\n",
        "\n",
        "- 两层全连接网络\n",
        "- 通常隐藏层维度是输入维度的4倍\n",
        "- 使用ReLU激活函数\n",
        "\n",
        "### 位置编码 PositionalEncoding (第246-278行)\n",
        "公式：\n",
        "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "\n",
        "- 为序列添加位置信息\n",
        "- 使用正弦和余弦函数，允许模型学习相对位置关系\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 完整的Transformer实现 - 从原始代码逐行解释\n",
        "\n",
        "# 先定义剩余的核心组件\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    '''Layer Norm 层 - 第95-110行详解'''\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        # 线性变换参数 - 可学习的缩放和偏移\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))    # γ (gamma) 缩放参数\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))   # β (beta) 偏移参数\n",
        "        self.eps = eps  # 防止除零的小常数\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # 在最后一个维度计算均值和标准差\n",
        "        mean = x.mean(-1, keepdim=True)  # 均值：[batch, seq_len, 1]\n",
        "        std = x.std(-1, keepdim=True)    # 标准差：[batch, seq_len, 1]\n",
        "        # LayerNorm公式：γ * (x-μ)/(σ+ε) + β\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    '''前馈神经网络 - 第112-128行详解'''\n",
        "    def __init__(self, dim: int, hidden_dim: int, dropout: float):\n",
        "        super().__init__()\n",
        "        # 第一层：维度扩展 (通常 hidden_dim = 4 * dim)\n",
        "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        # 第二层：恢复原始维度\n",
        "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
        "        # Dropout防止过拟合\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # FFN公式：dropout(W2 * ReLU(W1 * x))\n",
        "        return self.dropout(self.w2(F.relu(self.w1(x))))\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    '''位置编码 - 第246-278行详解'''\n",
        "    def __init__(self, args):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=args.dropout)\n",
        "\n",
        "        # 创建位置编码矩阵 [block_size, n_embd]\n",
        "        pe = torch.zeros(args.block_size, args.n_embd)\n",
        "        position = torch.arange(0, args.block_size).unsqueeze(1)  # [block_size, 1]\n",
        "        \n",
        "        # 计算频率项：10000^(-2i/d_model)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, args.n_embd, 2) * -(math.log(10000.0) / args.n_embd)\n",
        "        )\n",
        "        \n",
        "        # 位置编码公式：\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数位置用sin\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数位置用cos\n",
        "        \n",
        "        pe = pe.unsqueeze(0)  # 添加batch维度：[1, block_size, n_embd]\n",
        "        self.register_buffer(\"pe\", pe)  # 注册为缓冲区（不更新梯度）\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 将位置编码加到token嵌入上\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "print(\"核心组件定义完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 编码器和解码器实现\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"编码器层 - 第131-147行详解\"\"\"\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        # Pre-LayerNorm架构：先归一化再计算\n",
        "        self.attention_norm = LayerNorm(args.n_embd)\n",
        "        # 编码器使用非因果（双向）注意力\n",
        "        self.attention = MultiHeadAttention(args, is_causal=False)\n",
        "        self.fnn_norm = LayerNorm(args.n_embd)\n",
        "        # FFN隐藏层维度通常是输入维度的4倍\n",
        "        self.feed_forward = MLP(args.dim, args.dim * 4, args.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 第一个子层：多头自注意力 + 残差连接\n",
        "        # Pre-Norm：先LayerNorm再注意力\n",
        "        norm_x = self.attention_norm(x)\n",
        "        h = x + self.attention.forward(norm_x, norm_x, norm_x)  # 残差连接\n",
        "        \n",
        "        # 第二个子层：前馈网络 + 残差连接  \n",
        "        norm_h = self.fnn_norm(h)\n",
        "        out = h + self.feed_forward.forward(norm_h)  # 残差连接\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    '''编码器 - 第149-160行详解'''\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "        # 堆叠N个编码器层\n",
        "        self.layers = nn.ModuleList([EncoderLayer(args) for _ in range(args.n_layer)])\n",
        "        # 最终的层归一化\n",
        "        self.norm = LayerNorm(args.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 逐层通过编码器\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.norm(x)  # 最终归一化\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    '''解码器层 - 第162-186行详解'''\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        # 第一个注意力：掩码自注意力\n",
        "        self.attention_norm_1 = LayerNorm(args.n_embd)\n",
        "        self.mask_attention = MultiHeadAttention(args, is_causal=True)\n",
        "        \n",
        "        # 第二个注意力：编码器-解码器交叉注意力\n",
        "        self.attention_norm_2 = LayerNorm(args.n_embd)\n",
        "        self.attention = MultiHeadAttention(args, is_causal=False)\n",
        "        \n",
        "        # 前馈网络\n",
        "        self.ffn_norm = LayerNorm(args.n_embd)\n",
        "        self.feed_forward = MLP(args.dim, args.dim * 4, args.dropout)\n",
        "\n",
        "    def forward(self, x, enc_out):\n",
        "        # 子层1：掩码自注意力 + 残差连接\n",
        "        norm_x1 = self.attention_norm_1(x)\n",
        "        x = x + self.mask_attention.forward(norm_x1, norm_x1, norm_x1)\n",
        "        \n",
        "        # 子层2：编码器-解码器注意力 + 残差连接\n",
        "        # Q来自解码器，K和V来自编码器\n",
        "        norm_x2 = self.attention_norm_2(x)\n",
        "        h = x + self.attention.forward(norm_x2, enc_out, enc_out)\n",
        "        \n",
        "        # 子层3：前馈网络 + 残差连接\n",
        "        norm_h = self.ffn_norm(h)\n",
        "        out = h + self.feed_forward.forward(norm_h)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    '''解码器 - 第188-199行详解'''\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "        # 堆叠N个解码器层\n",
        "        self.layers = nn.ModuleList([DecoderLayer(args) for _ in range(args.n_layer)])\n",
        "        # 最终的层归一化\n",
        "        self.norm = LayerNorm(args.n_embd)\n",
        "\n",
        "    def forward(self, x, enc_out):\n",
        "        # 逐层通过解码器，每层都需要编码器输出\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out)\n",
        "        return self.norm(x)\n",
        "\n",
        "print(\"编码器和解码器定义完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 完整的Transformer模型\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    '''完整的Transformer模型 - 第280-349行详解'''\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        # 验证必要参数\n",
        "        assert args.vocab_size is not None, \"必须指定vocab_size\"\n",
        "        assert args.block_size is not None, \"必须指定block_size\"\n",
        "        \n",
        "        self.args = args\n",
        "        \n",
        "        # 主要组件 - 使用ModuleDict管理\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(args.vocab_size, args.n_embd),  # Token嵌入\n",
        "            wpe=PositionalEncoding(args),                     # 位置编码\n",
        "            drop=nn.Dropout(args.dropout),                    # Dropout\n",
        "            encoder=Encoder(args),                            # 编码器\n",
        "            decoder=Decoder(args),                            # 解码器\n",
        "        ))\n",
        "        \n",
        "        # 语言建模头：将隐藏状态映射到词汇表\n",
        "        self.lm_head = nn.Linear(args.n_embd, args.vocab_size, bias=False)\n",
        "        \n",
        "        # 权重初始化\n",
        "        self.apply(self._init_weights)\n",
        "        \n",
        "        # 打印参数统计\n",
        "        print(f\"模型参数数量: {self.get_num_params()/1e6:.2f}M\")\n",
        "\n",
        "    def get_num_params(self, non_embedding=False):\n",
        "        \"\"\"统计参数数量\"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wte.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"权重初始化 - 使用正态分布\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"前向传播 - 第326-349行详解\n",
        "        \n",
        "        Args:\n",
        "            idx: 输入token索引 [batch_size, seq_len]\n",
        "            targets: 目标序列（训练时使用） [batch_size, seq_len]\n",
        "        \n",
        "        Returns:\n",
        "            logits: 输出概率分布\n",
        "            loss: 损失值（如果提供targets）\n",
        "        \"\"\"\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        \n",
        "        # 检查序列长度限制\n",
        "        assert t <= self.args.block_size, f\"序列长度{t}超过最大长度{self.args.block_size}\"\n",
        "\n",
        "        print(f\"\\\\n=== Transformer前向传播 ===\")\n",
        "        print(f\"输入idx形状: {idx.shape}\")\n",
        "\n",
        "        # 1. Token嵌入：将token索引转换为向量\n",
        "        tok_emb = self.transformer.wte(idx)  # [B, T, n_embd]\n",
        "        print(f\"Token嵌入后: {tok_emb.shape}\")\n",
        "\n",
        "        # 2. 位置编码：添加位置信息\n",
        "        pos_emb = self.transformer.wpe(tok_emb)\n",
        "        print(f\"位置编码后: {pos_emb.shape}\")\n",
        "\n",
        "        # 3. Dropout\n",
        "        x = self.transformer.drop(pos_emb)\n",
        "        print(f\"Dropout后: {x.shape}\")\n",
        "\n",
        "        # 4. 编码器：理解输入序列\n",
        "        enc_out = self.transformer.encoder(x)\n",
        "        print(f\"编码器输出: {enc_out.shape}\")\n",
        "\n",
        "        # 5. 解码器：生成输出序列\n",
        "        x = self.transformer.decoder(x, enc_out)\n",
        "        print(f\"解码器输出: {x.shape}\")\n",
        "\n",
        "        if targets is not None:\n",
        "            # 训练模式：计算所有位置的损失\n",
        "            logits = self.lm_head(x)  # [B, T, vocab_size]\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)), \n",
        "                targets.view(-1), \n",
        "                ignore_index=-1\n",
        "            )\n",
        "            print(f\"训练模式 - logits: {logits.shape}, loss: {loss.item():.4f}\")\n",
        "        else:\n",
        "            # 推理模式：只计算最后一个位置\n",
        "            logits = self.lm_head(x[:, [-1], :])  # [B, 1, vocab_size]\n",
        "            loss = None\n",
        "            print(f\"推理模式 - logits: {logits.shape}\")\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "print(\"完整Transformer模型定义完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 完整示例运行\n",
        "def main_demo():\n",
        "    \"\"\"主函数演示 - 原代码第351-375行详解\"\"\"\n",
        "    print(\"=\"*50)\n",
        "    print(\"   Transformer模型完整演示\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # 第352行：配置模型参数\n",
        "    args = ModelArgs(\n",
        "        n_embd=128,       # 嵌入维度（简化版，原为100）\n",
        "        n_heads=8,        # 注意力头数（原为10）\n",
        "        dim=128,          # 模型维度（原为100）\n",
        "        dropout=0.1,      # Dropout率\n",
        "        max_seq_len=512,  # 最大序列长度\n",
        "        vocab_size=21128, # 词汇表大小（BERT中文）\n",
        "        block_size=512,   # 块大小\n",
        "        n_layer=3         # 层数（简化版，原为2）\n",
        "    )\n",
        "    print(f\"模型配置: {args}\")\n",
        "    \n",
        "    # 第353行：测试文本\n",
        "    text = \"我喜欢快乐地学习大模型\"\n",
        "    print(f\"\\\\n输入文本: '{text}'\")\n",
        "    \n",
        "    # 第354-361行：分词处理\n",
        "    try:\n",
        "        # 尝试加载BERT分词器\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
        "        inputs_token = tokenizer(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            max_length=args.max_seq_len,\n",
        "            truncation=True,\n",
        "            padding='max_length'\n",
        "        )\n",
        "        inputs_id = inputs_token['input_ids']\n",
        "        args.vocab_size = tokenizer.vocab_size\n",
        "        \n",
        "        print(f\"分词成功!\")\n",
        "        print(f\"  Token IDs (前20个): {inputs_id[0][:20].tolist()}\")\n",
        "        print(f\"  实际序列长度: {(inputs_id[0] != 0).sum().item()}\")\n",
        "        print(f\"  词汇表大小: {args.vocab_size}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"无法加载BERT分词器: {e}\")\n",
        "        print(\"使用模拟数据...\")\n",
        "        # 使用随机数据\n",
        "        inputs_id = torch.randint(1, 1000, (1, 20))  # 避免使用0（padding）\n",
        "        tokenizer = None\n",
        "        print(f\"  模拟Token IDs: {inputs_id[0].tolist()}\")\n",
        "    \n",
        "    # 第363行：创建Transformer模型\n",
        "    print(f\"\\\\n创建Transformer模型...\")\n",
        "    transformer = Transformer(args)\n",
        "    \n",
        "    # 第364-365行：推理模式\n",
        "    print(f\"\\\\n\" + \"=\"*30 + \" 推理模式 \" + \"=\"*30)\n",
        "    with torch.no_grad():\n",
        "        logits, loss = transformer.forward(inputs_id)\n",
        "    \n",
        "    # 第367-375行：结果分析\n",
        "    print(f\"\\\\n推理结果分析:\")\n",
        "    print(f\"  Logits形状: {logits.shape}\")\n",
        "    print(f\"  Logits数值范围: [{logits.min():.3f}, {logits.max():.3f}]\")\n",
        "    \n",
        "    # 获取概率最高的token\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    predicted_token_id = predicted_ids[0, 0].item()\n",
        "    print(f\"  预测的下一个Token ID: {predicted_token_id}\")\n",
        "    \n",
        "    if tokenizer is not None:\n",
        "        try:\n",
        "            predicted_token = tokenizer.decode([predicted_token_id])\n",
        "            print(f\"  预测的下一个Token: '{predicted_token}'\")\n",
        "        except:\n",
        "            print(f\"  无法解码Token ID: {predicted_token_id}\")\n",
        "    \n",
        "    # 训练模式演示\n",
        "    print(f\"\\\\n\" + \"=\"*30 + \" 训练模式 \" + \"=\"*30)\n",
        "    # 创建目标序列（向右偏移一位）\n",
        "    targets = torch.roll(inputs_id, shifts=-1, dims=1)\n",
        "    targets[:, -1] = -1  # 最后一位设为ignore_index\n",
        "    \n",
        "    print(f\"创建训练目标:\")\n",
        "    print(f\"  输入序列: {inputs_id[0][:10].tolist()}...\")  \n",
        "    print(f\"  目标序列: {targets[0][:10].tolist()}...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        logits_train, loss_train = transformer.forward(inputs_id, targets)\n",
        "    \n",
        "    print(f\"\\\\n训练结果:\")\n",
        "    print(f\"  训练Logits形状: {logits_train.shape}\")\n",
        "    print(f\"  交叉熵损失: {loss_train:.4f}\")\n",
        "    print(f\"  困惑度: {torch.exp(loss_train):.2f}\")\n",
        "    \n",
        "    print(f\"\\\\n\" + \"=\"*50)\n",
        "    print(\"   演示完成！\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    return transformer, args, tokenizer\n",
        "\n",
        "# 运行完整演示\n",
        "if __name__ == \"__main__\":\n",
        "    model, config, tokenizer = main_demo()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 总结与关键公式汇总\n",
        "\n",
        "### 🎯 Transformer核心思想\n",
        "Transformer是一种基于**自注意力机制**的序列到序列模型，完全摒弃了循环和卷积结构，通过注意力机制直接建模序列中任意两个位置之间的依赖关系。\n",
        "\n",
        "### 📐 关键数学公式\n",
        "\n",
        "#### 1. 多头注意力 (Multi-Head Attention)\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
        "\n",
        "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
        "\n",
        "#### 2. 层归一化 (Layer Normalization)\n",
        "$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta$$\n",
        "\n",
        "其中：$\\mu = \\frac{1}{d}\\sum_{i=1}^{d} x_i$, $\\sigma^2 = \\frac{1}{d}\\sum_{i=1}^{d} (x_i - \\mu)^2$\n",
        "\n",
        "#### 3. 位置编码 (Positional Encoding)\n",
        "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "\n",
        "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "\n",
        "#### 4. 前馈神经网络 (Feed Forward Network)\n",
        "$$\\text{FFN}(x) = \\text{dropout}(W_2 \\cdot \\text{ReLU}(W_1 \\cdot x + b_1) + b_2)$$\n",
        "\n",
        "### 🏗️ 架构特点\n",
        "\n",
        "#### 编码器 (Encoder)\n",
        "- **双向注意力**: 可以看到整个输入序列\n",
        "- **自注意力**: Q=K=V，学习输入序列内部的依赖关系\n",
        "- **残差连接**: $\\text{output} = \\text{LayerNorm}(x + \\text{SubLayer}(x))$\n",
        "\n",
        "#### 解码器 (Decoder)  \n",
        "- **掩码注意力**: 只能看到当前位置之前的信息\n",
        "- **交叉注意力**: Q来自解码器，K和V来自编码器\n",
        "- **三个子层**: 掩码自注意力 → 交叉注意力 → 前馈网络\n",
        "\n",
        "### 💡 关键创新点\n",
        "\n",
        "1. **并行化**: 不同于RNN的顺序计算，Transformer可以并行处理所有位置\n",
        "2. **长距离依赖**: 直接计算任意两个位置的关注度，有效处理长序列\n",
        "3. **多头机制**: 并行计算多个注意力头，捕获不同类型的依赖关系\n",
        "4. **残差连接**: 缓解梯度消失问题，使得可以堆叠更深的网络\n",
        "5. **位置编码**: 为模型注入位置信息，弥补注意力机制缺乏位置感知的不足\n",
        "\n",
        "### 🔧 实现细节\n",
        "\n",
        "- **缩放因子**: $\\frac{1}{\\sqrt{d_k}}$ 防止softmax进入饱和区域\n",
        "- **掩码机制**: 上三角矩阵掩码防止解码器看到未来信息  \n",
        "- **权重共享**: 输入嵌入和输出投影可以共享权重\n",
        "- **学习率调度**: 使用warmup策略，先增后减\n",
        "\n",
        "### 📊 复杂度分析\n",
        "\n",
        "| 层类型 | 序列复杂度 | 并行度 | 最短路径 |\n",
        "|--------|------------|--------|----------|\n",
        "| 自注意力 | $O(n^2 \\cdot d)$ | $O(1)$ | $O(1)$ |\n",
        "| 循环 | $O(n \\cdot d^2)$ | $O(n)$ | $O(n)$ |\n",
        "| 卷积 | $O(k \\cdot n \\cdot d^2)$ | $O(1)$ | $O(\\log_k(n))$ |\n",
        "\n",
        "其中n是序列长度，d是表示维度，k是卷积核大小。\n",
        "\n",
        "### 🌟 影响力\n",
        "\n",
        "Transformer架构催生了：\n",
        "- **BERT**: 双向编码器，擅长理解任务\n",
        "- **GPT**: 单向解码器，擅长生成任务  \n",
        "- **T5**: 编码器-解码器，统一的文本到文本框架\n",
        "- **大语言模型**: ChatGPT、GPT-4等革命性应用\n",
        "\n",
        "这个实现展示了完整的编码器-解码器Transformer架构，是理解现代NLP模型的重要基础！\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
