
# Intro
![[3.1.png]]
对 LLM 进行编码的三个主要阶段的心智模型，在通用文本数据集上预训练 LLM，并在标记数据集上对其进行微调。本章重点介绍注意力机制，它是 LLM 架构的一个组成部分。


我们将实现四种不同的注意力机制变体
我们将在本章中编写的不同注意力机制，从简化版本的自我注意力开始，然后添加可训练的权重。因果注意机制为自我注意力添加了一个掩码，允许 LLM 一次生成一个单词。最后，多头注意力将注意力机制组织成多个头，使模型能够并行捕获输入数据的各个方面。
![[3.2.png]]

## 长序列建模的问题
![[3.3.png]]
将文本从一种语言翻译成另一种语言时，例如德语翻译成英语时，不可能只是逐字翻译。相反，翻译过程需要上下文理解和语法对齐。

为了解决我们无法逐字翻译文本的问题，通常使用具有两个子模块的深度神经网络，即所谓的编码器和解码器。编码器的工作是首先读取并处理整个文本，然后解码器生成翻译后的文本。

在 Transformer 出现之前，递归神经网络 （RNN） 是语言翻译中最流行的编码器-解码器架构. RNN 是一种神经网络，其中先前步骤的输出作为输入馈送到当前步骤，使其非常适合文本等顺序数据。