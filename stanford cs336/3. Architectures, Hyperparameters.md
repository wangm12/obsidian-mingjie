# Transformers
## Original Transformer

![[original transformer.png]]

它主要分为三个部分：**编码器 (Encoder)**、**解码器 (Decoder)** 和模型右侧的**回顾部分 (Review)**

### 原始 Transformer 模型架构图

这个架构图是一个**编码器-解码器**结构。左边是编码器，右边是解码器。

#### 1. 输入和嵌入层 (Inputs and Embedding)

- **输入 (Inputs)**: 模型接收输入的词序列。这些可以是句子中的单词。
    
- **输入嵌入 (Input Embedding)**: 输入的每个单词被转换成一个高维的向量，这个向量包含了词的语义信息。
    
- **位置编码 (Positional Encoding)**: 这是 Transformer 的一个关键创新。由于 Transformer 不像 RNN 那样有固定的序列处理顺序，它需要一种方式来知道单词在句子中的位置信息。位置编码就是解决这个问题的。它是一个与输入嵌入相同维度的向量，被加到输入嵌入向量上，从而让模型能够区分不同位置的单词。图中的 "Positional Encoding" 模块就是做这件事的。
  位置编码的具体公式如下，其中 $pos$ 是词在序列中的位置，**$i$ 是词嵌入向量的维度索引**，$d_{model}$ 是模型的维度： $$ PE_{(pos, 2i)} = \sin(pos/10000^{2i/d_{model}}) $$ $$ PE_{(pos, 2i+1)} = \cos(pos/10000^{2i/d_{model}}) $$

##### 什么是位置编码
假设模型的维度 $d_{model}=4$。我们为位置 $pos=0$ 和 $pos=1$ 的词计算位置编码向量。 
*计算位置 $pos=0$ 的位置编码向量 $PE_0$**： $$ PE_0 = [PE_{(0,0)}, PE_{(0,1)}, PE_{(0,2)}, PE_{(0,3)}] $$ $$ PE_{(0, 0)} = \sin(0/10000^{0/4}) = \sin(0) = 0 $$ $$ PE_{(0, 1)} = \cos(0/10000^{0/4}) = \cos(0) = 1 $$ $$ PE_{(0, 2)} = \sin(0/10000^{2/4}) = \sin(0) = 0 $$ $$ PE_{(0, 3)} = \cos(0/10000^{2/4}) = \cos(0) = 1 $$ 因此，$PE_0 = [0, 1, 0, 1]$。 * **计算位置 $pos=1$ 的位置编码向量 $PE_1$**： $$ PE_1 = [PE_{(1,0)}, PE_{(1,1)}, PE_{(1,2)}, PE_{(1,3)}] $$ $$ PE_{(1, 0)} = \sin(1/10000^{0/4}) = \sin(1) \approx 0.841 $$ $$ PE_{(1, 1)} = \cos(1/10000^{0/4}) = \cos(1) \approx 0.540 $$ $$ PE_{(1, 2)} = \sin(1/10000^{2/4}) = \sin(1/100) \approx 0.010 $$ $$ PE_{(1, 3)} = \cos(1/10000^{2/4}) = \cos(1/100) \approx 0.999 $$ 因此，$PE_1 = [\sin(1), \cos(1), \sin(0.01), \cos(0.01)]$。 最终，这些位置编码向量会被加到相应的词嵌入向量上，作为 Transformer 模型的输入。

##### 为什么选择正弦和余弦？

这种基于正弦和余弦函数的位置编码有两个关键优点：

1. **能够表示相对位置：** 任意位置 pos+k 的位置编码可以表示成位置 pos 的位置编码的线性函数。这意味着模型可以很容易地学习到，某个词和它后面或前面一个词的相对位置关系。
    
    - 例如，sin(α+β)=sin(α)cos(β)+cos(α)sin(β)。通过三角函数的和角公式，我们可以用 PEpos​ 的分量来表示 PEpos+k​ 的分量。这为模型提供了捕获相对位置关系的便利。
        
2. **可以推广到任意长度的序列：** 由于公式不依赖于序列的固定长度，即使在训练时没有见过某个长度的序列，模型也能够为新位置生成位置编码。这使得模型可以处理比训练时更长的序列。

#### 2. 编码器 (Encoder)

编码器由一个堆叠的模块构成，图中用 "Nx" 表示，这意味着这个模块被重复了 N 次。在原始论文中，N=6。

[每个编码器模块包含两个主要子层](#多头注意力机制的输入和输出)

- **多头自注意力 (Multi-Head Attention)**: 这是 Transformer 的核心。它允许模型同时关注输入序列中的所有词，并根据它们的相对重要性为每个词分配权重。多头 (Multi-Head) 意味着模型不是只执行一次注意力计算，而是并行地执行多次（有不同的权重矩阵），然后将结果拼接起来，从而捕获不同类型的信息。图中的 "Multi-Head Attention" 模块就是这个。
    
- **前馈网络 (Feed Forward)**: 在注意力层之后，每个位置的输出都会独立地经过一个全连接的前馈网络。这个网络通常包含两个线性变换和一个激活函数（在原始论文中是 ReLU）。图中的 "Feed Forward" 模块就是这个。
  前馈网络的具体公式如下，其中 $W_1, b_1, W_2, b_2$ 是可学习的参数： $$ FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2 $$

在每个子层（注意力层和前馈网络）之后，都有一个 **"Add & Norm"** 模块。这代表了：

- **残差连接 (Residual Connection)**: 将子层的输入直接加到子层的输出上。这有助于解决深度网络中的梯度消失问题，使得训练更深的模型成为可能。
    
- **层归一化 (Layer Normalization)**: 对每个样本的特征进行归一化，使得训练更稳定。
    

#### 3. 解码器 (Decoder)

解码器也由一个堆叠的模块构成，同样用 "Nx" 表示（原始论文中也是 N=6）。

每个解码器模块包含三个主要子层：

- **掩码多头自注意力 (Masked Multi-Head Attention)**: 这个和编码器中的自注意力类似，但是多了一个**掩码 (Mask)** 操作。在训练解码器时，为了防止模型“偷看”未来的单词，它只能关注当前位置和之前的位置。掩码就是用来屏蔽掉未来位置的信息。
    
- **编码器-解码器多头注意力 (Encoder-Decoder Multi-Head Attention)**: 这是解码器的独有部分。它使得解码器可以关注编码器的输出，从而在生成每个输出词时，能够“回顾”输入句子中的所有相关信息。
    
- **前馈网络 (Feed Forward)**: 和编码器中的前馈网络相同。
    

和编码器一样，每个子层之后也都有一个 **"Add & Norm"** 模块。

#### 4. 输出层

解码器的堆栈之后是输出层，它将解码器的最终输出转换为词汇表中每个词的概率：

- **线性层 (Linear)**: 一个全连接层，将解码器最终的向量表示映射到词汇表的大小。
    
- **Softmax**: 将线性层的输出转换为概率分布，使得所有词的概率总和为1。我们选择概率最高的那个词作为当前步的输出。
    

---
### 多头注意力机制的输入和输出

多头注意力机制（Multi-Head Attention）是 Transformer 的核心组件之一。它的主要目的是让模型能够同时在多个不同的“表示子空间”（representation subspaces）中关注输入序列的不同部分。

#### 输入：Q, K, V

多头注意力机制的输入是三个向量序列：
* **Queries (Q)**：查询向量。
* **Keys (K)**：键向量。
* **Values (V)**：值向量。

这三个输入向量序列的维度通常是相同的，都是 **(序列长度, $d_{model}$)**。

* 在**编码器（Encoder）的自注意力层中，Q, K, V 都来自同一个输入**：编码器前一个子层的输出。
* 在**解码器（Decoder）的自注意力层中，Q, K, V 都来自同一个输入**：解码器前一个子层的输出（并且有掩码）。
* 在**解码器**的编码器-解码器注意力层中，Q 来自**解码器**前一个子层的输出，而 K 和 V 则来自**编码器**的最终输出。

#### 输出：$Z$

多头注意力机制的输出是一个经过加权平均和线性变换后的向量序列，其维度通常也是 **(序列长度, $d_{model}$)**。

这个输出 $Z$ 包含了对输入序列中所有相关信息的加权和，每个输出向量都反映了对输入序列中所有位置的“关注”程度。

#### 例子：一个简单的句子

假设我们有一个句子，"The cat sat on the mat."。

我们来看看**编码器**中的多头自注意力层是如何工作的。

**1. 输入**
假设输入到多头注意力层的向量序列是 $X$。$X$ 是一个形状为 **(6, $d_{model}$)** 的矩阵，其中 6 是句子的长度，每一行代表一个词的向量（即词嵌入加上位置编码）。

**2. Q, K, V 的生成**
模型首先为每个“头”（head）生成 Q, K, V。假设我们有 $h=8$ 个头，模型的维度 $d_{model}=512$。每个头的维度 $d_k = d_v = d_{model}/h = 512/8 = 64$。

模型会学习三个权重矩阵 $W^Q, W^K, W^V$ 来将输入 $X$ 映射到 Q, K, V。但多头注意力机制实际上为每个头学习独立的投影矩阵 $W_{i}^Q, W_{i}^K, W_{i}^V$。  

* 对于第 $i$ 个头，它会生成自己的 Q, K, V 矩阵：
    * $Q_i = X W_{i}^Q$
    * $K_i = X W_{i}^K$
    * $V_i = X W_{i}^V$
    这些 Q, K, V 矩阵的维度都是 **(6, 64)**。

**3. 每个头的注意力计算**
对于每个头，它独立地计算一个缩放点积注意力（Scaled Dot-Product Attention）。

* **注意力分数**：计算 Q 和 K 的点积，得到一个注意力分数矩阵。
    * $Score_i = Q_i K_i^T$
    这个 $Score_i$ 是一个 **(6, 6)** 的矩阵，其中每个元素代表一个词对另一个词的“关注”程度。例如，$Score_{i}[1, 3]$ 表示第一个词（"The"）对第三个词（"sat"）的关注度。

* **缩放**：将注意力分数除以 $\sqrt{d_k}$。
    * $Score'_i = Score_i / \sqrt{64}$

* **Softmax**：对缩放后的分数矩阵的每一行进行 Softmax 操作，得到注意力权重矩阵 $A_i$。
    * $A_i = \text{softmax}(Score'_i)$
    $A_i$ 也是一个 **(6, 6)** 的矩阵，它的每一行都表示一个词对句子中所有词的权重分布，这些权重加起来为 1。

* **加权求和**：将注意力权重矩阵 $A_i$ 与 V 矩阵相乘，得到该头的输出。
    * $Head_i = A_i V_i$
    这个 $Head_i$ 的维度是 **(6, 64)**。

**4. 多头输出的拼接和线性变换**
所有 8 个头的输出 $Head_1, ..., Head_8$ 都被拼接（concatenate）起来，形成一个大的矩阵 $Z_{concat}$。
* $Z_{concat} = [Head_1; Head_2; ...; Head_8]$
$Z_{concat}$ 的维度是 **(6, 64 * 8)**，也就是 **(6, 512)**。

最后，这个拼接后的矩阵 $Z_{concat}$ 经过一个最终的线性投影矩阵 $W^O$ 的变换，得到多头注意力机制的最终输出 $Z$。
* $Z = Z_{concat} W^O$
这个最终输出 $Z$ 的维度是 **(6, $d_{model}$)**，也就是 **(6, 512)**。

**总结**

多头注意力机制的输入是 Q, K, V，在编码器中它们都来自同一个输入。通过并行运行多个注意力头，模型能够从不同的角度和子空间去理解输入序列中词与词之间的关系。每个头捕捉到不同的信息（例如，一个头可能关注语法关系，另一个可能关注语义关系）。最后，所有这些信息被拼接并线性投影，形成一个丰富的、包含多种关系信息的输出向量序列，其维度与输入保持一致。

### 前馈网络（Feed Forward Network）

前馈网络是 Transformer 编码器和解码器中的另一个关键子层，它位于多头注意力机制之后。它的结构相对简单，但其作用至关重要。

#### 前馈网络的结构

前馈网络是一个**全连接层**（fully connected layer），它由两个线性变换（linear transformations）和一个激活函数组成。

* **第一个线性变换**：将输入向量的维度从 $d_{model}$ 扩展到一个更大的维度，通常是 $d_{ff}$。在原始论文中，$d_{ff}$ 是 $4 \times d_{model}$。
* **激活函数**：在第一个线性变换后应用一个非线性激活函数，通常是 **ReLU**（Rectified Linear Unit）。
* **第二个线性变换**：将向量的维度从 $d_{ff}$ 压缩回原始的 $d_{model}$ 维度。

#### 前馈网络的作用

前馈网络的主要作用是**对每个位置的输出进行独立和非线性的变换**。

* **独立性**：与多头注意力机制不同，前馈网络是**逐位置（position-wise）**操作的。这意味着对于序列中的每一个位置，模型都使用**相同的**前馈网络来处理其对应的向量。前馈网络不会在不同位置之间共享信息。
* **非线性变换**：前馈网络通过两个线性层和一个非线性激活函数，为模型提供了学习更复杂特征和模式的能力。它允许模型对注意力机制捕捉到的信息进行进一步的“思考”和处理。

前馈网络可以被看作是对每个词向量进行的一次“深度”处理，使其能够更好地表示该词在当前上下文中的含义。

#### 公式表示

前馈网络的计算可以用以下公式表示：

$$
FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2
$$

* $x$：输入向量，维度为 $d_{model}$。
* $W_1$：第一个权重矩阵，形状为 $(d_{model}, d_{ff})$。
* $b_1$：第一个偏置向量，维度为 $d_{ff}$。
* $\max(0, ...)$：ReLU 激活函数。
* $W_2$：第二个权重矩阵，形状为 $(d_{ff}, d_{model})$。
* $b_2$：第二个偏置向量，维度为 $d_{model}$。

#### 例子：一个词向量的变换

假设我们的模型维度 $d_{model} = 512$，前馈网络的内部维度 $d_{ff} = 2048$。

现在我们考虑一个句子中的单个词向量 $x$，其维度是 **(1, 512)**。

**1. 步骤 1：第一个线性变换 + ReLU 激活**

* 输入：词向量 $x$ (1, 512)
* 权重矩阵 $W_1$：(512, 2048)
* 偏置向量 $b_1$：(1, 2048)

计算过程：
1.  首先进行矩阵乘法：$x \cdot W_1$。得到一个维度为 **(1, 2048)** 的向量。
2.  然后加上偏置向量：$xW_1 + b_1$。维度仍然是 **(1, 2048)**。
3.  最后应用 ReLU 激活函数：$\max(0, xW_1 + b_1)$。这个操作会使向量中的所有负值变为 0。

经过这一步，我们的词向量从 512 维扩展到了 2048 维。

**2. 步骤 2：第二个线性变换**

* 输入：经过 ReLU 激活的向量（1, 2048）
* 权重矩阵 $W_2$：(2048, 512)
* 偏置向量 $b_2$：(1, 512)

计算过程：
1.  进行矩阵乘法：$\max(0, xW_1 + b_1) \cdot W_2$。得到一个维度为 **(1, 512)** 的向量。
2.  然后加上偏置向量：$\max(0, xW_1 + b_1)W_2 + b_2$。维度仍然是 **(1, 512)**。

最终，前馈网络输出一个维度为 **(1, 512)** 的新向量。

#### 总结

前馈网络接收一个维度为 $d_{model}$ 的向量，通过**扩展-激活-压缩**的过程，将其维度变换为 $d_{ff}$，再变回 $d_{model}$。

这个过程是独立地应用于序列中每个位置的向量的。虽然它不涉及序列中不同位置之间的交互，但它为模型提供了强大的非线性建模能力，是对多头注意力机制捕捉到的全局上下文信息的一种**局部精炼**和**深层处理**。

---

## Pre norm VS post norm

### 1. Post-Norm (原始 Transformer)

**Post-Norm** 是原始论文 "Attention Is All You Need" 中使用的架构，它将层归一化（Layer Normalization）和残差连接（Residual Connection）的“Add & Norm”模块放置在每个子层（如多头注意力或前馈网络）之后。

* **结构详解**：

    在 Post-Norm 架构中，一个子层（Sublayer）的计算流程如下：
    1. 接收输入向量 $x$。
    2. 将 $x$ 传入子层，得到子层输出 $\text{Sublayer}(x)$。
    3. 将子层输出与原始输入 $x$ 相加（残差连接），得到 $x + \text{Sublayer}(x)$。
    4. 对这个和进行层归一化，得到最终的输出 $x_{out}$。

    * **公式表示**：
        $$
        x_{out} = \text{LayerNorm}(x + \text{Sublayer}(x))
        $$

* **优点**：

    * **更好的最终性能**：在经过大量超参数调优和学习率预热后，如果模型能够稳定收敛，Post-Norm 模型在许多任务上通常能达到比 Pre-Norm 更好的最终性能。这被认为是由于它在模型深层对残差连接进行了更直接的累加，保留了更多原始信息。
    * **更强的泛化能力**：由于其在训练后期的稳定性和对原始输入的直接累加，Post-Norm 模型有时能表现出更好的泛化能力。

* **缺点**：

    * **训练不稳定**：这是 Post-Norm 最显著的缺点，尤其是在深度很深的 Transformer 模型中。在训练初期，特别是在没有学习率预热的情况下，梯度很容易发生爆炸。这是因为残差连接会将每一层的输出累加到下一层，如果某一层输出的方差很大，这个误差会在深层网络中累积并放大，导致不稳定的训练。
    * **依赖学习率预热**：为了解决训练不稳定的问题，Post-Norm 模型通常需要一个“学习率预热”阶段。这个过程从一个很小的学习率开始，逐渐增加到预设的最大值，从而让模型在训练初期缓慢地适应，避免梯度爆炸。这增加了训练的复杂性和超参数调优的工作量。

### 2. Pre-Norm (现代 Transformer 常用)

**Pre-Norm** 是一种改进的 Transformer 架构，它将层归一化放置在每个子层**之前**。这种设计旨在解决 Post-Norm 的训练不稳定性问题，使得训练深度网络更加容易。

* **结构详解**：

    在 Pre-Norm 架构中，一个子层（Sublayer）的计算流程如下：
    1. 接收输入向量 $x$。
    2. 首先对 $x$ 进行层归一化，得到 $\text{LayerNorm}(x)$。
    3. 将归一化后的向量传入子层，得到子层输出 $\text{Sublayer}(\text{LayerNorm}(x))$。
    4. 将子层输出与原始输入 $x$ 相加（残差连接），得到最终的输出 $x_{out}$。

    * **公式表示**：
        $$
        x_{out} = x + \text{Sublayer}(\text{LayerNorm}(x))
        $$

* **优点**：

    * **极高的训练稳定性**：这是 Pre-Norm 最大的优势。由于每个子层都接收归一化后的输入，其输入数据的方差被有效控制，因此子层的输出方差也得到了有效限制。这阻止了梯度在深层网络中的累积和爆炸，使得训练过程非常稳定。
    * **无需学习率预热**：由于训练过程天生稳定，Pre-Norm 模型通常不需要复杂的学习率预热策略。可以直接使用较大的学习率进行训练，这简化了超参数调优过程，也可能加快训练收敛。
    * **可以训练更深的网络**：Pre-Norm 架构的稳定性使得训练包含数百甚至数千层的超深度 Transformer 成为可能，这在 Post-Norm 中是难以实现的。

* **缺点**：

    * **最终性能可能略逊于 Post-Norm**：在某些情况下，尤其是在小规模模型上，Pre-Norm 模型的最终性能可能略低于经过精心调优的 Post-Norm 模型。这通常被看作是一种“稳定 vs. 性能”的权衡。

### 总结与对比

| 特性 | Post-Norm (原始 Transformer) | Pre-Norm (现代常用) |
|---|---|---|
| **归一化位置** | 子层之后，残差连接之后 | 子层之前 |
| **训练稳定性** | 差，容易出现梯度爆炸，特别是深层网络 | 优秀，非常稳定，适合训练深度网络 |
| **学习率预热** | 通常需要，是保证训练成功的关键 | 通常不需要，简化了训练流程 |
| **最终性能** | 在特定条件下可能更高，但需要精细调参 | 稳定可靠，但在某些情况下可能略低 |
| **训练收敛速度** | 初始慢，需要预热 | 初始快，训练过程更平滑 |
| **代表模型** | 原始论文的 Transformer | 许多现代大型语言模型，如 GPT-2/3、BERT 的变体 |

### 为什么 Pre-Norm 更稳定？

简单来说，Pre-Norm 的稳定性的根本原因在于它**控制了网络中的信号流**。

在 Post-Norm 中，每一层的输出都是 $x_{out} = \text{LayerNorm}(x + \text{Sublayer}(x))$。由于 $\text{Sublayer}(x)$ 的方差可能很大，并且它被直接加到 $x$ 上，这个不稳定的信号会传递到下一层。即使之后进行了归一化，这种累积效应在深层网络中仍然难以控制，导致梯度爆炸。

而在 Pre-Norm 中，每一层的输入 $\text{Sublayer}$ 都被强制归一化，即 $\text{Sublayer}(\text{LayerNorm}(x))$。这意味着每一层的输出方差被有效抑制在一个可控的范围内。残差连接的累加虽然仍在进行，但由于每一步的累加值方差都很小，整个网络的信号流变得非常稳定，从而避免了梯度爆炸。


---

## Layer Normalization (LayerNorm) vs. RMS Normalization (RMSNorm)

LayerNorm 和 RMSNorm 都是用于稳定 Transformer 模型训练的归一化技术，它们都对每个训练样本独立进行归一化，与 Batch Normalization 不同。它们的主要区别在于如何计算归一化的统计量。

### 1. Layer Normalization (LayerNorm)

LayerNorm 是在 Transformer 模型中最早被广泛使用的归一化方法，它的目标是使每个特征向量的激活值具有**零均值（zero mean）和单位方差（unit variance）**。

* **计算步骤**：
    对于一个输入向量 $x \in \mathbb{R}^D$（其中 $D$ 是特征维度），LayerNorm 的计算分为以下几步：
    1. **计算均值**：对向量 $x$ 的所有元素求平均值 $\mu$。
    $$
    \mu = \frac{1}{D} \sum_{i=1}^{D} x_i
    $$
    2. **计算方差**：对向量 $x$ 的所有元素，计算其与均值 $\mu$ 的差的平方的平均值 $\sigma^2$。
    $$
    \sigma^2 = \frac{1}{D} \sum_{i=1}^{D} (x_i - \mu)^2
    $$
    3. **归一化**：使用均值和方差对输入向量进行归一化。为了防止除以零，会添加一个很小的常数 $\epsilon$。
    $$
    \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
    $$
    4. **仿射变换 (Affine Transformation)**：归一化后，模型使用两个可学习的参数 $\gamma$（缩放因子）和 $\beta$（偏移量）对结果进行线性变换。
    $$
    y_i = \gamma \hat{x}_i + \beta
    $$

* **核心思想**：LayerNorm 旨在通过**均值居中（mean-centering）**和**方差缩放（variance-scaling）**来稳定激活值的分布。这使得模型训练更加稳定，并有助于解决梯度消失和爆炸问题。

### 2. RMS Normalization (RMSNorm)

RMSNorm 是 LayerNorm 的一个简化版本，它在 2019 年的论文 "Root Mean Square Layer Normalization" 中被提出。RMSNorm 的核心假设是：在许多情况下，均值居中操作（即减去均值 $\mu$）并不是必需的，只保留方差缩放就足够了。

* **计算步骤**：
    对于一个输入向量 $x \in \mathbb{R}^D$（其中 $D$ 是特征维度），RMSNorm 的计算步骤如下：
    1. **计算均方根 (Root Mean Square, RMS)**：计算向量 $x$ 的所有元素的平方的平均值的平方根。
    $$
    \text{RMS}(x) = \sqrt{\frac{1}{D} \sum_{i=1}^{D} x_i^2}
    $$
    2. **归一化**：直接使用 RMS 对输入向量进行归一化。
    $$
    \hat{x}_i = \frac{x_i}{\text{RMS}(x) + \epsilon}
    $$
    3. **仿射变换 (Affine Transformation)**：与 LayerNorm 类似，RMSNorm 也使用可学习的缩放因子 $\gamma$。但通常情况下，它**不使用偏移量 $\beta$**。
    $$
    y_i = \gamma \hat{x}_i
    $$

* **核心思想**：RMSNorm 假设**方差缩放**是归一化的关键，而**均值居中**是多余的。它通过移除均值计算步骤来简化 LayerNorm。

### 3. LayerNorm 与 RMSNorm 的详细对比

| 特性 | Layer Normalization (LayerNorm) | RMS Normalization (RMSNorm) |
|---|---|---|
| **归一化统计量** | **均值**和**标准差** | **均方根 (RMS)** |
| **计算公式** | $y = \gamma \frac{x - \mu}{\sigma} + \beta$ | $y = \gamma \frac{x}{\text{RMS}(x)}$ |
| **均值居中** | **是**（通过减去 $\mu$） | **否** |
| **可学习参数** | $\gamma$ (缩放) 和 $\beta$ (偏移) | $\gamma$ (缩放) |
| **计算效率** | 较低 | 较高（因为它省去了均值计算） |
| **训练稳定性** | 非常好 | 同样非常好，甚至在某些情况下表现更好 |
| **模型性能** | 强大，是原始 Transformer 的基石 | 强大，在许多现代大型语言模型（如 LLaMA, GPT-NeoX 等）中广泛应用，性能与 LayerNorm 相当或略优 |

### 4. 为什么 RMSNorm 越来越受欢迎？

* **计算效率**：RMSNorm 的计算比 LayerNorm 更快。在 LayerNorm 中，需要计算均值和方差，而在 RMSNorm 中，只需要计算均方根。虽然这看起来只是一个微小的区别，但在现代大型模型中，归一化层会占据相当一部分计算开销，尤其是在内存带宽受限的情况下，RMSNorm 的计算简化能带来明显的加速。

* **性能相当或更好**：研究表明，移除均值居中操作并没有显著影响模型的性能，甚至在一些情况下，RMSNorm 的表现更好。这表明在大多数深度学习任务中，**方差缩放**才是归一化的关键，而均值居中的作用相对较小。

* **简化模型设计**：RMSNorm 的设计更简单，没有偏移量参数 $\beta$（尽管一些实现会保留它）。这减少了模型的参数量，并在某些情况下能够让模型更高效。

### 结论

LayerNorm 和 RMSNorm 都是优秀的归一化技术，用于稳定 Transformer 模型的训练。LayerNorm 是原始 Transformer 的选择，它通过均值居中和方差缩放来规范激活值。而 RMSNorm 是 LayerNorm 的一个简化版本，它去除了均值居中，只保留了方差缩放。

在实践中，由于 RMSNorm 具有更高的计算效率，并且在大多数情况下能够提供与 LayerNorm 相当甚至更好的性能，它在许多最新的大型语言模型中越来越受欢迎，成为一种替代 LayerNorm 的流行选择。


---

## 激活函数：ReLU, GeLU, Swish 详解

在深度学习中，激活函数是为神经网络引入非线性的关键组件。不同的激活函数有不同的特性，对模型的训练稳定性和性能有着显著影响。这里我们将详细解释并比较 ReLU、GeLU 和 Swish。

### 1. ReLU (Rectified Linear Unit)

ReLU 是最常用的一种激活函数，尤其是在深度学习的早期和许多计算机视觉模型中。

* **公式**：
  $$
  f(x) = \max(0, x)
  $$

* **工作原理**：
  当输入 $x$ 大于 0 时，输出就是 $x$；当输入 $x$ 小于或等于 0 时，输出就是 0。

* **优点**：计算效率高，缓解梯度消失，产生稀疏性。
* **缺点**：死亡 ReLU 问题。

* **数值例子**：
  假设输入值为 $x = [-2.0, -0.5, 0.0, 0.5, 2.0]$
  * 对于 $x = -2.0$，ReLU($-2.0$) = $\max(0, -2.0) = 0$
  * 对于 $x = -0.5$，ReLU($-0.5$) = $\max(0, -0.5) = 0$
  * 对于 $x = 0.0$，ReLU($0.0$) = $\max(0, 0.0) = 0$
  * 对于 $x = 0.5$，ReLU($0.5$) = $\max(0, 0.5) = 0.5$
  * 对于 $x = 2.0$，ReLU($2.0$) = $\max(0, 2.0) = 2.0$
  
  因此，ReLU 的输出为 $[0.0, 0.0, 0.0, 0.5, 2.0]$。

### 2. GeLU (Gaussian Error Linear Unit)

GeLU 在许多现代大型语言模型（如 BERT、GPT-2、GPT-3）中得到了广泛应用。

* **公式**：
  $$
  GeLU(x) = x \cdot \Phi(x)
  $$
  其中，$\Phi(x)$ 是标准正态分布的累积分布函数（CDF）。

* **工作原理**：
  GeLU 并非简单地将负值置为零，而是将输入值乘以其服从标准正态分布的概率。在 $x$ 接近 0 的区域，GeLU 的曲线是平滑而非 ReLU 的尖锐转折。
  
  这里的 Φ(x) 就是标准正态分布的累积分布函数。GeLU 的设计思想是，一个神经元的输出 x 应该以与它自身值成正比的概率被“激活”。
	当 x 很大时，Φ(x) 接近 1，GeLU(x) 接近 x。
	
	当 x 接近 0 时，Φ(x) 接近 0.5，GeLU(x) 接近 0。
	
	当 x 很小时，Φ(x) 接近 0，GeLU(x) 也接近 0

* **优点**：平滑的非线性，更好的性能。
* **缺点**：计算略复杂。

* **数值例子**：
  假设输入值为 $x = [-2.0, -0.5, 0.0, 0.5, 2.0]$。我们使用标准正态分布的 CDF 值来计算：
  * 对于 $x = -2.0$，$\Phi(-2.0) \approx 0.023$。
    $GeLU(-2.0) \approx -2.0 \cdot 0.023 = -0.046$
  * 对于 $x = -0.5$，$\Phi(-0.5) \approx 0.309$。
    $GeLU(-0.5) \approx -0.5 \cdot 0.309 = -0.155$
  * 对于 $x = 0.0$，$\Phi(0.0) = 0.5$。
    $GeLU(0.0) = 0.0 \cdot 0.5 = 0.0$
  * 对于 $x = 0.5$，$\Phi(0.5) \approx 0.691$。
    $GeLU(0.5) \approx 0.5 \cdot 0.691 = 0.346$
  * 对于 $x = 2.0$，$\Phi(2.0) \approx 0.977$。
    $GeLU(2.0) \approx 2.0 \cdot 0.977 = 1.954$
  
  因此，GeLU 的输出近似为 $[-0.046, -0.155, 0.0, 0.346, 1.954]$。

### 3. Swish (SiLU)

Swish 在许多视觉任务和现代模型中显示出比 ReLU 更好的性能。它也被称为 SiLU。

* **公式**：
  $$
  Swish(x) = x \cdot \sigma(x)
  $$
  其中，$\sigma(x) = \frac{1}{1 + e^{-x}}$ 是 Sigmoid 函数。

* **工作原理**：
  Swish 通过 Sigmoid 函数对输入 $x$ 进行“自门控”。它在负值区域有一条平滑的“凹槽”，而不是完全为零，这有助于缓解 ReLU 的“死亡神经元”问题。

* **优点**：平滑的非线性，非单调性，性能好。
* **缺点**：计算略复杂。

* **数值例子**：
  假设输入值为 $x = [-2.0, -0.5, 0.0, 0.5, 2.0]$。我们使用 Sigmoid 函数来计算：
  * 对于 $x = -2.0$，$\sigma(-2.0) \approx 0.119$。
    $Swish(-2.0) \approx -2.0 \cdot 0.119 = -0.238$
  * 对于 $x = -0.5$，$\sigma(-0.5) \approx 0.378$。
    $Swish(-0.5) \approx -0.5 \cdot 0.378 = -0.189$
  * 对于 $x = 0.0$，$\sigma(0.0) = 0.5$。
    $Swish(0.0) = 0.0 \cdot 0.5 = 0.0$
  * 对于 $x = 0.5$，$\sigma(0.5) \approx 0.622$。
    $Swish(0.5) \approx 0.5 \cdot 0.622 = 0.311$
  * 对于 $x = 2.0$，$\sigma(2.0) \approx 0.881$。
    $Swish(2.0) \approx 2.0 \cdot 0.881 = 1.762$
  
  因此，Swish 的输出近似为 $[-0.238, -0.189, 0.0, 0.311, 1.762]$。


---

## 正弦-余弦位置编码 (Sinusoidal Positional Embedding) vs. 旋转位置编码 (Rotary Positional Embedding, RoPE)

这两种方法都使用正弦和余弦函数来编码词语的位置信息，但它们在根本上是不同的。
正弦-余弦编码是一种**绝对的、相加的**方法，而 RoPE 是一种**相对的、相乘的**方法。

### 正弦-余弦位置编码 (Sinusoidal Positional Embedding)

这是 2017 年原始 Transformer 论文《Attention Is All You Need》中提出的位置编码方法。它是一种**绝对**编码方法，因为它为序列中的每个位置分配一个独特的、固定的向量。它也是**相加**的，因为这个位置向量被直接加到词嵌入向量上。

* **工作原理**：
    它使用不同频率的正弦和余弦函数为序列中的每个位置生成一个独特的向量。这些频率呈几何级数递减，因此向量中的某些维度变化很快（捕捉微观位置），而另一些维度变化很慢（捕捉宏观位置）。对于位置 $pos$ 和维度索引 $i$ 的词，其位置编码向量由以下公式计算：

    $$
    PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})
    $$

    $$
    PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})
    $$

* **优点**：无需额外参数，理论上的长度外推性。
* **缺点**：编码的是绝对位置，对长序列性能下降。

* **例子**：

    假设我们有一个句子 "I am a student."，模型的词嵌入维度 $d_{model}=4$。我们来计算每个词的位置编码。
    
    * **位置 0 (I)**：
        $PE_{(0, 0)} = \sin(0 / 10000^{0/4}) = \sin(0) = 0$
        $PE_{(0, 1)} = \cos(0 / 10000^{0/4}) = \cos(0) = 1$
        $PE_{(0, 2)} = \sin(0 / 10000^{2/4}) = \sin(0) = 0$
        $PE_{(0, 3)} = \cos(0 / 10000^{2/4}) = \cos(0) = 1$
        其位置编码向量为：$[0, 1, 0, 1]$
    
    * **位置 1 (am)**：
        $PE_{(1, 0)} = \sin(1 / 10000^{0/4}) = \sin(1)$
        $PE_{(1, 1)} = \cos(1 / 10000^{0/4}) = \cos(1)$
        $PE_{(1, 2)} = \sin(1 / 10000^{2/4}) = \sin(1/100)$
        $PE_{(1, 3)} = \cos(1 / 10000^{2/4}) = \cos(1/100)$
        其位置编码向量为：$[\sin(1), \cos(1), \sin(0.01), \cos(0.01)]$
    
    * **位置 2 (a)**：
        $PE_{(2, 0)} = \sin(2 / 10000^{0/4}) = \sin(2)$
        ...以此类推...
    
    最终，每个词的词嵌入向量（如 "I" 的嵌入 $E_I$）都会加上其相应的位置编码向量（$E_I + PE_0$），作为 Transformer 的输入。

---

### 旋转位置编码 (Rotary Positional Embedding, RoPE)

RoPE 是一种更先进、更复杂的方法，被许多现代大型语言模型（如 Llama、GPT-NeoX 等）所采用。它是一种**相对**编码方法，因为它关注的是**词与词之间的距离**，而不是它们的绝对位置。它也是**相乘**的，因为它通过旋转的方式来修改词向量。

* **工作原理**：
    RoPE 不会创建单独的位置向量。相反，它在自注意力计算过程中，对**查询（Q）**和**键（K）**向量应用一种**旋转**操作。旋转的角度取决于词在序列中的位置。这种旋转经过精心设计，使得两个旋转后的向量点积只取决于它们的**相对距离**。
    
    核心思想：将向量 $q$ 和 $k$ 看作是复数向量，对它们进行旋转。一个向量 $q$ 在位置 $m$ 处的旋转操作可以表示为：$q_m = q \odot R_m$，其中 $R_m$ 是旋转矩阵。
    
    两个位置 $m$ 和 $n$ 的向量 $q_m$ 和 $k_n$ 的点积可以推导出只与它们的相对距离 $m-n$ 相关：
    $$
    <q_m, k_n> = <q \odot R_m, k \odot R_n> = <q, k \odot R_{n-m}>
    $$
    这使得模型能够天然地理解相对位置信息。

* **优点**：直接编码相对位置，优秀的外推性，与注意力机制深度融合。
* **缺点**：实现更复杂。

* **例子**：

    假设我们有一个 2 维的查询向量 $q = [q_0, q_1]$，它在位置 $m$。RoPE 会对其应用旋转。
    
    $$
    q_m = \begin{bmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{bmatrix} \begin{bmatrix} q_0 \\ q_1 \end{bmatrix} = \begin{bmatrix} q_0 \cos(m\theta) - q_1 \sin(m\theta) \\ q_0 \sin(m\theta) + q_1 \cos(m\theta) \end{bmatrix}
    $$
    
    同样地，键向量 $k$ 在位置 $n$ 处被旋转为 $k_n$。然后，在计算注意力分数时，我们计算旋转后的向量的点积 $<q_m, k_n>$。这个点积的值只取决于 $q$ 和 $k$ 的原始值以及它们的**相对距离** $m-n$。
    
    例如，对于句子 "the cat sat on the mat"，RoPE 会让模型在计算 "cat" 和 "mat" 的注意力分数时，只关注它们之间相隔的 4 个位置，而不会关心它们是在句子开头还是结尾。

### 总结

| 特性 | 正弦-余弦位置编码 | 旋转位置编码 (RoPE) |
| :--- | :--- | :--- |
| **方法** | 绝对，相加 | 相对，相乘 |
| **应用方式** | 一个固定的向量**加**到词嵌入上。 | 在注意力计算中，对查询和键向量应用**旋转**。 |
| **关注点** | 词的绝对位置。 | 词与词之间的相对距离。 |
| **实现位置** | 嵌入层的一部分。 | 注意力计算的一部分。 |
| **外推性** | 可以处理长序列，但性能可能下降。 | 专为长序列设计，性能稳定。 |
| **应用模型** | 原始 Transformer (2017)。 | 现代大语言模型（Llama, GPT-NeoX 等）。 |