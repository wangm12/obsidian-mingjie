## Original Transformer

![[original transformer.png]]

它主要分为三个部分：**编码器 (Encoder)**、**解码器 (Decoder)** 和模型右侧的**回顾部分 (Review)**

### 原始 Transformer 模型架构图

这个架构图是一个**编码器-解码器**结构。左边是编码器，右边是解码器。

#### 1. 输入和嵌入层 (Inputs and Embedding)

- **输入 (Inputs)**: 模型接收输入的词序列。这些可以是句子中的单词。
    
- **输入嵌入 (Input Embedding)**: 输入的每个单词被转换成一个高维的向量，这个向量包含了词的语义信息。
    
- **位置编码 (Positional Encoding)**: 这是 Transformer 的一个关键创新。由于 Transformer 不像 RNN 那样有固定的序列处理顺序，它需要一种方式来知道单词在句子中的位置信息。位置编码就是解决这个问题的。它是一个与输入嵌入相同维度的向量，被加到输入嵌入向量上，从而让模型能够区分不同位置的单词。图中的 "Positional Encoding" 模块就是做这件事的。
  位置编码的具体公式如下，其中 $pos$ 是词在序列中的位置，**$i$ 是词嵌入向量的维度索引**，$d_{model}$ 是模型的维度： $$ PE_{(pos, 2i)} = \sin(pos/10000^{2i/d_{model}}) $$ $$ PE_{(pos, 2i+1)} = \cos(pos/10000^{2i/d_{model}}) $$

##### 什么是位置编码
假设模型的维度 $d_{model}=4$。我们为位置 $pos=0$ 和 $pos=1$ 的词计算位置编码向量。 
*计算位置 $pos=0$ 的位置编码向量 $PE_0$**： $$ PE_0 = [PE_{(0,0)}, PE_{(0,1)}, PE_{(0,2)}, PE_{(0,3)}] $$ $$ PE_{(0, 0)} = \sin(0/10000^{0/4}) = \sin(0) = 0 $$ $$ PE_{(0, 1)} = \cos(0/10000^{0/4}) = \cos(0) = 1 $$ $$ PE_{(0, 2)} = \sin(0/10000^{2/4}) = \sin(0) = 0 $$ $$ PE_{(0, 3)} = \cos(0/10000^{2/4}) = \cos(0) = 1 $$ 因此，$PE_0 = [0, 1, 0, 1]$。 * **计算位置 $pos=1$ 的位置编码向量 $PE_1$**： $$ PE_1 = [PE_{(1,0)}, PE_{(1,1)}, PE_{(1,2)}, PE_{(1,3)}] $$ $$ PE_{(1, 0)} = \sin(1/10000^{0/4}) = \sin(1) \approx 0.841 $$ $$ PE_{(1, 1)} = \cos(1/10000^{0/4}) = \cos(1) \approx 0.540 $$ $$ PE_{(1, 2)} = \sin(1/10000^{2/4}) = \sin(1/100) \approx 0.010 $$ $$ PE_{(1, 3)} = \cos(1/10000^{2/4}) = \cos(1/100) \approx 0.999 $$ 因此，$PE_1 = [\sin(1), \cos(1), \sin(0.01), \cos(0.01)]$。 最终，这些位置编码向量会被加到相应的词嵌入向量上，作为 Transformer 模型的输入。

##### 为什么选择正弦和余弦？

这种基于正弦和余弦函数的位置编码有两个关键优点：

1. **能够表示相对位置：** 任意位置 pos+k 的位置编码可以表示成位置 pos 的位置编码的线性函数。这意味着模型可以很容易地学习到，某个词和它后面或前面一个词的相对位置关系。
    
    - 例如，sin(α+β)=sin(α)cos(β)+cos(α)sin(β)。通过三角函数的和角公式，我们可以用 PEpos​ 的分量来表示 PEpos+k​ 的分量。这为模型提供了捕获相对位置关系的便利。
        
2. **可以推广到任意长度的序列：** 由于公式不依赖于序列的固定长度，即使在训练时没有见过某个长度的序列，模型也能够为新位置生成位置编码。这使得模型可以处理比训练时更长的序列。

#### 2. 编码器 (Encoder)

编码器由一个堆叠的模块构成，图中用 "Nx" 表示，这意味着这个模块被重复了 N 次。在原始论文中，N=6。

[每个编码器模块包含两个主要子层](#多头注意力机制的输入和输出)

- **多头自注意力 (Multi-Head Attention)**: 这是 Transformer 的核心。它允许模型同时关注输入序列中的所有词，并根据它们的相对重要性为每个词分配权重。多头 (Multi-Head) 意味着模型不是只执行一次注意力计算，而是并行地执行多次（有不同的权重矩阵），然后将结果拼接起来，从而捕获不同类型的信息。图中的 "Multi-Head Attention" 模块就是这个。
    
- **前馈网络 (Feed Forward)**: 在注意力层之后，每个位置的输出都会独立地经过一个全连接的前馈网络。这个网络通常包含两个线性变换和一个激活函数（在原始论文中是 ReLU）。图中的 "Feed Forward" 模块就是这个。
  前馈网络的具体公式如下，其中 $W_1, b_1, W_2, b_2$ 是可学习的参数： $$ FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2 $$

在每个子层（注意力层和前馈网络）之后，都有一个 **"Add & Norm"** 模块。这代表了：

- **残差连接 (Residual Connection)**: 将子层的输入直接加到子层的输出上。这有助于解决深度网络中的梯度消失问题，使得训练更深的模型成为可能。
    
- **层归一化 (Layer Normalization)**: 对每个样本的特征进行归一化，使得训练更稳定。
    

#### 3. 解码器 (Decoder)

解码器也由一个堆叠的模块构成，同样用 "Nx" 表示（原始论文中也是 N=6）。

每个解码器模块包含三个主要子层：

- **掩码多头自注意力 (Masked Multi-Head Attention)**: 这个和编码器中的自注意力类似，但是多了一个**掩码 (Mask)** 操作。在训练解码器时，为了防止模型“偷看”未来的单词，它只能关注当前位置和之前的位置。掩码就是用来屏蔽掉未来位置的信息。
    
- **编码器-解码器多头注意力 (Encoder-Decoder Multi-Head Attention)**: 这是解码器的独有部分。它使得解码器可以关注编码器的输出，从而在生成每个输出词时，能够“回顾”输入句子中的所有相关信息。
    
- **前馈网络 (Feed Forward)**: 和编码器中的前馈网络相同。
    

和编码器一样，每个子层之后也都有一个 **"Add & Norm"** 模块。

#### 4. 输出层

解码器的堆栈之后是输出层，它将解码器的最终输出转换为词汇表中每个词的概率：

- **线性层 (Linear)**: 一个全连接层，将解码器最终的向量表示映射到词汇表的大小。
    
- **Softmax**: 将线性层的输出转换为概率分布，使得所有词的概率总和为1。我们选择概率最高的那个词作为当前步的输出。
    

---
### 多头注意力机制的输入和输出

多头注意力机制（Multi-Head Attention）是 Transformer 的核心组件之一。它的主要目的是让模型能够同时在多个不同的“表示子空间”（representation subspaces）中关注输入序列的不同部分。

#### 输入：Q, K, V

多头注意力机制的输入是三个向量序列：
* **Queries (Q)**：查询向量。
* **Keys (K)**：键向量。
* **Values (V)**：值向量。

这三个输入向量序列的维度通常是相同的，都是 **(序列长度, $d_{model}$)**。

* 在**编码器（Encoder）的自注意力层中，Q, K, V 都来自同一个输入**：编码器前一个子层的输出。
* 在**解码器（Decoder）的自注意力层中，Q, K, V 都来自同一个输入**：解码器前一个子层的输出（并且有掩码）。
* 在**解码器**的编码器-解码器注意力层中，Q 来自**解码器**前一个子层的输出，而 K 和 V 则来自**编码器**的最终输出。

#### 输出：$Z$

多头注意力机制的输出是一个经过加权平均和线性变换后的向量序列，其维度通常也是 **(序列长度, $d_{model}$)**。

这个输出 $Z$ 包含了对输入序列中所有相关信息的加权和，每个输出向量都反映了对输入序列中所有位置的“关注”程度。

#### 例子：一个简单的句子

假设我们有一个句子，"The cat sat on the mat."。

我们来看看**编码器**中的多头自注意力层是如何工作的。

**1. 输入**
假设输入到多头注意力层的向量序列是 $X$。$X$ 是一个形状为 **(6, $d_{model}$)** 的矩阵，其中 6 是句子的长度，每一行代表一个词的向量（即词嵌入加上位置编码）。

**2. Q, K, V 的生成**
模型首先为每个“头”（head）生成 Q, K, V。假设我们有 $h=8$ 个头，模型的维度 $d_{model}=512$。每个头的维度 $d_k = d_v = d_{model}/h = 512/8 = 64$。

模型会学习三个权重矩阵 $W^Q, W^K, W^V$ 来将输入 $X$ 映射到 Q, K, V。但多头注意力机制实际上为每个头学习独立的投影矩阵 $W_{i}^Q, W_{i}^K, W_{i}^V$。  

* 对于第 $i$ 个头，它会生成自己的 Q, K, V 矩阵：
    * $Q_i = X W_{i}^Q$
    * $K_i = X W_{i}^K$
    * $V_i = X W_{i}^V$
    这些 Q, K, V 矩阵的维度都是 **(6, 64)**。

**3. 每个头的注意力计算**
对于每个头，它独立地计算一个缩放点积注意力（Scaled Dot-Product Attention）。

* **注意力分数**：计算 Q 和 K 的点积，得到一个注意力分数矩阵。
    * $Score_i = Q_i K_i^T$
    这个 $Score_i$ 是一个 **(6, 6)** 的矩阵，其中每个元素代表一个词对另一个词的“关注”程度。例如，$Score_{i}[1, 3]$ 表示第一个词（"The"）对第三个词（"sat"）的关注度。

* **缩放**：将注意力分数除以 $\sqrt{d_k}$。
    * $Score'_i = Score_i / \sqrt{64}$

* **Softmax**：对缩放后的分数矩阵的每一行进行 Softmax 操作，得到注意力权重矩阵 $A_i$。
    * $A_i = \text{softmax}(Score'_i)$
    $A_i$ 也是一个 **(6, 6)** 的矩阵，它的每一行都表示一个词对句子中所有词的权重分布，这些权重加起来为 1。

* **加权求和**：将注意力权重矩阵 $A_i$ 与 V 矩阵相乘，得到该头的输出。
    * $Head_i = A_i V_i$
    这个 $Head_i$ 的维度是 **(6, 64)**。

**4. 多头输出的拼接和线性变换**
所有 8 个头的输出 $Head_1, ..., Head_8$ 都被拼接（concatenate）起来，形成一个大的矩阵 $Z_{concat}$。
* $Z_{concat} = [Head_1; Head_2; ...; Head_8]$
$Z_{concat}$ 的维度是 **(6, 64 * 8)**，也就是 **(6, 512)**。

最后，这个拼接后的矩阵 $Z_{concat}$ 经过一个最终的线性投影矩阵 $W^O$ 的变换，得到多头注意力机制的最终输出 $Z$。
* $Z = Z_{concat} W^O$
这个最终输出 $Z$ 的维度是 **(6, $d_{model}$)**，也就是 **(6, 512)**。

**总结**

多头注意力机制的输入是 Q, K, V，在编码器中它们都来自同一个输入。通过并行运行多个注意力头，模型能够从不同的角度和子空间去理解输入序列中词与词之间的关系。每个头捕捉到不同的信息（例如，一个头可能关注语法关系，另一个可能关注语义关系）。最后，所有这些信息被拼接并线性投影，形成一个丰富的、包含多种关系信息的输出向量序列，其维度与输入保持一致。

### 前馈网络（Feed Forward Network）

前馈网络是 Transformer 编码器和解码器中的另一个关键子层，它位于多头注意力机制之后。它的结构相对简单，但其作用至关重要。

#### 前馈网络的结构

前馈网络是一个**全连接层**（fully connected layer），它由两个线性变换（linear transformations）和一个激活函数组成。

* **第一个线性变换**：将输入向量的维度从 $d_{model}$ 扩展到一个更大的维度，通常是 $d_{ff}$。在原始论文中，$d_{ff}$ 是 $4 \times d_{model}$。
* **激活函数**：在第一个线性变换后应用一个非线性激活函数，通常是 **ReLU**（Rectified Linear Unit）。
* **第二个线性变换**：将向量的维度从 $d_{ff}$ 压缩回原始的 $d_{model}$ 维度。

#### 前馈网络的作用

前馈网络的主要作用是**对每个位置的输出进行独立和非线性的变换**。

* **独立性**：与多头注意力机制不同，前馈网络是**逐位置（position-wise）**操作的。这意味着对于序列中的每一个位置，模型都使用**相同的**前馈网络来处理其对应的向量。前馈网络不会在不同位置之间共享信息。
* **非线性变换**：前馈网络通过两个线性层和一个非线性激活函数，为模型提供了学习更复杂特征和模式的能力。它允许模型对注意力机制捕捉到的信息进行进一步的“思考”和处理。

前馈网络可以被看作是对每个词向量进行的一次“深度”处理，使其能够更好地表示该词在当前上下文中的含义。

#### 公式表示

前馈网络的计算可以用以下公式表示：

$$
FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2
$$

* $x$：输入向量，维度为 $d_{model}$。
* $W_1$：第一个权重矩阵，形状为 $(d_{model}, d_{ff})$。
* $b_1$：第一个偏置向量，维度为 $d_{ff}$。
* $\max(0, ...)$：ReLU 激活函数。
* $W_2$：第二个权重矩阵，形状为 $(d_{ff}, d_{model})$。
* $b_2$：第二个偏置向量，维度为 $d_{model}$。

#### 例子：一个词向量的变换

假设我们的模型维度 $d_{model} = 512$，前馈网络的内部维度 $d_{ff} = 2048$。

现在我们考虑一个句子中的单个词向量 $x$，其维度是 **(1, 512)**。

**1. 步骤 1：第一个线性变换 + ReLU 激活**

* 输入：词向量 $x$ (1, 512)
* 权重矩阵 $W_1$：(512, 2048)
* 偏置向量 $b_1$：(1, 2048)

计算过程：
1.  首先进行矩阵乘法：$x \cdot W_1$。得到一个维度为 **(1, 2048)** 的向量。
2.  然后加上偏置向量：$xW_1 + b_1$。维度仍然是 **(1, 2048)**。
3.  最后应用 ReLU 激活函数：$\max(0, xW_1 + b_1)$。这个操作会使向量中的所有负值变为 0。

经过这一步，我们的词向量从 512 维扩展到了 2048 维。

**2. 步骤 2：第二个线性变换**

* 输入：经过 ReLU 激活的向量（1, 2048）
* 权重矩阵 $W_2$：(2048, 512)
* 偏置向量 $b_2$：(1, 512)

计算过程：
1.  进行矩阵乘法：$\max(0, xW_1 + b_1) \cdot W_2$。得到一个维度为 **(1, 512)** 的向量。
2.  然后加上偏置向量：$\max(0, xW_1 + b_1)W_2 + b_2$。维度仍然是 **(1, 512)**。

最终，前馈网络输出一个维度为 **(1, 512)** 的新向量。

#### 总结

前馈网络接收一个维度为 $d_{model}$ 的向量，通过**扩展-激活-压缩**的过程，将其维度变换为 $d_{ff}$，再变回 $d_{model}$。

这个过程是独立地应用于序列中每个位置的向量的。虽然它不涉及序列中不同位置之间的交互，但它为模型提供了强大的非线性建模能力，是对多头注意力机制捕捉到的全局上下文信息的一种**局部精炼**和**深层处理**。

---

## Pre norm VS post norm

### 1. Post-Norm (原始 Transformer)

**Post-Norm** 是原始论文 "Attention Is All You Need" 中使用的架构，它将层归一化（Layer Normalization）和残差连接（Residual Connection）的“Add & Norm”模块放置在每个子层（如多头注意力或前馈网络）之后。

* **结构详解**：

    在 Post-Norm 架构中，一个子层（Sublayer）的计算流程如下：
    1. 接收输入向量 $x$。
    2. 将 $x$ 传入子层，得到子层输出 $\text{Sublayer}(x)$。
    3. 将子层输出与原始输入 $x$ 相加（残差连接），得到 $x + \text{Sublayer}(x)$。
    4. 对这个和进行层归一化，得到最终的输出 $x_{out}$。

    * **公式表示**：
        $$
        x_{out} = \text{LayerNorm}(x + \text{Sublayer}(x))
        $$

* **优点**：

    * **更好的最终性能**：在经过大量超参数调优和学习率预热后，如果模型能够稳定收敛，Post-Norm 模型在许多任务上通常能达到比 Pre-Norm 更好的最终性能。这被认为是由于它在模型深层对残差连接进行了更直接的累加，保留了更多原始信息。
    * **更强的泛化能力**：由于其在训练后期的稳定性和对原始输入的直接累加，Post-Norm 模型有时能表现出更好的泛化能力。

* **缺点**：

    * **训练不稳定**：这是 Post-Norm 最显著的缺点，尤其是在深度很深的 Transformer 模型中。在训练初期，特别是在没有学习率预热的情况下，梯度很容易发生爆炸。这是因为残差连接会将每一层的输出累加到下一层，如果某一层输出的方差很大，这个误差会在深层网络中累积并放大，导致不稳定的训练。
    * **依赖学习率预热**：为了解决训练不稳定的问题，Post-Norm 模型通常需要一个“学习率预热”阶段。这个过程从一个很小的学习率开始，逐渐增加到预设的最大值，从而让模型在训练初期缓慢地适应，避免梯度爆炸。这增加了训练的复杂性和超参数调优的工作量。

### 2. Pre-Norm (现代 Transformer 常用)

**Pre-Norm** 是一种改进的 Transformer 架构，它将层归一化放置在每个子层**之前**。这种设计旨在解决 Post-Norm 的训练不稳定性问题，使得训练深度网络更加容易。

* **结构详解**：

    在 Pre-Norm 架构中，一个子层（Sublayer）的计算流程如下：
    1. 接收输入向量 $x$。
    2. 首先对 $x$ 进行层归一化，得到 $\text{LayerNorm}(x)$。
    3. 将归一化后的向量传入子层，得到子层输出 $\text{Sublayer}(\text{LayerNorm}(x))$。
    4. 将子层输出与原始输入 $x$ 相加（残差连接），得到最终的输出 $x_{out}$。

    * **公式表示**：
        $$
        x_{out} = x + \text{Sublayer}(\text{LayerNorm}(x))
        $$

* **优点**：

    * **极高的训练稳定性**：这是 Pre-Norm 最大的优势。由于每个子层都接收归一化后的输入，其输入数据的方差被有效控制，因此子层的输出方差也得到了有效限制。这阻止了梯度在深层网络中的累积和爆炸，使得训练过程非常稳定。
    * **无需学习率预热**：由于训练过程天生稳定，Pre-Norm 模型通常不需要复杂的学习率预热策略。可以直接使用较大的学习率进行训练，这简化了超参数调优过程，也可能加快训练收敛。
    * **可以训练更深的网络**：Pre-Norm 架构的稳定性使得训练包含数百甚至数千层的超深度 Transformer 成为可能，这在 Post-Norm 中是难以实现的。

* **缺点**：

    * **最终性能可能略逊于 Post-Norm**：在某些情况下，尤其是在小规模模型上，Pre-Norm 模型的最终性能可能略低于经过精心调优的 Post-Norm 模型。这通常被看作是一种“稳定 vs. 性能”的权衡。

### 总结与对比

| 特性 | Post-Norm (原始 Transformer) | Pre-Norm (现代常用) |
|---|---|---|
| **归一化位置** | 子层之后，残差连接之后 | 子层之前 |
| **训练稳定性** | 差，容易出现梯度爆炸，特别是深层网络 | 优秀，非常稳定，适合训练深度网络 |
| **学习率预热** | 通常需要，是保证训练成功的关键 | 通常不需要，简化了训练流程 |
| **最终性能** | 在特定条件下可能更高，但需要精细调参 | 稳定可靠，但在某些情况下可能略低 |
| **训练收敛速度** | 初始慢，需要预热 | 初始快，训练过程更平滑 |
| **代表模型** | 原始论文的 Transformer | 许多现代大型语言模型，如 GPT-2/3、BERT 的变体 |

### 为什么 Pre-Norm 更稳定？

简单来说，Pre-Norm 的稳定性的根本原因在于它**控制了网络中的信号流**。

在 Post-Norm 中，每一层的输出都是 $x_{out} = \text{LayerNorm}(x + \text{Sublayer}(x))$。由于 $\text{Sublayer}(x)$ 的方差可能很大，并且它被直接加到 $x$ 上，这个不稳定的信号会传递到下一层。即使之后进行了归一化，这种累积效应在深层网络中仍然难以控制，导致梯度爆炸。

而在 Pre-Norm 中，每一层的输入 $\text{Sublayer}$ 都被强制归一化，即 $\text{Sublayer}(\text{LayerNorm}(x))$。这意味着每一层的输出方差被有效抑制在一个可控的范围内。残差连接的累加虽然仍在进行，但由于每一步的累加值方差都很小，整个网络的信号流变得非常稳定，从而避免了梯度爆炸。


---

## Layer Normalization (LayerNorm) vs. RMS Normalization (RMSNorm)

LayerNorm 和 RMSNorm 都是用于稳定 Transformer 模型训练的归一化技术，它们都对每个训练样本独立进行归一化，与 Batch Normalization 不同。它们的主要区别在于如何计算归一化的统计量。

### 1. Layer Normalization (LayerNorm)

LayerNorm 是在 Transformer 模型中最早被广泛使用的归一化方法，它的目标是使每个特征向量的激活值具有**零均值（zero mean）和单位方差（unit variance）**。

* **计算步骤**：
    对于一个输入向量 $x \in \mathbb{R}^D$（其中 $D$ 是特征维度），LayerNorm 的计算分为以下几步：
    1. **计算均值**：对向量 $x$ 的所有元素求平均值 $\mu$。
    $$
    \mu = \frac{1}{D} \sum_{i=1}^{D} x_i
    $$
    2. **计算方差**：对向量 $x$ 的所有元素，计算其与均值 $\mu$ 的差的平方的平均值 $\sigma^2$。
    $$
    \sigma^2 = \frac{1}{D} \sum_{i=1}^{D} (x_i - \mu)^2
    $$
    3. **归一化**：使用均值和方差对输入向量进行归一化。为了防止除以零，会添加一个很小的常数 $\epsilon$。
    $$
    \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
    $$
    4. **仿射变换 (Affine Transformation)**：归一化后，模型使用两个可学习的参数 $\gamma$（缩放因子）和 $\beta$（偏移量）对结果进行线性变换。
    $$
    y_i = \gamma \hat{x}_i + \beta
    $$

* **核心思想**：LayerNorm 旨在通过**均值居中（mean-centering）**和**方差缩放（variance-scaling）**来稳定激活值的分布。这使得模型训练更加稳定，并有助于解决梯度消失和爆炸问题。

### 2. RMS Normalization (RMSNorm)

RMSNorm 是 LayerNorm 的一个简化版本，它在 2019 年的论文 "Root Mean Square Layer Normalization" 中被提出。RMSNorm 的核心假设是：在许多情况下，均值居中操作（即减去均值 $\mu$）并不是必需的，只保留方差缩放就足够了。

* **计算步骤**：
    对于一个输入向量 $x \in \mathbb{R}^D$（其中 $D$ 是特征维度），RMSNorm 的计算步骤如下：
    1. **计算均方根 (Root Mean Square, RMS)**：计算向量 $x$ 的所有元素的平方的平均值的平方根。
    $$
    \text{RMS}(x) = \sqrt{\frac{1}{D} \sum_{i=1}^{D} x_i^2}
    $$
    2. **归一化**：直接使用 RMS 对输入向量进行归一化。
    $$
    \hat{x}_i = \frac{x_i}{\text{RMS}(x) + \epsilon}
    $$
    3. **仿射变换 (Affine Transformation)**：与 LayerNorm 类似，RMSNorm 也使用可学习的缩放因子 $\gamma$。但通常情况下，它**不使用偏移量 $\beta$**。
    $$
    y_i = \gamma \hat{x}_i
    $$

* **核心思想**：RMSNorm 假设**方差缩放**是归一化的关键，而**均值居中**是多余的。它通过移除均值计算步骤来简化 LayerNorm。

### 3. LayerNorm 与 RMSNorm 的详细对比

| 特性 | Layer Normalization (LayerNorm) | RMS Normalization (RMSNorm) |
|---|---|---|
| **归一化统计量** | **均值**和**标准差** | **均方根 (RMS)** |
| **计算公式** | $y = \gamma \frac{x - \mu}{\sigma} + \beta$ | $y = \gamma \frac{x}{\text{RMS}(x)}$ |
| **均值居中** | **是**（通过减去 $\mu$） | **否** |
| **可学习参数** | $\gamma$ (缩放) 和 $\beta$ (偏移) | $\gamma$ (缩放) |
| **计算效率** | 较低 | 较高（因为它省去了均值计算） |
| **训练稳定性** | 非常好 | 同样非常好，甚至在某些情况下表现更好 |
| **模型性能** | 强大，是原始 Transformer 的基石 | 强大，在许多现代大型语言模型（如 LLaMA, GPT-NeoX 等）中广泛应用，性能与 LayerNorm 相当或略优 |

### 4. 为什么 RMSNorm 越来越受欢迎？

* **计算效率**：RMSNorm 的计算比 LayerNorm 更快。在 LayerNorm 中，需要计算均值和方差，而在 RMSNorm 中，只需要计算均方根。虽然这看起来只是一个微小的区别，但在现代大型模型中，归一化层会占据相当一部分计算开销，尤其是在内存带宽受限的情况下，RMSNorm 的计算简化能带来明显的加速。

* **性能相当或更好**：研究表明，移除均值居中操作并没有显著影响模型的性能，甚至在一些情况下，RMSNorm 的表现更好。这表明在大多数深度学习任务中，**方差缩放**才是归一化的关键，而均值居中的作用相对较小。

* **简化模型设计**：RMSNorm 的设计更简单，没有偏移量参数 $\beta$（尽管一些实现会保留它）。这减少了模型的参数量，并在某些情况下能够让模型更高效。

### 结论

LayerNorm 和 RMSNorm 都是优秀的归一化技术，用于稳定 Transformer 模型的训练。LayerNorm 是原始 Transformer 的选择，它通过均值居中和方差缩放来规范激活值。而 RMSNorm 是 LayerNorm 的一个简化版本，它去除了均值居中，只保留了方差缩放。

在实践中，由于 RMSNorm 具有更高的计算效率，并且在大多数情况下能够提供与 LayerNorm 相当甚至更好的性能，它在许多最新的大型语言模型中越来越受欢迎，成为一种替代 LayerNorm 的流行选择。


---

## 激活函数：ReLU, GeLU, Swish 详解

在深度学习中，激活函数是为神经网络引入非线性的关键组件。不同的激活函数有不同的特性，对模型的训练稳定性和性能有着显著影响。这里我们将详细解释并比较 ReLU、GeLU 和 Swish。

### 1. ReLU (Rectified Linear Unit)

ReLU 是最常用的一种激活函数，尤其是在深度学习的早期和许多计算机视觉模型中。

* **公式**：
  $$
  f(x) = \max(0, x)
  $$

* **工作原理**：
  当输入 $x$ 大于 0 时，输出就是 $x$；当输入 $x$ 小于或等于 0 时，输出就是 0。

* **优点**：计算效率高，缓解梯度消失，产生稀疏性。
* **缺点**：死亡 ReLU 问题。

* **数值例子**：
  假设输入值为 $x = [-2.0, -0.5, 0.0, 0.5, 2.0]$
  * 对于 $x = -2.0$，ReLU($-2.0$) = $\max(0, -2.0) = 0$
  * 对于 $x = -0.5$，ReLU($-0.5$) = $\max(0, -0.5) = 0$
  * 对于 $x = 0.0$，ReLU($0.0$) = $\max(0, 0.0) = 0$
  * 对于 $x = 0.5$，ReLU($0.5$) = $\max(0, 0.5) = 0.5$
  * 对于 $x = 2.0$，ReLU($2.0$) = $\max(0, 2.0) = 2.0$
  
  因此，ReLU 的输出为 $[0.0, 0.0, 0.0, 0.5, 2.0]$。

### 2. GeLU (Gaussian Error Linear Unit)

GeLU 在许多现代大型语言模型（如 BERT、GPT-2、GPT-3）中得到了广泛应用。

* **公式**：
  $$
  GeLU(x) = x \cdot \Phi(x)
  $$
  其中，$\Phi(x)$ 是标准正态分布的累积分布函数（CDF）。

* **工作原理**：
  GeLU 并非简单地将负值置为零，而是将输入值乘以其服从标准正态分布的概率。在 $x$ 接近 0 的区域，GeLU 的曲线是平滑而非 ReLU 的尖锐转折。
  
  这里的 Φ(x) 就是标准正态分布的累积分布函数。GeLU 的设计思想是，一个神经元的输出 x 应该以与它自身值成正比的概率被“激活”。
	当 x 很大时，Φ(x) 接近 1，GeLU(x) 接近 x。
	
	当 x 接近 0 时，Φ(x) 接近 0.5，GeLU(x) 接近 0。
	
	当 x 很小时，Φ(x) 接近 0，GeLU(x) 也接近 0

* **优点**：平滑的非线性，更好的性能。
* **缺点**：计算略复杂。

* **数值例子**：
  假设输入值为 $x = [-2.0, -0.5, 0.0, 0.5, 2.0]$。我们使用标准正态分布的 CDF 值来计算：
  * 对于 $x = -2.0$，$\Phi(-2.0) \approx 0.023$。
    $GeLU(-2.0) \approx -2.0 \cdot 0.023 = -0.046$
  * 对于 $x = -0.5$，$\Phi(-0.5) \approx 0.309$。
    $GeLU(-0.5) \approx -0.5 \cdot 0.309 = -0.155$
  * 对于 $x = 0.0$，$\Phi(0.0) = 0.5$。
    $GeLU(0.0) = 0.0 \cdot 0.5 = 0.0$
  * 对于 $x = 0.5$，$\Phi(0.5) \approx 0.691$。
    $GeLU(0.5) \approx 0.5 \cdot 0.691 = 0.346$
  * 对于 $x = 2.0$，$\Phi(2.0) \approx 0.977$。
    $GeLU(2.0) \approx 2.0 \cdot 0.977 = 1.954$
  
  因此，GeLU 的输出近似为 $[-0.046, -0.155, 0.0, 0.346, 1.954]$。

### 3. Swish (SiLU)

Swish 在许多视觉任务和现代模型中显示出比 ReLU 更好的性能。它也被称为 SiLU。

* **公式**：
  $$
  Swish(x) = x \cdot \sigma(x)
  $$
  其中，$\sigma(x) = \frac{1}{1 + e^{-x}}$ 是 Sigmoid 函数。

* **工作原理**：
  Swish 通过 Sigmoid 函数对输入 $x$ 进行“自门控”。它在负值区域有一条平滑的“凹槽”，而不是完全为零，这有助于缓解 ReLU 的“死亡神经元”问题。

* **优点**：平滑的非线性，非单调性，性能好。
* **缺点**：计算略复杂。

* **数值例子**：
  假设输入值为 $x = [-2.0, -0.5, 0.0, 0.5, 2.0]$。我们使用 Sigmoid 函数来计算：
  * 对于 $x = -2.0$，$\sigma(-2.0) \approx 0.119$。
    $Swish(-2.0) \approx -2.0 \cdot 0.119 = -0.238$
  * 对于 $x = -0.5$，$\sigma(-0.5) \approx 0.378$。
    $Swish(-0.5) \approx -0.5 \cdot 0.378 = -0.189$
  * 对于 $x = 0.0$，$\sigma(0.0) = 0.5$。
    $Swish(0.0) = 0.0 \cdot 0.5 = 0.0$
  * 对于 $x = 0.5$，$\sigma(0.5) \approx 0.622$。
    $Swish(0.5) \approx 0.5 \cdot 0.622 = 0.311$
  * 对于 $x = 2.0$，$\sigma(2.0) \approx 0.881$。
    $Swish(2.0) \approx 2.0 \cdot 0.881 = 1.762$
  
  因此，Swish 的输出近似为 $[-0.238, -0.189, 0.0, 0.311, 1.762]$。


---

## 正弦-余弦位置编码 (Sinusoidal Positional Embedding) vs. 旋转位置编码 (Rotary Positional Embedding, RoPE)

这两种方法都使用正弦和余弦函数来编码词语的位置信息，但它们在根本上是不同的。
正弦-余弦编码是一种**绝对的、相加的**方法，而 RoPE 是一种**相对的、相乘的**方法。

### 正弦-余弦位置编码 (Sinusoidal Positional Embedding)

这是 2017 年原始 Transformer 论文《Attention Is All You Need》中提出的位置编码方法。它是一种**绝对**编码方法，因为它为序列中的每个位置分配一个独特的、固定的向量。它也是**相加**的，因为这个位置向量被直接加到词嵌入向量上。

* **工作原理**：
    它使用不同频率的正弦和余弦函数为序列中的每个位置生成一个独特的向量。这些频率呈几何级数递减，因此向量中的某些维度变化很快（捕捉微观位置），而另一些维度变化很慢（捕捉宏观位置）。对于位置 $pos$ 和维度索引 $i$ 的词，其位置编码向量由以下公式计算：

    $$
    PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})
    $$

    $$
    PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})
    $$

* **优点**：无需额外参数，理论上的长度外推性。
* **缺点**：编码的是绝对位置，对长序列性能下降。

* **例子**：

    假设我们有一个句子 "I am a student."，模型的词嵌入维度 $d_{model}=4$。我们来计算每个词的位置编码。
    
    * **位置 0 (I)**：
        $PE_{(0, 0)} = \sin(0 / 10000^{0/4}) = \sin(0) = 0$
        $PE_{(0, 1)} = \cos(0 / 10000^{0/4}) = \cos(0) = 1$
        $PE_{(0, 2)} = \sin(0 / 10000^{2/4}) = \sin(0) = 0$
        $PE_{(0, 3)} = \cos(0 / 10000^{2/4}) = \cos(0) = 1$
        其位置编码向量为：$[0, 1, 0, 1]$
    
    * **位置 1 (am)**：
        $PE_{(1, 0)} = \sin(1 / 10000^{0/4}) = \sin(1)$
        $PE_{(1, 1)} = \cos(1 / 10000^{0/4}) = \cos(1)$
        $PE_{(1, 2)} = \sin(1 / 10000^{2/4}) = \sin(1/100)$
        $PE_{(1, 3)} = \cos(1 / 10000^{2/4}) = \cos(1/100)$
        其位置编码向量为：$[\sin(1), \cos(1), \sin(0.01), \cos(0.01)]$
    
    * **位置 2 (a)**：
        $PE_{(2, 0)} = \sin(2 / 10000^{0/4}) = \sin(2)$
        ...以此类推...
    
    最终，每个词的词嵌入向量（如 "I" 的嵌入 $E_I$）都会加上其相应的位置编码向量（$E_I + PE_0$），作为 Transformer 的输入。

---

### 旋转位置编码 (Rotary Positional Embedding, RoPE)

RoPE 是一种更先进、更复杂的方法，被许多现代大型语言模型（如 Llama、GPT-NeoX 等）所采用。它是一种**相对**编码方法，因为它关注的是**词与词之间的距离**，而不是它们的绝对位置。它也是**相乘**的，因为它通过旋转的方式来修改词向量。

* **工作原理**：
    RoPE 不会创建单独的位置向量。相反，它在自注意力计算过程中，对**查询（Q）**和**键（K）**向量应用一种**旋转**操作。旋转的角度取决于词在序列中的位置。这种旋转经过精心设计，使得两个旋转后的向量点积只取决于它们的**相对距离**。
    
    核心思想：将向量 $q$ 和 $k$ 看作是复数向量，对它们进行旋转。一个向量 $q$ 在位置 $m$ 处的旋转操作可以表示为：$q_m = q \odot R_m$，其中 $R_m$ 是旋转矩阵。
    
    两个位置 $m$ 和 $n$ 的向量 $q_m$ 和 $k_n$ 的点积可以推导出只与它们的相对距离 $m-n$ 相关：
    $$
    <q_m, k_n> = <q \odot R_m, k \odot R_n> = <q, k \odot R_{n-m}>
    $$
    这使得模型能够天然地理解相对位置信息。

* **优点**：直接编码相对位置，优秀的外推性，与注意力机制深度融合。
* **缺点**：实现更复杂。

#### 位置编码角度计算公式

位置编码的角度，即 `sin` 和 `cos` 函数的参数，由以下公式计算：

$$
\text{angle} = \frac{pos}{10000^{2i/d_{\text{model}}}}
$$

其中：

* $pos$ 是词语在序列中的**绝对位置**（例如，0, 1, 2, ...）。
* $i$ 是词嵌入向量的维度索引，从 0 到 $d_{\text{model}}/2 - 1$。
* $d_{\text{model}}$ 是词嵌入向量的总维度。


**例子**：

    假设我们有一个 2 维的查询向量 $q = [q_0, q_1]$，它在位置 $m$。RoPE 会对其应用旋转。
    
    $$
    q_m = \begin{bmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{bmatrix} \begin{bmatrix} q_0 \\ q_1 \end{bmatrix} = \begin{bmatrix} q_0 \cos(m\theta) - q_1 \sin(m\theta) \\ q_0 \sin(m\theta) + q_1 \cos(m\theta) \end{bmatrix}
    $$
    
    同样地，键向量 $k$ 在位置 $n$ 处被旋转为 $k_n$。然后，在计算注意力分数时，我们计算旋转后的向量的点积 $<q_m, k_n>$。这个点积的值只取决于 $q$ 和 $k$ 的原始值以及它们的**相对距离** $m-n$。
    
    例如，对于句子 "the cat sat on the mat"，RoPE 会让模型在计算 "cat" 和 "mat" 的注意力分数时，只关注它们之间相隔的 4 个位置，而不会关心它们是在句子开头还是结尾。

### 总结

| 特性 | 正弦-余弦位置编码 | 旋转位置编码 (RoPE) |
| :--- | :--- | :--- |
| **方法** | 绝对，相加 | 相对，相乘 |
| **应用方式** | 一个固定的向量**加**到词嵌入上。 | 在注意力计算中，对查询和键向量应用**旋转**。 |
| **关注点** | 词的绝对位置。 | 词与词之间的相对距离。 |
| **实现位置** | 嵌入层的一部分。 | 注意力计算的一部分。 |
| **外推性** | 可以处理长序列，但性能可能下降。 | 专为长序列设计，性能稳定。 |
| **应用模型** | 原始 Transformer (2017)。 | 现代大语言模型（Llama, GPT-NeoX 等）。 |


---

## Hyperparameters

### 前馈网络（Feed Forward Network, FFN）详解

前馈网络是 Transformer 编码器和解码器中的一个关键组件，位于多头注意力机制之后。你可以将它理解为一个非常简单的、作用于每个词语的独立处理器。

#### 1. 前馈网络的作用

前馈网络的主要任务是**对每个位置的输出进行独立的、非线性的变换**。

* **逐位置处理（Position-wise）**：前馈网络对序列中的每个词语向量都进行相同的计算，它们之间没有任何交互。这与多头注意力机制形成了鲜明对比，后者会关注序列中所有词之间的关系。
* **引入非线性**：通过前馈网络中的非线性激活函数（例如 **ReLU** 或 **GeLU**），模型能够学习和表达更复杂的特征。它相当于对多头注意力层捕捉到的全局上下文信息进行了一次“深度”加工和精炼。

#### 2. 前馈网络的结构

前馈网络通常由两层全连接层（也叫线性层）和一个激活函数组成。它的结构可以被描述为：**扩展 -> 激活 -> 压缩**。

1.  **第一个线性层**：将输入向量的维度从模型的维度 $d_{model}$ **扩展**到一个更大的维度 $d_{ff}$。
2.  **激活函数**：在扩展后的向量上应用非线性激活函数。
3.  **第二个线性层**：将向量的维度从 $d_{ff}$ **压缩**回原始的 $d_{model}$。

#### 3. 公式和例子

前馈网络的计算可以用以下公式表示：

$$
FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2
$$

* $x$：输入向量，维度为 $d_{model}$。
* $W_1$ 和 $b_1$：第一个线性层的权重和偏置，将维度从 $d_{model}$ 扩展到 $d_{ff}$。
* $\max(0, ...)$：这里使用的是 ReLU 激活函数。
* $W_2$ 和 $b_2$：第二个线性层的权重和偏置，将维度从 $d_{ff}$ 压缩回 $d_{model}$。

**举个例子**：

假设你的模型维度 $d_{model}=512$，前馈网络维度 $d_{ff}=2048$（$d_{ff} = 4 \times d_{model}$）。

一个输入词向量 $x$（维度 512）会经历以下过程：
1.  **扩展**：$x$ 与一个 $(512, 2048)$ 的权重矩阵相乘，得到一个 2048 维的向量。
2.  **激活**：对这个 2048 维的向量应用 ReLU 函数，将所有负值变为 0。
3.  **压缩**：激活后的向量与一个 $(2048, 512)$ 的权重矩阵相乘，最终得到一个 512 维的输出向量。

最终，前馈网络将一个 $d_{model}$ 维的向量作为输入，也输出一个 $d_{model}$ 维的向量，确保了维度的一致性，方便与后续的残差连接和 Layer Normalization 协同工作。


---
### 模型维度 ($d_{model}$), 头数 ($h$), 头维度 ($d_{head}$) 详解

在 Transformer 模型中，尤其是在多头注意力（Multi-Head Attention）机制里，这三个超参数是紧密相连的，它们共同决定了注意力层的配置和运作方式。

#### 1. 模型维度（Model Dimension, $d_{model}$）

**模型维度**，或 $d_{model}$，是 Transformer 中最基础、最重要的超参数。它定义了模型中所有主要向量的统一尺寸。你可以把它想象成模型内部信息流动的“主干道”的宽度。

* **定义**：单个词语嵌入向量的维度。
* **作用**：序列中的每个词语都被表示成一个 $d_{model}$ 维的向量。模型中的每个子层（多头注意力和前馈网络）的输入和输出都必须是这个维度，以确保残差连接（residual connections）能够正常工作。
* **例子**：如果 $d_{model} = 512$，那么每个词语都由一个 512 维的向量表示。

#### 2. 头数（Number of Heads, $h$）

**头数**，或 $h$，是多头注意力机制特有的超参数。它决定了有多少个平行的“注意力”计算会同时进行。

* **定义**：在多头注意力层中，并行运行的独立注意力机制的数量。
* **作用**：每个头都能够让模型在不同的“表示子空间”中关注输入序列的不同部分。例如，一个头可能专注于捕捉语法关系，而另一个头则可能专注于捕捉语义关系。通过并行运行多个头，模型可以从多个角度和维度来理解序列中的信息。
* **例子**：如果 $h = 8$，那么多头注意力层会同时执行 8 个独立的注意力计算。

#### 3. 头维度（Head Dimension, $d_{head}$）

**头维度**，或 $d_{head}$，是**单个注意力头**内部的查询（Query）、键（Key）和值（Value）向量的维度。这个值与模型维度和头数之间存在一个至关重要的关系。

* **定义**：单个注意力头中，Q、K、V 向量的维度。
* **作用**：所有头加起来的总维度必须等于模型的总维度。为了实现这一点，头维度通常通过将模型维度除以头数来计算。
* **公式**：$d_{head} = d_{model} / h$

#### 总结

这三个参数是紧密相连的，它们共同定义了多头注意力层的结构。

* 你首先确定模型的主干道宽度：**$d_{model}$**。
* 然后，你决定要并行运行多少个注意力计算：**$h$**。
* 这个决定自然地确定了每个注意力头内部的向量大小：**$d_{head}$**。

**一个具体的例子**：

假设你的模型维度 **$d_{model}=512$**，你选择使用 **$h=8$** 个头。那么：
1.  每个头的维度 **$d_{head}$** 将是 $512 / 8 = 64$。
2.  多头注意力层会接收 512 维的输入向量。
3.  它会将这些向量拆分成 8 个平行的头，每个头都使用更小的 64 维 Q、K、V 向量进行计算。
4.  计算完成后，8 个头的输出（每个 64 维）会被拼接（concatenate）起来，形成一个 512 维的向量。
5.  这个拼接后的向量会通过一个线性层，将其维度投影回 $d_{model}$，最终作为注意力层的输出，用于残差连接。

这三者之间的关系是多头注意力机制能同时从多个子空间捕捉信息的关键所在。

---

### Dropout（丢弃法）详解

**Dropout**（**丢弃法**）是一种在深度学习中广泛使用的**正则化（Regularization）**技术，用于**防止过拟合（Overfitting）**。它的核心思想非常简单：在训练过程中，以一定的概率随机地“丢弃”或“关闭”一些神经元，让它们暂时不参与前向传播和反向传播。

这个过程就像是每次训练都使用一个“瘦身版”的神经网络，而不是完整的网络。

#### 为什么要用 Dropout？

在没有 Dropout 的情况下，神经网络中的每个神经元都可能过度依赖其他特定的神经元来做出预测。这会导致网络学习到一些过于复杂和脆弱的模式，只在训练数据上表现良好，但在未见过的新数据上泛化能力很差，这就是**过拟合**。

Dropout 通过以下方式解决这个问题：
1.  **防止共适应（Co-adaptation）**：由于每次训练都有神经元被随机丢弃，一个神经元无法再依赖另一个特定的神经元。这迫使每个神经元学习到更鲁棒、更独立的特征，以更好地适应各种子网络结构。
2.  **相当于集成学习**：每次训练迭代，我们实际上都在一个不同的神经网络上进行训练。在一个 epoch 结束后，整个网络可以被看作是无数个“瘦身版”子网络的集成。当进行推理时，我们使用完整的网络，这类似于对所有这些子网络的预测结果取平均，从而大大提高了模型的泛化能力。

#### Dropout 的工作流程

Dropout 只在**训练阶段**使用，在**推理（Inference）阶段**则会被关闭。

##### 1. 训练阶段 (Training)

* **随机丢弃**：在每次前向传播时，对于隐藏层中的每个神经元，以一个预先设定的概率 $p$（比如 0.5），将其输出设置为 0。
* **缩放（Scaling）**：由于部分神经元被关闭，下一层接收到的输入总和会变小。为了保持输出的期望值不变，Dropout 会对未被丢弃的神经元的输出进行缩放，通常是乘以 $1/(1-p)$。例如，如果 $p=0.5$，未被丢弃的神经元的输出就会乘以 2。

**例子**：
假设一个隐藏层有 100 个神经元，Dropout 概率 $p=0.5$。
在一次训练迭代中，我们随机丢弃 50% 的神经元。剩下的 50 个神经元会继续工作，但它们的输出值都会乘以 2，以补偿被丢弃的神经元，从而保证这一层输出的期望值与没有使用 Dropout 时相同。

##### 2. 推理阶段 (Inference)

* **关闭 Dropout**：在测试或实际应用时，所有神经元都会被保留，Dropout 不再生效。
* **不进行缩放**：由于在训练时已经对激活值进行了缩放，因此在推理时无需进行任何额外的缩放操作。这简化了推理过程，因为所有神经元的输出都保持了预期的值。
#### 总结

| 特性 | 训练阶段 | 推理阶段 |
| :--- | :--- | :--- |
| **神经元** | 以概率 $p$ 随机丢弃 | 全部保留 |
| **缩放** | 对未丢弃神经元输出乘以 $1/(1-p)$ | 不进行任何缩放 |
| **目的** | 防止过拟合，增强泛化能力 | 使用完整的网络，得到稳定、可靠的预测 |

Dropout 是一种简单而有效的正则化方法，通过随机丢弃神经元来防止模型过度依赖特定特征，从而提高模型的泛化能力。

---

### Weight Decay（权重衰减）详解

**Weight Decay** 是一种最古老、最有效的**正则化（Regularization）**技术之一，它的主要目的是**防止模型过拟合**。

它的核心思想很简单：**在模型的损失函数中增加一个惩罚项，用于惩罚那些过大的权重**。这个惩罚项会鼓励模型在训练过程中保持权重尽可能小。

为什么小权重是好的？因为一个拥有较小权重的模型通常意味着它更简单、更平滑。一个平滑的函数对输入的微小变化不那么敏感，因此它的泛化能力更强，不容易在训练数据上过拟合。

#### 1. Weight Decay 的工作原理

Weight Decay 的实现方式是在**损失函数**中添加一个正则化项。这个正则化项通常是所有权重平方和的一半，乘以一个超参数 $\lambda$（Lambda，权重衰减系数）。

**添加了权重衰减后的损失函数**：
$$
L(\theta) = L_0(\theta) + \frac{1}{2}\lambda\sum_{w \in W} w^2
$$

其中：
* $L_0(\theta)$ 是你的原始损失函数，比如交叉熵损失。
* $\frac{1}{2}\lambda\sum_{w \in W} w^2$ 是权重衰减项，也称为 **L2 正则化**。
* $\lambda$ 是**权重衰减系数**，一个超参数，用于控制惩罚的强度。$\lambda$ 越大，对大权重的惩罚就越强。
* $w$ 是模型中的每一个权重参数。

在训练过程中，优化器的目标是最小化 $L(\theta)$。通过对这个新的损失函数进行梯度下降，权重的更新规则会变成：

$$
w_{\text{new}} = (1 - \eta\lambda)w_{\text{old}} - \eta \frac{\partial L_0}{\partial w}
$$

其中，$\eta$ 是学习率。这个公式清楚地揭示了“权重衰减”的含义：在每次更新时，权重会乘以一个小于 1 的因子 $(1 - \eta\lambda)$，从而**自动衰减**（Decay）它的值。

#### 2. 举一个例子

假设我们有一个非常简单的模型，只有一个权重 $w=1.0$。
* 学习率 $\eta = 0.01$
* 权重衰减系数 $\lambda = 0.1$
* 此时，假设原始损失的梯度 $\frac{\partial L_0}{\partial w}$ 为 0.5。

我们来计算一次权重更新：
1. **计算总梯度**：
   $$
   \frac{\partial L}{\partial w} = \frac{\partial L_0}{\partial w} + \lambda w = 0.5 + 0.1 \cdot 1.0 = 0.6
   $$
2. **更新权重**：
   $$
   w_{\text{new}} = w_{\text{old}} - \eta \cdot 0.6 = 1.0 - 0.01 \cdot 0.6 = 1.0 - 0.006 = 0.994
   $$

可以看到，在这次更新中，权重从 1.0 变成了 0.994，它的值变小了。这种“衰减”作用在每一次训练迭代中都会发生，从而有效地控制了权重的大小，防止模型变得过于复杂。

#### 3. Weight Decay 和 L2 正则化的关系

在传统的 SGD 优化器中，**Weight Decay** 和 **L2 正则化**在数学上是等价的。但在现代优化器（如 Adam）中，它们被**解耦**了。**AdamW** 优化器就是专门为解耦的权重衰减而设计的，它能够更有效地将权重衰减应用到优化过程中，这也是它在大型模型训练中如此受欢迎的原因。

总而言之，Weight Decay 是一种通过惩罚大权重来增强模型泛化能力的强大技术，它通过在每次迭代中“衰减”权重，鼓励模型保持简洁。


---

### Weight Decay vs. Dropout 对比

Weight Decay 和 Dropout 都是常用的正则化技术，用于防止模型过拟合。但它们的工作原理和适用场景有所不同。

| 特性            | Weight Decay（权重衰减）                                                 | Dropout（丢弃法）                             |
| :------------ | :----------------------------------------------------------------- | :--------------------------------------- |
| **工作方式**      | 在损失函数中增加对大权重的惩罚。它鼓励模型使用更小的权重，从而保持简洁。                               | 随机地“关闭”一些神经元，迫使网络不依赖特定的神经元。              |
| **数学原理**      | 在梯度下降时，权重会乘以一个小于1的因子而自动“衰减”。这与 **L2 正则化**在数学上紧密相关。                 | 在训练时，以概率 $p$ 随机将神经元输出置为零，并在前向传播时进行缩放。    |
| **引入的训练噪声**   | **无**。这是一种确定性的方法，不会引入随机性。                                          | **高**。每次训练迭代都会有随机性，导致训练过程有波动。            |
| **训练稳定性**     | 训练过程通常更平稳，尤其适用于大型模型。                                               | 随机性可能导致训练不稳定，尤其是在与某些归一化层结合时。             |
| **与归一化层的兼容性** | **较好**。它不会干扰 Batch Normalization 或 Layer Normalization 等归一化层的统计计算。 | **较差**。随机性会改变神经元输出的分布，可能干扰归一化层的稳定性。      |
| **应用阶段**      | 在**训练和优化**的所有步骤中都生效。                                               | **只在训练阶段**使用，推理阶段关闭。                     |
| **主要优点**      | 训练稳定，与现代优化器（如 AdamW）和归一化层兼容性好。                                     | 阻止神经元之间的共适应（co-adaptation），相当于训练了一个集成模型。 |
| **典型应用场景**    | 现代大型语言模型（如 Transformer）、与 AdamW 优化器结合。                             | 早期深度学习模型、计算机视觉任务（CNN）。                   |


---

### Softmax 在 Transformer 中的应用与改进

#### 1. 输出层 Softmax 的改进

**输出层 Softmax** 的主要任务是将模型的原始预测（logits）转换为一个概率分布，用于多分类任务，例如预测下一个词。它面临的主要挑战是词汇量巨大（通常有数万到数十万个词），导致计算量非常大。

虽然 Softmax 函数本身的核心工作原理没有被“改进”，但人们在**计算效率**和**训练策略**上做出了重大改进。

1.  **分层 Softmax（Hierarchical Softmax）**
    * **改进点**：将巨大的词汇表构建成一棵树。在预测时，模型不是一次性计算所有词的概率，而是从根节点开始，沿着树的路径逐层计算概率。这大大减少了计算量，将时间复杂度从 $O(|V|)$ 降低到 $O(\log|V|)$，其中 $|V|$ 是词汇表大小。
    * **例子**：一个词 "cat" 的概率可能被分解为 $P(\text{animal}) \times P(\text{mammal|animal}) \times P(\text{cat|mammal})$。

2.  **采样 Softmax（Sampled Softmax）**
    * **改进点**：在训练阶段，不计算所有词的概率，而是只计算正确词的概率，以及从词汇表中随机抽样的一些负样本词的概率。
    * **例子**：像 **Noise Contrastive Estimation (NCE)** 或 **Negative Sampling** 这样的技术，通过将多分类问题转化为一系列二分类问题来加速训练。

3.  **自适应 Softmax（Adaptive Softmax）**
    * **改进点**：根据词语的频率将词汇表分成几个簇。频繁出现的词放在一个小的、高效的 Softmax 层中，而稀有词则放在一个或多个更大的 Softmax 层中。
    * **例子**：这使得模型可以在绝大多数情况下使用一个快速的 Softmax 层，只在需要时才处理更稀有的词，从而在不牺牲准确性的前提下大大提高效率。

**总结**：输出层 Softmax 的改进主要集中在如何**高效地处理大规模词汇表**，以减少计算量和内存占用。

---

#### 2. 自注意力 Softmax 的改进

**自注意力层的 Softmax** 用于将注意力得分（即 Q 和 K 向量的点积）归一化为一个概率分布，这些概率决定了模型在生成每个词时应该“关注”输入序列中的哪些词。这个 Softmax 的主要挑战在于**处理长序列**和**提高注意力机制的鲁棒性**。

1.  **使用不同的激活函数**
    * **改进点**：一些研究尝试用其他激活函数替换 Softmax。例如，**ReLU Softmax** 旨在通过简单地将所有负值置为零来创建稀疏的注意力图，这可以提高计算效率并防止注意力分数过度分散。

2.  **注意力机制的修改**
    * **改进点**：**Softmax 的计算与序列长度的平方成正比**，这在处理长序列时是一个瓶颈。为了解决这个问题，很多模型通过改变注意力机制本身来避免使用 Softmax，或者使用一个更高效的替代品。
    * **例子**：
        * **Performer** 模型使用随机特征映射来近似 Softmax 注意力，将计算复杂度从 $O(L^2)$ 降至 $O(L)$。
        * **Longformer** 使用稀疏注意力（Sparse Attention），即只计算部分而非所有词对的 Softmax，从而可以处理更长的序列。
        * **FlashAttention** 是一种优化 Softmax 计算的实现方法，它在硬件层面通过减少内存访问次数来大大加速计算，而不是改变 Softmax 的数学原理。

3.  **Softmax 的平滑（Temperature Scaling）**
    * **改进点**：这是一种简单的技巧，通过在 Softmax 之前将 logits 除以一个温度参数 $T$ 来调整分布的平滑度。较高的 $T$ 会使分布更平滑，而较低的 $T$ 会使其更尖锐。
    * **例子**：$\text{Softmax}(z/T)$。在训练时，这可以帮助模型更好地收敛；在推理时，这可以用来调整模型的自信度。

**总结**：**输出层 Softmax 的改进**主要是为了解决**大规模分类**问题带来的**计算效率**挑战。**自注意力层的 Softmax 的改进**则更侧重于解决**序列长度的平方级复杂性**问题，并探索替代方案以提高效率和模型的泛化能力。


---

### 多查询注意力 (MQA) 与 分组查询注意力 (GQA) 详解

在 Transformer 模型中，多头注意力（Multi-Head Attention, MHA）是核心组件。随着模型规模的增大，MHA 在推理时会产生大量的键（Key）和值（Value）缓存（KV Cache），导致内存占用高、推理速度慢。MQA 和 GQA 就是为了解决这个问题而提出的 MHA 变体。

#### 1. 多头注意力 (Multi-Head Attention, MHA)

在原始的 MHA 中，每个注意力头都有一组独立的查询（Q）、键（K）和值（V）矩阵。如果有 $H$ 个头，那么就有 $H$ 组 Q、K、V 矩阵。

* **特点**：每个头都可以独立地学习不同的信息，这使得模型具有很强的表达能力。
* **问题**：推理时需要为每个头存储独立的 K 和 V 向量，导致 KV 缓存巨大，占用大量内存。
#### 2. 多查询注意力 (Multi-Query Attention, MQA)

**MQA** 的核心思想是**让所有的查询头共享同一组键（K）和值（V）矩阵**。

* **工作原理**：
    * MQA 仍然使用多个**查询**（Query）头，就像 MHA 一样。
    * 但它只使用一个**键**（Key）头和一个**值**（Value）头，这一个 K 和 V 会被所有的 Q 头共享。
* **优点**：
    * **大幅减少 KV 缓存**：KV 缓存的大小与头数无关，大大节省了内存。
    * **加速推理**：减少了内存带宽的需求，因为每个头只需要从 KV 缓存中读取一次数据。
* **缺点**：
    * **性能下降**：所有头共享同一组 K 和 V，可能会限制模型的表达能力，因为它剥夺了每个头从不同角度提取信息的能力，可能导致性能略微下降。
#### 3. 分组查询注意力 (Grouped-Query Attention, GQA)

**GQA** 是 MQA 和 MHA 的一个**折中方案**，旨在在保持 MQA 的推理速度优势的同时，缓解其潜在的性能下降问题。

* **工作原理**：
    * GQA 将查询头分成多个**组**（Group）。
    * **每个组**共享同一组键（K）和值（V）矩阵，但不同的组之间使用独立的 K 和 V 矩阵。
    * 如果查询头数为 $H$，组数为 $G$，则 $1 < G < H$。当 $G=1$ 时，GQA 就退化成了 MQA；当 $G=H$ 时，GQA 就变回了 MHA。
* **优点**：
    * **平衡性能和速度**：GQA 允许在推理速度和模型性能之间进行权衡。你可以选择一个合适的组数 $G$ 来获得接近 MQA 的速度，同时保留一些 MHA 的多头能力，从而实现更好的性能。
    * **优于 MQA**：实验证明，GQA 的性能通常优于 MQA，因为它缓解了 MQA 中所有头共用一个 KV 矩阵导致的“信息瓶颈”问题。
#### 总结与比较

| 特性 | 多头注意力 (MHA) | 多查询注意力 (MQA) | 分组查询注意力 (GQA) |
| :--- | :--- | :--- | :--- |
| **KV 头数** | $H$（与查询头数相同） | 1 | $G$（$1 < G < H$） |
| **KV 缓存大小** | 最大（与 $H$ 成正比） | 最小（与 $G=1$ 成正比） | 介于 MHA 和 MQA 之间（与 $G$ 成正比） |
| **推理速度** | 较慢 | 最快 | 较快，优于 MHA |
| **模型性能** | 最好 | 可能略有下降 | 接近 MHA，优于 MQA |
| **适用场景** | 原始设计，通用性强 | 注重推理速度和内存优化的场景 | 平衡推理速度和性能的场景（如 Llama 2/3） |

GQA 是当前大型语言模型（如 Llama 2、Llama 3）的首选，因为它在**推理速度**和**模型性能**之间找到了一个绝佳的平衡点。