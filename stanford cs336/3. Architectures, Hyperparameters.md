# Transformers
## Original Transformer

![[original transformer.png]]

它主要分为三个部分：**编码器 (Encoder)**、**解码器 (Decoder)** 和模型右侧的**回顾部分 (Review)**

### 原始 Transformer 模型架构图

这个架构图是一个**编码器-解码器**结构。左边是编码器，右边是解码器。

#### 1. 输入和嵌入层 (Inputs and Embedding)

- **输入 (Inputs)**: 模型接收输入的词序列。这些可以是句子中的单词。
    
- **输入嵌入 (Input Embedding)**: 输入的每个单词被转换成一个高维的向量，这个向量包含了词的语义信息。
    
- **位置编码 (Positional Encoding)**: 这是 Transformer 的一个关键创新。由于 Transformer 不像 RNN 那样有固定的序列处理顺序，它需要一种方式来知道单词在句子中的位置信息。位置编码就是解决这个问题的。它是一个与输入嵌入相同维度的向量，被加到输入嵌入向量上，从而让模型能够区分不同位置的单词。图中的 "Positional Encoding" 模块就是做这件事的。
  位置编码的具体公式如下，其中 $pos$ 是词在序列中的位置，**$i$ 是词嵌入向量的维度索引**，$d_{model}$ 是模型的维度： $$ PE_{(pos, 2i)} = \sin(pos/10000^{2i/d_{model}}) $$ $$ PE_{(pos, 2i+1)} = \cos(pos/10000^{2i/d_{model}}) $$

##### 什么是位置编码
假设模型的维度 $d_{model}=4$。我们为位置 $pos=0$ 和 $pos=1$ 的词计算位置编码向量。 
*计算位置 $pos=0$ 的位置编码向量 $PE_0$**： $$ PE_0 = [PE_{(0,0)}, PE_{(0,1)}, PE_{(0,2)}, PE_{(0,3)}] $$ $$ PE_{(0, 0)} = \sin(0/10000^{0/4}) = \sin(0) = 0 $$ $$ PE_{(0, 1)} = \cos(0/10000^{0/4}) = \cos(0) = 1 $$ $$ PE_{(0, 2)} = \sin(0/10000^{2/4}) = \sin(0) = 0 $$ $$ PE_{(0, 3)} = \cos(0/10000^{2/4}) = \cos(0) = 1 $$ 因此，$PE_0 = [0, 1, 0, 1]$。 * **计算位置 $pos=1$ 的位置编码向量 $PE_1$**： $$ PE_1 = [PE_{(1,0)}, PE_{(1,1)}, PE_{(1,2)}, PE_{(1,3)}] $$ $$ PE_{(1, 0)} = \sin(1/10000^{0/4}) = \sin(1) \approx 0.841 $$ $$ PE_{(1, 1)} = \cos(1/10000^{0/4}) = \cos(1) \approx 0.540 $$ $$ PE_{(1, 2)} = \sin(1/10000^{2/4}) = \sin(1/100) \approx 0.010 $$ $$ PE_{(1, 3)} = \cos(1/10000^{2/4}) = \cos(1/100) \approx 0.999 $$ 因此，$PE_1 = [\sin(1), \cos(1), \sin(0.01), \cos(0.01)]$。 最终，这些位置编码向量会被加到相应的词嵌入向量上，作为 Transformer 模型的输入。

##### 为什么选择正弦和余弦？

这种基于正弦和余弦函数的位置编码有两个关键优点：

1. **能够表示相对位置：** 任意位置 pos+k 的位置编码可以表示成位置 pos 的位置编码的线性函数。这意味着模型可以很容易地学习到，某个词和它后面或前面一个词的相对位置关系。
    
    - 例如，sin(α+β)=sin(α)cos(β)+cos(α)sin(β)。通过三角函数的和角公式，我们可以用 PEpos​ 的分量来表示 PEpos+k​ 的分量。这为模型提供了捕获相对位置关系的便利。
        
2. **可以推广到任意长度的序列：** 由于公式不依赖于序列的固定长度，即使在训练时没有见过某个长度的序列，模型也能够为新位置生成位置编码。这使得模型可以处理比训练时更长的序列。

#### 2. 编码器 (Encoder)

编码器由一个堆叠的模块构成，图中用 "Nx" 表示，这意味着这个模块被重复了 N 次。在原始论文中，N=6。

每个编码器模块包含两个主要子层：

- **多头自注意力 (Multi-Head Attention)**: 这是 Transformer 的核心。它允许模型同时关注输入序列中的所有词，并根据它们的相对重要性为每个词分配权重。多头 (Multi-Head) 意味着模型不是只执行一次注意力计算，而是并行地执行多次（有不同的权重矩阵），然后将结果拼接起来，从而捕获不同类型的信息。图中的 "Multi-Head Attention" 模块就是这个。
    
- **前馈网络 (Feed Forward)**: 在注意力层之后，每个位置的输出都会独立地经过一个全连接的前馈网络。这个网络通常包含两个线性变换和一个激活函数（在原始论文中是 ReLU）。图中的 "Feed Forward" 模块就是这个。
  前馈网络的具体公式如下，其中 $W_1, b_1, W_2, b_2$ 是可学习的参数： $$ FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2 $$

在每个子层（注意力层和前馈网络）之后，都有一个 **"Add & Norm"** 模块。这代表了：

- **残差连接 (Residual Connection)**: 将子层的输入直接加到子层的输出上。这有助于解决深度网络中的梯度消失问题，使得训练更深的模型成为可能。
    
- **层归一化 (Layer Normalization)**: 对每个样本的特征进行归一化，使得训练更稳定。
    

#### 3. 解码器 (Decoder)

解码器也由一个堆叠的模块构成，同样用 "Nx" 表示（原始论文中也是 N=6）。

每个解码器模块包含三个主要子层：

- **掩码多头自注意力 (Masked Multi-Head Attention)**: 这个和编码器中的自注意力类似，但是多了一个**掩码 (Mask)** 操作。在训练解码器时，为了防止模型“偷看”未来的单词，它只能关注当前位置和之前的位置。掩码就是用来屏蔽掉未来位置的信息。
    
- **编码器-解码器多头注意力 (Encoder-Decoder Multi-Head Attention)**: 这是解码器的独有部分。它使得解码器可以关注编码器的输出，从而在生成每个输出词时，能够“回顾”输入句子中的所有相关信息。
    
- **前馈网络 (Feed Forward)**: 和编码器中的前馈网络相同。
    

和编码器一样，每个子层之后也都有一个 **"Add & Norm"** 模块。

#### 4. 输出层

解码器的堆栈之后是输出层，它将解码器的最终输出转换为词汇表中每个词的概率：

- **线性层 (Linear)**: 一个全连接层，将解码器最终的向量表示映射到词汇表的大小。
    
- **Softmax**: 将线性层的输出转换为概率分布，使得所有词的概率总和为1。我们选择概率最高的那个词作为当前步的输出。
    

---

## Pre norm VS post norm

### 1. Post-Norm (原始 Transformer)

**Post-Norm** 是原始论文 "Attention Is All You Need" 中使用的架构，它将层归一化（Layer Normalization）和残差连接（Residual Connection）的“Add & Norm”模块放置在每个子层（如多头注意力或前馈网络）之后。

* **结构详解**：

    在 Post-Norm 架构中，一个子层（Sublayer）的计算流程如下：
    1. 接收输入向量 $x$。
    2. 将 $x$ 传入子层，得到子层输出 $\text{Sublayer}(x)$。
    3. 将子层输出与原始输入 $x$ 相加（残差连接），得到 $x + \text{Sublayer}(x)$。
    4. 对这个和进行层归一化，得到最终的输出 $x_{out}$。

    * **公式表示**：
        $$
        x_{out} = \text{LayerNorm}(x + \text{Sublayer}(x))
        $$

* **优点**：

    * **更好的最终性能**：在经过大量超参数调优和学习率预热后，如果模型能够稳定收敛，Post-Norm 模型在许多任务上通常能达到比 Pre-Norm 更好的最终性能。这被认为是由于它在模型深层对残差连接进行了更直接的累加，保留了更多原始信息。
    * **更强的泛化能力**：由于其在训练后期的稳定性和对原始输入的直接累加，Post-Norm 模型有时能表现出更好的泛化能力。

* **缺点**：

    * **训练不稳定**：这是 Post-Norm 最显著的缺点，尤其是在深度很深的 Transformer 模型中。在训练初期，特别是在没有学习率预热的情况下，梯度很容易发生爆炸。这是因为残差连接会将每一层的输出累加到下一层，如果某一层输出的方差很大，这个误差会在深层网络中累积并放大，导致不稳定的训练。
    * **依赖学习率预热**：为了解决训练不稳定的问题，Post-Norm 模型通常需要一个“学习率预热”阶段。这个过程从一个很小的学习率开始，逐渐增加到预设的最大值，从而让模型在训练初期缓慢地适应，避免梯度爆炸。这增加了训练的复杂性和超参数调优的工作量。

### 2. Pre-Norm (现代 Transformer 常用)

**Pre-Norm** 是一种改进的 Transformer 架构，它将层归一化放置在每个子层**之前**。这种设计旨在解决 Post-Norm 的训练不稳定性问题，使得训练深度网络更加容易。

* **结构详解**：

    在 Pre-Norm 架构中，一个子层（Sublayer）的计算流程如下：
    1. 接收输入向量 $x$。
    2. 首先对 $x$ 进行层归一化，得到 $\text{LayerNorm}(x)$。
    3. 将归一化后的向量传入子层，得到子层输出 $\text{Sublayer}(\text{LayerNorm}(x))$。
    4. 将子层输出与原始输入 $x$ 相加（残差连接），得到最终的输出 $x_{out}$。

    * **公式表示**：
        $$
        x_{out} = x + \text{Sublayer}(\text{LayerNorm}(x))
        $$

* **优点**：

    * **极高的训练稳定性**：这是 Pre-Norm 最大的优势。由于每个子层都接收归一化后的输入，其输入数据的方差被有效控制，因此子层的输出方差也得到了有效限制。这阻止了梯度在深层网络中的累积和爆炸，使得训练过程非常稳定。
    * **无需学习率预热**：由于训练过程天生稳定，Pre-Norm 模型通常不需要复杂的学习率预热策略。可以直接使用较大的学习率进行训练，这简化了超参数调优过程，也可能加快训练收敛。
    * **可以训练更深的网络**：Pre-Norm 架构的稳定性使得训练包含数百甚至数千层的超深度 Transformer 成为可能，这在 Post-Norm 中是难以实现的。

* **缺点**：

    * **最终性能可能略逊于 Post-Norm**：在某些情况下，尤其是在小规模模型上，Pre-Norm 模型的最终性能可能略低于经过精心调优的 Post-Norm 模型。这通常被看作是一种“稳定 vs. 性能”的权衡。

### 总结与对比

| 特性 | Post-Norm (原始 Transformer) | Pre-Norm (现代常用) |
|---|---|---|
| **归一化位置** | 子层之后，残差连接之后 | 子层之前 |
| **训练稳定性** | 差，容易出现梯度爆炸，特别是深层网络 | 优秀，非常稳定，适合训练深度网络 |
| **学习率预热** | 通常需要，是保证训练成功的关键 | 通常不需要，简化了训练流程 |
| **最终性能** | 在特定条件下可能更高，但需要精细调参 | 稳定可靠，但在某些情况下可能略低 |
| **训练收敛速度** | 初始慢，需要预热 | 初始快，训练过程更平滑 |
| **代表模型** | 原始论文的 Transformer | 许多现代大型语言模型，如 GPT-2/3、BERT 的变体 |

### 为什么 Pre-Norm 更稳定？

简单来说，Pre-Norm 的稳定性的根本原因在于它**控制了网络中的信号流**。

在 Post-Norm 中，每一层的输出都是 $x_{out} = \text{LayerNorm}(x + \text{Sublayer}(x))$。由于 $\text{Sublayer}(x)$ 的方差可能很大，并且它被直接加到 $x$ 上，这个不稳定的信号会传递到下一层。即使之后进行了归一化，这种累积效应在深层网络中仍然难以控制，导致梯度爆炸。

而在 Pre-Norm 中，每一层的输入 $\text{Sublayer}$ 都被强制归一化，即 $\text{Sublayer}(\text{LayerNorm}(x))$。这意味着每一层的输出方差被有效抑制在一个可控的范围内。残差连接的累加虽然仍在进行，但由于每一步的累加值方差都很小，整个网络的信号流变得非常稳定，从而避免了梯度爆炸。


---

## Layer Normalization (LayerNorm) vs. RMS Normalization (RMSNorm)

LayerNorm 和 RMSNorm 都是用于稳定 Transformer 模型训练的归一化技术，它们都对每个训练样本独立进行归一化，与 Batch Normalization 不同。它们的主要区别在于如何计算归一化的统计量。

### 1. Layer Normalization (LayerNorm)

LayerNorm 是在 Transformer 模型中最早被广泛使用的归一化方法，它的目标是使每个特征向量的激活值具有**零均值（zero mean）和单位方差（unit variance）**。

* **计算步骤**：
    对于一个输入向量 $x \in \mathbb{R}^D$（其中 $D$ 是特征维度），LayerNorm 的计算分为以下几步：
    1. **计算均值**：对向量 $x$ 的所有元素求平均值 $\mu$。
    $$
    \mu = \frac{1}{D} \sum_{i=1}^{D} x_i
    $$
    2. **计算方差**：对向量 $x$ 的所有元素，计算其与均值 $\mu$ 的差的平方的平均值 $\sigma^2$。
    $$
    \sigma^2 = \frac{1}{D} \sum_{i=1}^{D} (x_i - \mu)^2
    $$
    3. **归一化**：使用均值和方差对输入向量进行归一化。为了防止除以零，会添加一个很小的常数 $\epsilon$。
    $$
    \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
    $$
    4. **仿射变换 (Affine Transformation)**：归一化后，模型使用两个可学习的参数 $\gamma$（缩放因子）和 $\beta$（偏移量）对结果进行线性变换。
    $$
    y_i = \gamma \hat{x}_i + \beta
    $$

* **核心思想**：LayerNorm 旨在通过**均值居中（mean-centering）**和**方差缩放（variance-scaling）**来稳定激活值的分布。这使得模型训练更加稳定，并有助于解决梯度消失和爆炸问题。

### 2. RMS Normalization (RMSNorm)

RMSNorm 是 LayerNorm 的一个简化版本，它在 2019 年的论文 "Root Mean Square Layer Normalization" 中被提出。RMSNorm 的核心假设是：在许多情况下，均值居中操作（即减去均值 $\mu$）并不是必需的，只保留方差缩放就足够了。

* **计算步骤**：
    对于一个输入向量 $x \in \mathbb{R}^D$（其中 $D$ 是特征维度），RMSNorm 的计算步骤如下：
    1. **计算均方根 (Root Mean Square, RMS)**：计算向量 $x$ 的所有元素的平方的平均值的平方根。
    $$
    \text{RMS}(x) = \sqrt{\frac{1}{D} \sum_{i=1}^{D} x_i^2}
    $$
    2. **归一化**：直接使用 RMS 对输入向量进行归一化。
    $$
    \hat{x}_i = \frac{x_i}{\text{RMS}(x) + \epsilon}
    $$
    3. **仿射变换 (Affine Transformation)**：与 LayerNorm 类似，RMSNorm 也使用可学习的缩放因子 $\gamma$。但通常情况下，它**不使用偏移量 $\beta$**。
    $$
    y_i = \gamma \hat{x}_i
    $$

* **核心思想**：RMSNorm 假设**方差缩放**是归一化的关键，而**均值居中**是多余的。它通过移除均值计算步骤来简化 LayerNorm。

### 3. LayerNorm 与 RMSNorm 的详细对比

| 特性 | Layer Normalization (LayerNorm) | RMS Normalization (RMSNorm) |
|---|---|---|
| **归一化统计量** | **均值**和**标准差** | **均方根 (RMS)** |
| **计算公式** | $y = \gamma \frac{x - \mu}{\sigma} + \beta$ | $y = \gamma \frac{x}{\text{RMS}(x)}$ |
| **均值居中** | **是**（通过减去 $\mu$） | **否** |
| **可学习参数** | $\gamma$ (缩放) 和 $\beta$ (偏移) | $\gamma$ (缩放) |
| **计算效率** | 较低 | 较高（因为它省去了均值计算） |
| **训练稳定性** | 非常好 | 同样非常好，甚至在某些情况下表现更好 |
| **模型性能** | 强大，是原始 Transformer 的基石 | 强大，在许多现代大型语言模型（如 LLaMA, GPT-NeoX 等）中广泛应用，性能与 LayerNorm 相当或略优 |

### 4. 为什么 RMSNorm 越来越受欢迎？

* **计算效率**：RMSNorm 的计算比 LayerNorm 更快。在 LayerNorm 中，需要计算均值和方差，而在 RMSNorm 中，只需要计算均方根。虽然这看起来只是一个微小的区别，但在现代大型模型中，归一化层会占据相当一部分计算开销，尤其是在内存带宽受限的情况下，RMSNorm 的计算简化能带来明显的加速。

* **性能相当或更好**：研究表明，移除均值居中操作并没有显著影响模型的性能，甚至在一些情况下，RMSNorm 的表现更好。这表明在大多数深度学习任务中，**方差缩放**才是归一化的关键，而均值居中的作用相对较小。

* **简化模型设计**：RMSNorm 的设计更简单，没有偏移量参数 $\beta$（尽管一些实现会保留它）。这减少了模型的参数量，并在某些情况下能够让模型更高效。

### 结论

LayerNorm 和 RMSNorm 都是优秀的归一化技术，用于稳定 Transformer 模型的训练。LayerNorm 是原始 Transformer 的选择，它通过均值居中和方差缩放来规范激活值。而 RMSNorm 是 LayerNorm 的一个简化版本，它去除了均值居中，只保留了方差缩放。

在实践中，由于 RMSNorm 具有更高的计算效率，并且在大多数情况下能够提供与 LayerNorm 相当甚至更好的性能，它在许多最新的大型语言模型中越来越受欢迎，成为一种替代 LayerNorm 的流行选择。