## Inference
Goal: Generate tokens given a specific prompt
使用一个已经训练好的模型来进行预测或决策的过程
- **训练 (Training):** 在这个阶段，模型通过学习大量的数据来识别模式和规律。例如，给一个图像识别模型展示数百万张猫的图片，让它学习猫的特征。
- **推理 (Inference):** 训练完成后，模型就被部署用于实际应用。当给模型输入新的、它从未见过的数据时，它会运用所学到的知识进行判断。这个过程就叫做 Inference。
    - **例子:** 将一张新的动物图片输入到已经训练好的图像识别模型中，模型输出“这是一只猫”的判断，这个过程就是 Inference。在自动驾驶汽车中，车载计算机会实时进行 Inference，分析传感器数据以识别行人、车辆和交通标志。
### **Prefill (预填充) 阶段**

**核心任务：并行处理输入，为生成做好准备**

`Prefill` 阶段是模型处理用户输入（Prompt）的第一个环节。在这个阶段，模型会一次性地、并行地处理你提供的所有文本。

- **并行计算 (Parallel Processing):** 这是 `prefill` 阶段最大的特点。模型中的计算单元（比如 Transformer 架构中的注意力机制）会同时关注输入文本中的所有词元（tokens）。例如，在处理“介绍一下图灵测试”这句话时，模型会同时计算“介绍”、“图灵”、“测试”这几个词之间的关联性。这使得它能快速构建对整个输入内容的宏观理解。
- **计算密集 (Computationally Intensive):** 因为需要一次性处理大量文本并计算它们之间复杂的相互关系，这个阶段通常需要大量的计算资源和内存。你可以想象，输入的文本越长，需要同时计算的连接就越多，计算量就越大。
- **生成第一个词元 (First Token):** `Prefill` 阶段的最终产物，是计算出接下来最应该生成的**第一个**词元，并生成一个包含输入信息的重要中间状态——**KV 缓存 (KV Cache)**。

### **Decode (解码) 阶段**

**核心任务：自回归地、逐个生成输出词元**

当 `prefill` 阶段完成，并且第一个输出词元被生成后，模型就进入了 `decode` 阶段。这个阶段的目标是逐个生成后续的回答。

- **自回归 (Autoregressive):** 这是 `decode` 阶段的核心机制。模型每生成一个新词元，都会将这个新词元加入到已有的序列中，然后基于这个更新后的完整序列来预测下一个最可能的词元。这个过程就像滚雪球一样，不断重复，直到生成完整的回答或遇到停止指令。
    
    - 例如：生成了 "人" -> 接下来预测 "工" -> 序列变成 "人工"，再预测 "智" -> 序列变成 "人工智能"，以此类推。
        
- **利用 KV 缓存 (KV Cache):** 为了避免在生成每一个新词元时都重新处理一遍完整的输入文本，模型会利用 `prefill` 阶段生成的 **KV 缓存**。这个缓存储存了输入文本的关键信息（Key 和 Value）。在 `decode` 的每一步，模型只需要计算当前新生成的词元与 KV 缓存中信息的关联，而无需重新计算整个输入，极大地提升了生成速度。
    
- **内存带宽敏感 (Memory-Bandwidth Bound):** 与 `prefill` 的计算密集不同，`decode` 阶段的计算量相对较小（因为每次只处理一个新词元），但它需要频繁地从内存中读取庞大的模型参数和 KV 缓存。因此，它的速度瓶颈通常在于内存的读取速度（带宽），而不是计算能力。
    
---
### **SFT 是什么？一句话概括**
Supervised Fine-Tuning
SFT 是一种利用**高质量的、人工标注的“指令-回答”数据对**，来进一步训练一个已经经过预训练的语言模型，目的是让模型学会遵循人类的指令并给出有用、无害、符合期望的回答。

### **为什么需要 SFT？预训练还不够吗？**

一个只经过**预训练 (Pre-training)** 的基础模型（Base Model），就像一个博览群书但涉世未深的学生。

- **它的能力：** 它学习了海量文本数据（如维基百科、书籍、网页）中的语言规律、语法结构和事实知识。它能“预测下一个词”，从而生成流畅的文本。
    
- **它的问题：** 它不知道如何“做特定任务”。你对它说“总结一下这段文字”，它可能会续写这段文字，而不是去总结。它也不知道什么是好的回答，什么是坏的回答。它只是一个语言的“补全机器”，而不是一个乐于助人的“对话助手”。
    
**SFT 的目的就是解决这个问题，实现模型的“行为对齐” (Behavior Alignment) 的第一步。** SFT 教会模型如何识别并遵循用户的指令，将它的知识以一种结构化、有帮助的方式呈现出来。

---
# Tokenization

Tokenization 就是将一段人类语言的文本（字符串），切分成一个个更小的、有意义的单元（Tokens）的过程。

计算机模型（如 LLM）无法直接理解“你好世界”这样的字符串。它们只能处理数字。因此，必须先把文本打碎成一个个“零件”，然后将每个“零件”映射到一个唯一的数字 ID。这个过程就叫做 Tokenization。

- **文本 (Text):** "I love learning AI."
- **Tokens:** `["I", "love", "learning", "AI", "."]`
- **Token IDs:** `[101, 2293, 7856, 17672, 102]` (这里的数字是示例)

这些 Token 就是模型能够处理的最小单位。

### Tokenization 的主要方法
#### **方法一：基于词 (Word-based Tokenization)**

这是最符合人类直觉的方法。直接按照空格或标点符号来切分单词。

- **文本:** `"Let's learn tokenization!"`    
- **Tokens:** `["Let's", "learn", "tokenization", "!"]`

**优点：**
- 简单直观，切分出的 Token 具有完整的语义。

**缺点：**
- **词汇表爆炸 (Vocabulary Explosion):** 英语中 `learn`, `learning`, `learned` 都会被当作不同的 Token，导致词汇表变得异常庞大。对于德语、芬兰语等形态丰富的语言，问题更严重。
- **未登录词 (Out-of-Vocabulary, OOV):** 如果模型在训练时没见过某个词（如一个新名字 `“Bilbo”` 或拼写错误 `“learnning”`），它就无法处理，会将其标记为 `[UNK]` (Unknown)。这会导致信息丢失。

#### **方法二：基于字符 (Character-based Tokenization)**

将文本切分成最基本的字符单元。
- **文本:** `"Hello world"`
- **Tokens:** `["H", "e", "l", "l", "o", " ", "w", "o", "r", "l", "d"]`

**优点：**
- **没有未登录词：** 词汇表非常小（只包含所有字母、数字、符号），永远不会遇到不认识的 Token。
- 对拼写错误有更强的鲁棒性。

**缺点：**
- **序列过长 (Sequence Length):** 一个单词被拆成很多个 Token，导致输入给模型的序列变得非常长，这会极大地增加计算负担和内存消耗。
- **丢失语义信息：** 单个字母（如 `H`）本身几乎没有语义，模型需要消耗大量计算资源去学习如何将这些字母重新组合成有意义的单词。

#### **方法三：子词 (Subword-based Tokenization) - 当前主流**

这是介于基于词和基于字符之间的一种**黄金方案**。它的核心思想是：

> **常用词保持为完整的 Token，不常用的词拆分为有意义的子词单元。**

这样既能避免词汇表爆炸，又能有效处理未登录词，同时保留了大部分语义。

- **文本:** `"I have a new smartphone"`
- **Tokens:** `["I", "have", "a", "new", "smart", "phone"]` (常用词 `I`, `have`, `a`, `new` 保持完整，不常用的 `smartphone` 被拆分)
    
- **文本:** `"unhappily"`
- **Tokens:** `["un", "happi", "ly"]` (拆分成有意义的前缀 `un-`、词根 `happi` 和后缀 `-ly`)
    

**主流的子词算法包括：**

1. **BPE (Byte Pair Encoding):**
    - **工作原理：** 从字符级别开始，迭代地将最常出现的一对 Token 合并成一个新的 Token，并加入词汇表，直到达到预设的词汇表大小。    
    - **例子：** 初始 `{"h", "e", "l", "o", ...}` -> 发现 `l` 和 `l` 经常一起出现，合并成 `ll` -> 词汇表变为 `{"h", "e", "l", "o", "ll", ...}` -> 再发现 `e` 和 `ll` 经常一起出现...
    - **应用：** GPT 系列模型（如 GPT-3, GPT-4）使用这种方法。
    - 缺点：long sequence
        
2. **WordPiece:**
    - **工作原理：** 与 BPE 类似，但合并的依据不是频率最高，而是能最大程度增加训练数据“似然度”（Likelihood）的组合。简单理解就是，它会选择合并后能让整个语料库的压缩效果最好的那对 Token。
    - **应用：** Google 的 BERT 和 T5 模型使用此方法。
    
    - 当遇到一个新词时，WordPiece 会如何进行令牌化呢？
	    **核心规则：贪心匹配 (Greedy Matching)**
		WordPiece 会从词的开头开始，**寻找词汇表里能匹配到的最长的子词**
		**一个关键特征：** 对于不构成词语开头的子词，WordPiece（及其在 BERT 中的实现）会使用 `##` 前缀来标记。
		
		例1：令牌化单词 bugs
		从头开始看 bugs。
		在词汇表中寻找最长的匹配前缀。b? 在。bu? 不在。bug? 在！
		匹配到了 bug。剩下的部分是 s。
		看 s。s 在词汇表中吗？在。
		因为 s 不是单词的开头，所以标记为 ##s。
		结果： bugs ➡️ ["bug", "##s"]

		例2：令牌化一个未知词 rugs
		假设我们的模型在训练中从未见过 rugs。
		从头开始看 rugs。
		最长的前缀是什么？r? 
		假设 r 在我们的初始字符集里，所以在词汇表中。ru? 不在。rug? 不在。所以第一个 Token 是 r。
		现在看剩下的部分 ugs。
		在词汇表中寻找 ugs 的最长匹配前缀。u? 在。ug? 在！ugs? 也在！
		根据贪心规则，选择最长的 ugs。因为它不是单词的开头，标记为 ##ugs。
		结果： rugs ➡️ ["r", "##ugs"]
		

3. **SentencePiece:**
    - **工作原理：** 这是 Google 开发的一套更通用的 Tokenization 工具，它将所有文本（包括空格）都视为普通符号来处理。它把 Tokenization 视作一个端到端的任务，可以直接从原始文本（Raw Text）生成 Token 序列，而不需要预先按空格分词。
    - **优点：** 
	    - 对多语言处理非常友好，尤其是不使用空格分隔的语言（如中文、日文、泰文）。
	    - **可逆性 (Reversible):** 这是 SentencePiece 的一个杀手级特性。任何被令牌化的文本都可以完美地、无歧义地逆转回原始文本。这在很多需要精确还原文本的场景中至关重要。
    - **应用：** Llama、Mistral 等许多现代开源模型都采用 SentencePiece。

    - 例子：规则同样是贪心匹配（寻找词汇表里能匹配到的最长的子词）。

		句子： Hello world wide
		预处理： 首先，将句子中的空格替换为下划线 "_"。
		
		Hello world wide ➡️ Hello_world_wide
		
		令牌化这个新字符串 Hello world wide：
		从头开始看，H? 在。He? 在。Hel? 在。Hell? 在。Hello? 在词汇表中！好的，第一个 Token 就是 Hello。
		
		现在看剩下的部分：_world_wide。
		从 _ 开始，_? 在。 _w? 在。 _wo? 在。 _wor? 在。 _worl? 在。 _world? 在词汇表中！好的，第二个 Token 就是  _world。
		
		看最后剩下的部分： _wide。
		假设我们的词汇表中没有  _wide，但有  _wi 和 de。
		从 _ 开始，最长的匹配是  _wi。
		剩下 de，词汇表里有 de。
		
		结果： Hello world wide ➡️ ["Hello", "_world", "_wi", "de"]


---
