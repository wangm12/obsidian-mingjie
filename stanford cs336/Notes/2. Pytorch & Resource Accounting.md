## Tensor - 张量 是什么
Tensor 是数据的容器
**从最核心、最实用的角度来讲，张量就是一个多维数组，是计算机用来组织和存储各种数据的一种容器。**

你可以把它想象成一个可以有任意“方向”或“维度”的盒子：
- **0 阶张量 (0D Tensor / Rank-0 Tensor):** 只有一个数字，没有维度。它就是一个**标量 (Scalar)**。
    - 例子：`5`，`3.14`，或者一张图片里某个像素的亮度值 `255`。
        
- **1階张量 (1D Tensor / Rank-1 Tensor):** 一列数字，有一个维度。它就是一个**向量 (Vector)**。
    - 例子：`[1, 2, 3]`，或者一个学生在数学、语文、英语上的成绩 `[95, 88, 92]`。
        
- **2階张量 (2D Tensor / Rank-2 Tensor):** 一个二维数组，有行和列两个维度。它就是一个**矩阵 (Matrix)**。
    - 例子：一张灰度图片。行可以代表图片的高度，列可以代表宽度，每个位置的值是该像素的灰度。
        ```
        [[1, 2, 3],
         [4, 5, 6],
         [7, 8, 9]]
        ```
        
- **3階张量 (3D Tensor / Rank-3 Tensor):** 一个三维数组，可以想象成一个立方体。
    - 例子：一张**彩色图片**。通常表示为 `(高度, 宽度, 通道数)`。例如，一张 1920x1080 的彩色图片就是一个 `(1080, 1920, 3)` 的 3D 张量，这里的 `3` 代表红(R)、绿(G)、蓝(B)三个颜色通道。
        
- **4階张量 (4D Tensor / Rank-4 Tensor):** 一个四维数组。
    - 例子：**一批 (batch) 彩色图片**。在机器学习训练中，我们通常不会一次只处理一张图片，而是一批。如果每批有 16 张 1080x1920 的彩色图片，那么这个数据就被组织成一个 `(16, 1080, 1920, 3)` 的 4D 张量。这里的 `16` 就是批次大小 (batch size)。
        
- **5階张量 (5D Tensor / Rank-5 Tensor) 及更高：**
    - 例子：**一批视频数据**。视频可以看作是图片的序列。如果有一批（16个）、每个包含100帧、分辨率为 1080x1920 的彩色视频，那么这个数据就是一个 `(16, 100, 1080, 1920, 3)` 的 5D 张量。
        
**在机器学习领域，"Tensor" 这个词几乎总是指这种多维数据容器。** 像 TensorFlow 和 PyTorch 这样的主流框架，其名字中的 "Tensor" 就源于此。所有的数据输入、模型参数（权重）、计算过程中的中间值，都是以张量的形式存在和流动的。


## Memory
`float32`（也称为单精度）是标准格式，使用32位内存。`float16`（半精度）和 `bfloat16`（“大脑浮点数”）都是16位格式，旨在节省内存并加速计算

### **格式剖析**

在数字系统中，浮点数通常由三个部分组成：**符号位**（决定数字是正还是负）、**指数位**（决定数字的表示范围大小）和**尾数位/小数位**（决定数字的精度）。

以下是每种格式如何分配其比特位：

| 格式           | 总比特数 | 符号位 | 指数位 (决定范围) | 尾数位 (决定精度) |
| ------------ | ---- | --- | ---------- | ---------- |
| **float32**  | 32   | 1   | 8          | 23         |
| **float16**  | 16   | 1   | 5          | 10         |
| **bfloat16** | 16   | 1   | 8          | 7          |
### **核心差异与应用场景**

#### **float32 (单精度浮点数)**

`float32` 是绝大多数编程语言和应用程序中默认的浮点数类型。
- **范围与精度：** 它提供了非常宽的动态范围和很高的精度（约7位十进制有效数字）。这使其成为各种通用科学计算和工程任务的理想选择。
- **在AI中的应用：** 在机器学习中，`float32` 通常用于从零开始训练模型以及需要高数值准确性的任务。它被视为性能和稳定性的基准。
    

#### **float16 (半精度浮点数)**
`float16` 将 `float32` 的内存占用减半，这可以在现代GPU上显著加快计算速度。

- **范围与精度：** 追求效率的代价是其动态范围急剧缩小，精度也更低（约3-4位十进制有效数字）。仅有的5个指数位意味着它无法表示非常大或非常小的数字，因此在训练过程中容易出现“上溢”（数字过大）或“下溢”（数字过小，接近于零）的错误。
- **在AI中的应用：** `float16` 主要用于模型推理，以及在训练中更谨慎地使用。通常需要借助**混合精度训练 (Mixed-Precision Training)** 技术，即关键计算（如权重更新）保持为`float32`以维持稳定性，而大部分的矩阵乘法则在`float16`中完成以追求速度。
    

#### **bfloat16 (大脑浮点数)**
`bfloat16`（Brain Floating-Point）是由谷歌专为机器学习开发的格式。它是 `float32` 和 `float16` 之间的一个巧妙折中。

- **范围与精度：** `bfloat16` 保留了与 `float32` 相同数量的指数位（8位）。这使其拥有与 `float32` **相同的动态范围**，从而有效避免了 `float16` 中常见的上溢/下溢问题。为此付出的代价是尾数位被严重压缩（仅7位），导致其精度甚至比 `float16` 还要低（约2-3位十进制有效数字）。
- **在AI中的应用：** 其设计理念是：对于深度学习而言，能够表示一个宽广的数值范围比对单个数字具有高精度更为关键。`bfloat16` 已经成为训练大型模型的流行选择，尤其是在谷歌的TPU和现代NVIDIA、AMD的GPU上，因为它在提供与`float16`相当的速度和内存节省的同时，没有其数值不稳定的缺点。
    
---
### **对比总结**

|特性|`float32`|`float16`|`bfloat16`|
|---|---|---|---|
|**大小**|32位|16位|16位|
|**数值范围**|**宽**|窄|**宽**|
|**精度**|**高** (约7位有效数字)|低 (约3-4位有效数字)|非常低 (约2-3位有效数字)|
|**核心优势**|高精度和高稳定性|内存占用小，计算速度快|数值范围广，训练稳定|
|**核心劣势**|内存/计算成本更高|容易上溢/下溢|精度非常低|
|**主要用途**|通用计算，稳定的模型训练|模型推理，混合精度训练|训练大型AI模型|
|**形象比喻** 🧠|一台**精密科学仪器**，读数准确。|一把**小巧但刻度粗糙的卷尺**，量得快但不准。|一把**能测量天文距离但也刻度粗糙的卷尺**，范围广但细节差。|

---

## Compute

### **Tensor** 在 **PyTorch** 中具体是指什么
> PyTorch Tensors are **pointers** into allocated memory

### **PyTorch Tensor 的三大“超能力”**

如果 `Tensor` 仅仅是一个多维数组，那和 Python 的 NumPy 库就没什么区别了。PyTorch 的 `Tensor` 之所以成为深度学习的核心，是因为它具备以下三大关键特性：

#### **超能力一：GPU 加速 (Hardware Acceleration)**
这是 PyTorch `Tensor` 与 NumPy 数组最显著的区别之一。

- **NumPy Array:** 只能在 CPU (中央处理器) 上运行。
- **PyTorch Tensor:** 可以非常轻松地在 CPU 和 GPU (图形处理器) 之间移动。

```
# 创建一个在 CPU 上的 Tensor
cpu_tensor = torch.tensor([[1., 2.], [3., 4.]], device='cpu')

# 将其移动到 GPU 上 (如果 GPU 可用)
if torch.cuda.is_available():
    gpu_tensor = cpu_tensor.to('cuda')
    print(gpu_tensor.device) # 输出: cuda:0
```

GPU 拥有数千个核心，能够并行执行大规模的矩阵运算，这对于深度学习中的海量计算至关重要。将 `Tensor` 放到 GPU 上，可以使其后续的计算速度提升几十甚至几百倍。

#### **超能力二：自动求导 (Automatic Differentiation)**
这是 PyTorch 动态计算图和反向传播机制的基石，也是 `Tensor` 最神奇的地方。
当你创建一个 `Tensor` 时，可以设置一个属性 `requires_grad=True`。
这相当于告诉 PyTorch：“请追踪所有基于这个 `Tensor` 的计算历史。”

```
# x 是模型的输入或权重，需要计算梯度
x = torch.tensor(2.0, requires_grad=True)

# y 和 z 是一系列计算
y = x ** 2
z = 3 * y + 5

# 执行反向传播
z.backward()

# PyTorch 会自动计算出 z 对 x 的梯度 (dz/dx)
print(x.grad) # 输出: tensor(12.)
```

在神经网络训练中，我们就是利用这个功能来计算损失函数（Loss）对于模型所有参数（权重）的梯度，然后优化器（Optimizer）根据这些梯度来更新参数，从而让模型不断学习。这个过程被称为**反向传播 (Backpropagation)**。`Tensor` 的 `autograd` 引擎将这个极其复杂的过程完全自动化了。

#### **超能力三：丰富的生态系统和函数库**
PyTorch 围绕 `Tensor` 构建了一个庞大且高效的生态系统。`torch` 模块提供了海量的数学运算函数，这些函数都针对 `Tensor` 进行了高度优化。

- **创建操作:** `torch.zeros()`, `torch.randn()`, `torch.arange()`
- **数学运算:** `torch.abs()`, `torch.sin()`, `torch.matmul()` (矩阵乘法)
- **神经网络层:** `torch.nn.Linear`, `torch.nn.Conv2d` 等模块的输入和权重也都是 `Tensor`。
- **变形操作:** `tensor.view()`, `tensor.reshape()`, `tensor.permute()`


---
### 什么是 pytorch 的 contiguous 
**`.contiguous()` 的核心作用是确保一个张量（Tensor）在内存中的存储是“连续的”。**

如果一个张量本身已经是内存连续的，调用这个方法不会有任何效果，会直接返回它自身。如果它不是内存连续的，这个方法会**创建一个新的、数据内容相同但内存布局是连续的张量副本**，并返回这个新的副本。

### **为什么需要 `.contiguous()`？**

要理解它的作用，首先要明白什么是“内存连续”和“不连续”。

1. **默认情况：内存连续** 当你创建一个新的张量时，比如 `torch.randn(2, 3)`，PyTorch 会在内存中开辟一块**完整的、未中断的**空间来存储它的所有元素。对于一个 2x3 的张量 `[[1, 2, 3], [4, 5, 6]]`，它在内存里就是这样存放的： `[1, 2, 3, 4, 5, 6]` 这种布局就是**连续的 (Contiguous)**。访问这样的数据速度非常快，因为CPU/GPU可以高效地进行缓存和预读取。
2. **问题来源：产生不连续的“视图”操作** PyTorch 中有很多操作并不会真的去修改内存中的数据，而是创建一个新的“视图 (View)”来解释这些数据。最典型的操作就是**转置 (Transpose)**。
    
    让我们看一个例子：
    ```
    import torch
    
    # 1. 创建一个 2x3 的连续张量 a
    a = torch.arange(6).reshape(2, 3)
    print(a)
    # tensor([[0, 1, 2],
    #         [3, 4, 5]])
    print("a 是连续的吗?", a.is_contiguous()) # 输出: True
    
    # 2. 对 a 进行转置，得到 b
    b = a.T  # 或者 a.transpose(0, 1)
    print(b)
    # tensor([[0, 3],
    #         [1, 4],
    #         [2, 5]])
    print("b 是连续的吗?", b.is_contiguous()) # 输出: False
    ```
    
    这里发生了什么？
    
    - 张量 `b` 在逻辑上是一个 3x2 的矩阵。
    - 但是，在物理内存中，数据**根本没有动**！它仍然是 `[0, 1, 2, 3, 4, 5]`。
    - 为了读取 `b` 的第一行 `[0, 3]`，程序需要先读取内存地址第0位的 `0`，然后**跳过** `1` 和 `2`，去读取内存地址第3位的 `3`。
    - 因为访问内存需要“跳跃”，所以我们说张量 `b` 是**不连续的 (Non-contiguous)**。
### **`.contiguous()` 的用武之地**

不连续的张量会带来两个主要问题，而 `.contiguous()` 就是用来解决它们的。
#### **1. 兼容性问题：某些操作强制要求连续内存**

最常见的场景就是在使用 `.view()` 方法时。`.view()` 方法要求张量必须是内存连续的，因为它只是改变了描述张量形状的元信息，并不会重新排列数据。如果你对一个不连续的张量使用 `.view()`，PyTorch 会直接报错。

继续上面的例子：

```
try:
    # 尝试对不连续的张量 b 使用 .view()
    b.view(6)
except RuntimeError as e:
    print("出错了:", e)
    # 出错了: view size is not compatible with input tensor's size and stride
    # (or similar error indicating non-contiguous memory)
```

**解决方案：** 在调用 `.view()` 之前，先调用 `.contiguous()`。

```
# 1. 先调用 .contiguous() 创建一个内存连续的新张量 c
c = b.contiguous()
print("c 是连续的吗?", c.is_contiguous()) # 输出: True

# 2. 现在 c 的内存布局变成了 [0, 3, 1, 4, 2, 5]
# 3. 对 c 使用 .view() 就没有问题了
print(c.view(6)) # 输出: tensor([0, 3, 1, 4, 2, 5])
```

#### **2. 性能问题**

如前所述，对不连续内存的访问会降低计算效率。虽然不是每次都会报错，但在进行密集的计算之前，将张量转换为连续布局可能会带来性能提升。

### **总结**

你应该在何时使用 `.contiguous()`？
1. **当你对一个张量执行了 `transpose()`, `permute()`, `stride()` 等改变其内存步幅（stride）的操作后。**
2. **当你需要调用一个明确要求输入为连续张量的方法时（最典型的就是 `.view()`）。**
3. **当 PyTorch 抛出与 "contiguous" 或 "stride" 相关的错误时，这通常就是最直接的解决方案。**
可以把 `.contiguous()` 理解为一个“内存整理”工具。它确保了张量的逻辑形状和其在物理内存中的排列方式是一致的，从而保证了代码的兼容性和运行效率。

---

### `einops` 的核心理念

`einops` 的核心是提供一种**声明式**的、基于字符串的迷你语言来描述张量操作。你只需要用简单的字符串来说明“输入是什么样的，我想要输出变成什么样的”，而不需要编写繁琐、易错的多层嵌套代码。

它主要提供了三个核心函数：

- **`rearrange`**: 用于重排、变形张量的维度。
- **`reduce`**: 用于聚合、减少张量的维度。
- **`repeat`**: 用于重复、扩展张量的维度。

#### 核心功能示例

下面我们通过具体例子来看看 `einops` 如何让代码变得优雅。

### 1. `rearrange` - 重排维度

`rearrange` 是 `einops` 最强大的函数，可以替代 `reshape`, `view`, `transpose`, `permute` 等多种操作。

**场景**：将一批图片从 `(批次, 高度, 宽度, 通道)` 格式转换为 `(批次, 通道, 高度, 宽度)` 格式。
- **传统方法 (PyTorch)**:
    ```
    # images_torch 的形状: [16, 224, 224, 3]
    output = images_torch.permute(0, 3, 1, 2)
    # output 的形状: [16, 3, 224, 224]
    ```
    
    这里的 `0, 3, 1, 2` 是维度的索引，可读性很差，容易出错。
    
- **`einops` 方法**:    
    ```
    from einops import rearrange
    
    # 只需要描述维度的名字和它们的顺序
    output = rearrange(images_torch, 'b h w c -> b c h w')
    ```
    
    代码清晰地表达了意图：将 `b h w c` 的布局重排为 `b c h w`。

### 2. `reduce` - 聚合维度

`reduce` 可以替代 `mean`, `sum`, `max` 等聚合操作。

**场景**：对一批图片在高度和宽度维度上计算平均值，得到一个特征向量。

- **传统方法 (PyTorch)**:
    ```
    # images_torch 的形状: [16, 3, 224, 224]
    # 在维度2和3上求平均
    output = torch.mean(images_torch, dim=(2, 3))
    # output 的形状: [16, 3]
    ```
    
- **`einops` 方法**:
    ```
    from einops import reduce
    
    # 描述哪些维度需要被聚合掉
    output = reduce(images_torch, 'b c h w -> b c', 'mean')
    ```
    
    代码意图明确：保留 `b` 和 `c` 维度，将 `h` 和 `w` 维度通过 `mean` 操作聚合掉。

### 3. `repeat` - 重复维度

`repeat` 可以替代 `expand` 或 `tile` 等操作。

**场景**：将一个单通道的灰度图复制成一个三通道的彩色图。

- **传统方法 (PyTorch)**:
    ```
    # image 的形状: [224, 224]
    # 增加一个通道维度，然后扩展3次
    output = image.unsqueeze(0).repeat(3, 1, 1)
    # output 的形状: [3, 224, 224]
    ```
    
- **`einops` 方法**:
    ```
    from einops import repeat
    
    # 描述要重复哪个维度以及重复的次数
    output = repeat(image, 'h w -> c h w', c=3)
    ```
    
    代码清晰地说明了将一个 `h w` 的图像重复 `c` 次（`c=3`），最终得到 `c h w` 的形状。

#### 为什么使用 `einops`？

1. **可读性强**：代码即文档，通过简单的字符串就能清晰地理解复杂的张量变换。
2. **减少错误**：避免了记忆繁琐的维度索引和复杂的函数组合，从源头上减少了bug。
3. **框架无关**：同一套 `einops` 代码可以无缝运行在 PyTorch, TensorFlow, NumPy, JAX 等多种后端上，方便代码迁移和复用。
4. **功能强大**：可以用非常简洁的语法实现多种操作的组合，例如在一次 `rearrange` 中同时完成变形和转置。

--- 

## FLOP

### **Fl**oating-point **O**perations **P**er **S**econd (每秒浮点运算次数)
- **定义**: 这是一个衡量计算设备（如CPU、GPU、TPU）**性能/算力**的标尺。它表示一个处理器**每秒钟**能执行多少次浮点数运算（加、减、乘、除等）。
    
- **用途**: 用来描述硬件的“马力”有多大。数字越大，代表硬件的计算能力越强，跑模型、做计算的速度就越快。

### **Floating-point **OP**erations (浮点运算次数)
- **定义**: 这是一个衡量某个特定计算任务（比如，运行一次神经网络的正向传播）**总共需要多少次浮点运算**的单位。它衡量的是**算法或模型的计算成本/复杂度**。
    
- **用途**: 用来评估一个模型有多“大”，运行起来有多“昂贵”。FLOPs 越大的模型，需要的计算资源就越多，推理和训练的时间也越长。

* 记住
```
x = torch.ones(B, D)
w = torch.randn(D, K)
y = x @ w

# we have one multiplication
# x[i][j] * w[j][k]
# AND
# one addition

# actual_num_flops = 2 * B * D * K
```

### MFU
**MFU** 的全称是 **Model FLOPs Utilization**，中文意思是“**模型FLOPs利用率**”。

简单来说，MFU 回答了这样一个问题：

> “我的 GPU 每秒能进行一万亿次计算（理论峰值），但为了运行我的模型，它实际上一秒钟执行了多少次有效的计算？这个实际值占理论峰值的百分比是多少？”

这个百分比就是 MFU。一个更高的 MFU 意味着你的代码、模型架构和训练配置能够更充分地“压榨”出硬件的潜能，训练效率更高。

通常来说 >=0.5 是不错的
#### **MFU 如何计算？**

MFU 的计算公式直观地反映了它的定义
$$
MFU=\frac{实际达到的TFLOPs (Achieved TFLOPs)}{硬件的理论峰值TFLOPs (Peak TFLOPs)}​
$$

其中：
1. **硬件的理论峰值TFLOPs (分母)**：这是一个固定值，由硬件制造商提供。例如，NVIDIA A100 GPU 在FP16精度下的理论峰值是 312 TFLOPs。
2. **实际达到的TFLOPs (分子)**：这个值需要通过训练过程来测量。
    - 首先，你需要计算出你的模型**处理一个Token需要多少FLOPs**（这取决于模型架构，如层数、头数、隐藏层维度等）。这个值通常表示为 `FLOPs/Token`。
    - 然后，你测量训练过程中的**实际吞吐量**，即每秒处理了多少个Token (`Tokens/Second`)。
    - **实际达到的TFLOPs = (FLOPs/Token) × (Tokens/Second)**
    
**综合起来，完整的公式是：**
$$
MFU=\frac{(模型处理单个Token所需的FLOPs)×(实际吞吐量, Tokens/Second)}{硬件的理论峰值TFLOPs (Peak TFLOPs)}​
$$

---

## Gradients
梯度在机器学习中的作用 —— 指引模型优化的方向

机器学习的核心目标是**优化**，即调整模型的参数（比如神经网络中的权重 w 和偏置 b），来让一个被称为**损失函数 (Loss Function)** 的值变得最小。

**损失函数**衡量的是模型“预测的有多差”。
损失值越小，代表模型的预测越接近真实答案。

现在，我们可以把整个优化过程想象成**一个盲人下山的比喻**：

- **群山**: 所有可能的参数组合构成的“损失函数的表面”，高低起伏。
- **山谷的最低点**: 我们要寻找的目标，即让损失函数最小的最优参数组合。
- **盲人**: 我们的优化算法。他看不见整个地图，只能感知脚下所处位置的情况。
- **脚下的坡度**: 这就是**梯度**！

梯度告诉了盲人“哪个方向是上坡最陡峭的”。那么，盲人要想最快地下山，最聪明的策略就是朝着**梯度的相反方向**走一步。这个“下山”的过程，就是机器学习中最核心的优化算法——**梯度下降 (Gradient Descent)**。

#### 梯度下降 (Gradient Descent)

梯度下降算法的步骤非常直观：

1. **随机初始化参数**：把盲人随机放在山上的某个位置。
2. **计算当前位置的梯度**：用脚（算法）感受一下当前位置哪个方向是上坡最陡的。
3. **朝着梯度的反方向走一小步**：朝着最陡的下坡方向迈出一步，这一步的大小由**学习率 (Learning Rate)** 控制。
4. **重复**：在新位置上，重复第2步和第3步，不断地“下山”。

这个过程的数学更新公式为：

新参数=旧参数−学习率×梯度

wnew​=wold​−η∇L(wold​)

### 如何计算梯度？—— 反向传播算法 (Backpropagation)

对于一个简单的函数，我们可以手动计算梯度。但神经网络是一个极其复杂的复合函数，可能包含数百万甚至数十亿个参数。手动计算梯度是不可能的。

这时就需要**反向传播 (Backpropagation)** 算法。

**反向传播**是应用微积分中的**链式法则 (Chain Rule)** 来高效计算神经网络中所有参数梯度的算法。

它的工作流程如下：

1. **前向传播 (Forward Pass)**：输入一个数据样本，让它流过整个网络，从输入层到输出层，最终计算出预测值，并根据预测值和真实值计算出**总损失**。
2. **反向传播 (Backward Pass)**：
    - 从最终的损失值开始，“反向”地通过网络
    - 首先计算损失对于输出层参数的梯度。
    - 然后，利用链式法则，逐层向后计算每一层参数对于最终损失的梯度，就像把“误差”或“责任”一层层地向后传递一样。
    - 这个过程会一直持续到输入层，最终得到网络中**每一个参数**的梯度。

### 计算flops
* Forward pass: 2 (# data points) (# parameters)
* Backward pass: 4 (# data points) (# parameters)
* Total: 6 (# data points) (# parameters)

---
## Optimizer

**AdaGrad (Adaptive Gradient Algorithm)** optimizer

AdaGrad 是在标准梯度下降（SGD）基础上发展而来的一种**自适应学习率**优化算法。它的核心思想是：**为模型中的每一个参数，都独立地、动态地调整其学习率。**

### 1. 为什么需要 AdaGrad？—— 解决“一视同仁”的问题

传统的随机梯度下降（SGD）算法有一个问题：它对所有参数都使用同一个固定的学习率（learning rate, η）。但这在很多现实场景中并不理想。

- **稀疏数据问题**：在自然语言处理或推荐系统中，某些特征可能非常稀疏（即大部分时间都为0，偶尔才出现）。
    - **频繁出现的特征**：我们希望它的学习率能够小一些，因为它已经被充分更新，我们想让它更稳定地收敛。
    - **稀疏（不常出现）的特征**：我们希望它的学习率能够大一些，以便在它难得出现时，能够有一次较大的更新，从而学到更多信息。
- **特征尺度问题**：不同参数的梯度大小可能差异巨大。如果用同一个学习率，梯度大的参数可能会更新过快产生震荡，而梯度小的参数则可能更新过慢。

AdaGrad 的诞生就是为了解决这个问题。它不再“一视同仁”，而是为每个参数量身定制一个学习率。

### 2. AdaGrad 的核心工作原理

AdaGrad 的工作原理非常直观：**它会记录下每个参数从训练开始至今所有梯度的平方和。在更新参数时，每个参数的学习率都会除以这个历史梯度平方和的平方根。**

这意味着
- **对于梯度一直很大的参数**：它的历史梯度平方和会很大，导致其有效学习率不断减小，从而让更新变得更谨慎、更稳定。
- **对于梯度一直很小的参数**：它的历史梯度平方和会很小，导致其有效学习率相对较大，从而让更新更激进，不会被“埋没”。
    
### 3. AdaGrad 的数学公式

我们来分解一下 AdaGrad 在第 t 个时间步的更新过程：

1. **计算梯度**： 首先，计算当前时刻损失函数 L 对参数 θt​ 的梯度 gt​。
    
    gt​=∇L(θt​)
    
2. **累积平方梯度**： 维护一个累加器 Gt​（一个与参数 θ 同样大小的向量/矩阵），用来存储每个参数历史梯度的平方和。Gt​ 的计算方式如下（⊙ 表示按元素相乘）：
    
    Gt​=Gt−1​+gt​⊙gt​
    
    在训练开始时，G0​ 通常被初始化为0。
    
3. **更新参数**： 在更新参数 θ 时，将全局学习率 η 除以 sqrt(G)
    ![[AdaGradUpdateEqual.png]]
    
    - η：全局初始学习率，通常设置为 0.01。
        
    - ϵ：一个非常小的平滑项（例如 10−8），用来防止分母为零。
        
    - sqrt (G)：这一项就是为每个参数量身定制的“调节器”。
        
### 4. AdaGrad 的优缺点

#### a. 优点

	1. **自适应学习率**：自动为不同参数调整学习率，减少了手动调整学习率的需要。
	2. **对稀疏数据友好**：在处理像词嵌入（Word Embeddings）这样的稀疏数据时表现非常出色，能够为不常更新的参数分配更大的学习率。
	3. **适合非凸优化问题**：在某些复杂的优化场景中表现稳定。

#### b. 缺点

AdaGrad 有一个非常致命的缺点，也导致了它现在不常被用作首选优化器：

1. **学习率单调递减且可能过早衰减**：
    
    - 从公式可以看出，Gt​ 是一个不断累积的正数，它只会越来越大。
    - 这导致分母 也会单调递增。
    - 因此，每个参数的有效学习率会随着训练的进行而**持续、单调地减小**。
    - 在训练后期，学习率可能会变得**无限小**，导致模型参数几乎不再更新，从而使得训练**过早停止**，未能达到最优解。
        
### 5. 总结与后续发展

AdaGrad 是自适应学习率优化算法的开山之作，它的思想极具启发性。虽然它本身因为学习率衰减过快的问题而不常被直接使用，但它的核心思想——**利用历史梯度信息来调整当前学习率**——被后续更先进的优化器所继承和改进。

- **RMSprop**：通过引入**指数移动平均**来计算历史梯度平方和，解决了 AdaGrad 学习率无限单调递减的问题，只关注最近一段时间的梯度信息。
    
- **Adam**：结合了 RMSprop 的思想（自适应学习率）和 Momentum 的思想（梯度方向的惯性），是目前最流行、最常用的优化器之一。
    

